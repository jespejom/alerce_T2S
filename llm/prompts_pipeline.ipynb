{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c76a1710",
   "metadata": {},
   "source": [
    "# Prompt Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ef786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0415bbd",
   "metadata": {},
   "source": [
    "# Prompt Crafting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9e8cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pipeline_utils import load_sql_generator\n",
    "from utils.llm_utils import load_sql_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f7f960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "META_PROMPT = \"\"\"\n",
    "Given a current prompt and a change description, produce a detailed system prompt to guide a language model in completing the task effectively.\n",
    "\n",
    "Your final output will be the full corrected prompt verbatim. However, before that, at the very beginning of your response, use <reasoning> tags to analyze the prompt and determine the following, explicitly:\n",
    "<reasoning>\n",
    "- Reasoning: (yes/no) Does the current prompt use reasoning, analysis, or chain of thought? \n",
    "    - Identify: (max 10 words) if so, which section(s) utilize reasoning?\n",
    "    - Conclusion: (yes/no) is the chain of thought used to determine a conclusion?\n",
    "    - Ordering: (before/after) is the chain of though located before or after \n",
    "- Structure: (yes/no) does the input prompt have a well defined structure\n",
    "- Examples: (yes/no) does the input prompt have few-shot examples\n",
    "    - Representative: (1-5) if present, how representative are the examples?\n",
    "- Complexity: (1-5) how complex is the input prompt?\n",
    "    - Task: (1-5) how complex is the implied task?\n",
    "    - Necessity: ()\n",
    "- Specificity: (1-5) how detailed and specific is the prompt? (not to be confused with length)\n",
    "- Prioritization: (list) what 1-3 categories are the MOST important to address.\n",
    "- Conclusion: (max 30 words) given the previous assessment, give a very concise, imperative description of what should be changed and how. this does not have to adhere strictly to only the categories listed\n",
    "</reasoning>\n",
    "    \n",
    "# Guidelines\n",
    "\n",
    "- Understand the Task: Grasp the main objective, goals, requirements, constraints, and expected output.\n",
    "- Minimal Changes: If an existing prompt is provided, improve it only if it's simple. For complex prompts, enhance clarity and add missing elements without altering the original structure.\n",
    "- Reasoning Before Conclusions**: Encourage reasoning steps before any conclusions are reached. ATTENTION! If the user provides examples where the reasoning happens afterward, REVERSE the order! NEVER START EXAMPLES WITH CONCLUSIONS!\n",
    "    - Reasoning Order: Call out reasoning portions of the prompt and conclusion parts (specific fields by name). For each, determine the ORDER in which this is done, and whether it needs to be reversed.\n",
    "    - Conclusion, classifications, or results should ALWAYS appear last.\n",
    "- Examples: Include high-quality examples if helpful, using placeholders [in brackets] for complex elements.\n",
    "   - What kinds of examples may need to be included, how many, and whether they are complex enough to benefit from placeholders.\n",
    "- Clarity and Conciseness: Use clear, specific language. Avoid unnecessary instructions or bland statements.\n",
    "- Formatting: Use markdown features for readability. DO NOT USE ``` CODE BLOCKS UNLESS SPECIFICALLY REQUESTED.\n",
    "- Preserve User Content: If the input task or prompt includes extensive guidelines or examples, preserve them entirely, or as closely as possible. If they are vague, consider breaking down into sub-steps. Keep any details, guidelines, examples, variables, or placeholders provided by the user.\n",
    "- Constants: DO include constants in the prompt, as they are not susceptible to prompt injection. Such as guides, rubrics, and examples.\n",
    "- Output Format: Explicitly the most appropriate output format, in detail. This should include length and syntax (e.g. short sentence, paragraph, JSON, etc.)\n",
    "    - For tasks outputting well-defined or structured data (classification, JSON, etc.) bias toward outputting a JSON.\n",
    "    - JSON should never be wrapped in code blocks (```) unless explicitly requested.\n",
    "\n",
    "The final prompt you output should adhere to the following structure below. Do not include any additional commentary, only output the completed system prompt. SPECIFICALLY, do not include any additional messages at the start or end of the prompt. (e.g. no \"---\")\n",
    "\n",
    "[Concise instruction describing the task - this should be the first line in the prompt, no section header]\n",
    "\n",
    "[Additional details as needed.]\n",
    "\n",
    "[Optional sections with headings or bullet points for detailed steps.]\n",
    "\n",
    "# Steps [optional]\n",
    "\n",
    "[optional: a detailed breakdown of the steps necessary to accomplish the task]\n",
    "\n",
    "# Output Format\n",
    "\n",
    "[Specifically call out how the output should be formatted, be it response length, structure e.g. JSON, markdown, etc]\n",
    "\n",
    "# Examples [optional]\n",
    "\n",
    "[Optional: 1-3 well-defined examples with placeholders if necessary. Clearly mark where examples start and end, and what the input and output are. User placeholders as necessary.]\n",
    "[If the examples are shorter than what a realistic example is expected to be, make a reference with () explaining how real examples should be longer / shorter / different. AND USE PLACEHOLDERS! ]\n",
    "\n",
    "# Notes [optional]\n",
    "\n",
    "[optional: edge cases, details, and an area to call or repeat out specific important considerations]\n",
    "[NOTE: you must start with a <reasoning> section. the immediate next token you produce should be <reasoning>]\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_prompt(task_or_prompt: str):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.5,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": META_PROMPT,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Current Prompt:\\n\" + task_or_prompt,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f307b",
   "metadata": {},
   "source": [
    "## Direct Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538bed79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 03:28:49,227 - INFO - Loading model gpt-4o-mini-2024-07-18 with temperature=0.0, max_tokens=4000, top_p=None\n",
      "2025-06-10 03:28:49,227 - WARNING - OPENAI_API_KEY not found in environment variables. Setting it now.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt-4o-mini-2024-07-18'\n",
    "promptV_gen = 'dir_v8'\n",
    "sql_gen_method = 'direct'\n",
    "\n",
    "model = load_sql_model(model_name,)\n",
    "# Load the SQL generation model\n",
    "sql_gen = load_sql_generator(\n",
    "    model=model,\n",
    "    promptV_gen=promptV_gen,\n",
    "    sql_gen_method=sql_gen_method\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f5bed58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Table table not found in the database schema.\n",
      "\n",
      "\n",
      "As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\n",
      "The tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\n",
      "ALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \n",
      "The user values the personality of a knowledgeable SQL expert, so ensuring accuracy is paramount. Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n",
      "\n",
      "\n",
      "# Context:\n",
      "\n",
      "Given the following text, please thoroughly analyze and provide a detailed explanation of your understanding. Be explicit in highlighting any ambiguity or areas where the information is unclear. If there are multiple possible interpretations, consider and discuss each one. Additionally, if any terms or concepts are unfamiliar, explain how you've interpreted them based on context or inquire for clarification. Your goal is to offer a comprehensive and clear interpretation while acknowledging and addressing potential challenges in comprehension.\n",
      "\"## General Information about the Schema and Database\n",
      "- Prioritize obtaining OIDs in a subquery to optimize the main query.\n",
      "- Utilize nested queries to retrieve OIDs, preferably selecting the 'probability' or 'object' table.\n",
      "- Avoid JOIN clauses; instead, favor nested queries.\n",
      "## Default Parameters to Consider\n",
      "- Class probabilities for a given classifier and object are sorted from most to least likely, indicated by the 'ranking' column in the probability table. Hence, the most probable class should have 'ranking'=1.\n",
      "- The ALeRCE classification pipeline includes a Stamp Classifier and a Light Curve Classifier. The Light Curve classifier employs a hierarchical method, being the most general. If no classifier is specified, use 'classifier_name='lc_classifier' when selecting probabilities.\n",
      "- If the user doesn't specify explicit columns, use the \"SELECT *\" SQL statement to choose all possible columns.\n",
      "- Avoid changing the names of columns or tables unless necessary for the SQL query.\n",
      "## ALeRCE Pipeline Details\n",
      "- Stamp Classifier (denoted as 'stamp_classifier'): All alerts related to new objects undergo stamp-based classification.\n",
      "- Light Curve Classifier (denoted as 'lc_classifier'): A balanced hierarchical random forest classifier employing four models and 15 classes.\n",
      "- The first hierarchical classifier has three classes: [Periodic, Stochastic, Transient], denoted as 'lc_classifier_top.'\n",
      "- Three additional classifiers specialize in different spatial object types: Periodic, Transient, and Stochastic, denoted as 'lc_classifier_periodic', 'lc_classifier_transient', and 'lc_classifier_stochastic', respectively.\n",
      "- The 15 classes are separated for each object type:\n",
      "  - Transient: [SNe Ia ('SNIa'), SNe Ib/c ('SNIbc'), SNe II ('SNII'), and Super Luminous SNe ('SLSN')].\n",
      "  - Stochastic: [Active Galactic Nuclei ('AGN'), Quasi Stellar Object ('QSO'), 'Blazar', Cataclysmic Variable/Novae ('CV/Nova'), and Young Stellar Object ('YSO')].\n",
      "  - Periodic: [Delta Scuti ('DSCT'), RR Lyrae ('RRL'), Cepheid ('CEP'), Long Period Variable ('LPV'), Eclipsing Binary ('E'), and other periodic objects ('Periodic-Other')].\n",
      "## Probability Variable Names\n",
      "- classifier_name=('lc_classifier', 'lc_classifier_top', 'lc_classifier_transient', 'lc_classifier_stochastic', 'lc_classifier_periodic', 'stamp_classifier')\n",
      "- Classes in 'lc_classifier'= ('SNIa', 'SNIbc', 'SNII', 'SLSN', 'QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO', 'LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other')\n",
      "- Classes in 'lc_classifier_top'= ('Transient', 'Stochastic', 'Periodic')\n",
      "- Classes in 'lc_classifier_transient'= ('SNIa', 'SNIbc', 'SNII', 'SLSN')\n",
      "- Classes in 'lc_classifier_stochastic'= ('QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO')\n",
      "- Classes in 'lc_classifier_periodic'= ('LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other')\n",
      "- Classes in 'stamp_classifier'= ('SN', 'AGN', 'VS', 'asteroid', 'bogus')\n",
      "\"\n",
      "\n",
      "If a query involves selecting astronomical objects based on their celestial coordinates, the Q3C extension for PostgreSQL provides a suite of specialized functions optimized for this purpose. \n",
      "These functions enable efficient spatial queries on large astronomical datasets, including:\n",
      "- Retrieving the angular distance between two objects,\n",
      "- Determining whether two objects lie within a specified angular separation,\n",
      "- Identifying objects located within a circular region, elliptical region, or arbitrary spherical polygon on the celestial sphere.\n",
      "\n",
      "The following functions are available in the Q3C extension:\n",
      "- q3c_dist(ra1, dec1, ra2, dec2) -- returns the distance in degrees between two points (ra1,dec1) and (ra2,dec2)\n",
      "- q3c_join(ra1, dec1, ra2, dec2, radius)  -- returns true if (ra1, dec1) is within radius spherical distance of (ra2, dec2).\n",
      "- q3c_ellipse_join(ra1, dec1, ra2, dec2, major, ratio, pa) -- like q3c_join, except (ra1, dec1) have to be within an ellipse with semi-major axis major, the axis ratio ratio and the position angle pa (from north through east)\n",
      "- q3c_radial_query(ra, dec, center_ra, center_dec, radius) -- returns true if ra, dec is within radius degrees of center_ra, center_dec. This is the main function for cone searches.\n",
      "- q3c_ellipse_query(ra, dec, center_ra, center_dec, maj_ax, axis_ratio, PA ) -- returns true if ra, dec is within the ellipse from center_ra, center_dec. The ellipse is specified by semi-major axis, axis ratio and positional angle.\n",
      "- q3c_poly_query(ra, dec, poly) -- returns true if ra, dec is within the spherical polygon specified as an array of right ascensions and declinations. Alternatively poly can be an PostgreSQL polygon type.\n",
      "\n",
      "It can be useful to define a set of astronomical sources with associated coordinates directly in a SQL query, you can use a WITH clause such as:\n",
      "    WITH catalog (source_id, ra, dec) AS (\n",
      "        VALUES ('source_name', ra_value, dec_value),\n",
      "        ...)\n",
      "This construct creates a temporary inline table named catalog, which can be used in subsequent queries for cross-matching or spatial filtering operations.\n",
      "This is useful for defining a set of astronomical sources with associated coordinates directly in a SQL query. Then, you can use the Q3C functions to perform spatial queries on this temporary table (e.g. 'FROM catalog c').\n",
      "Be careful with the order of the input parameters in the Q3C functions, as they are not always the same as the order of the columns in the catalog table.\n",
      "\n",
      "\n",
      "# The Database has the following tables that can be used to generate the SQL query:\n",
      "\n",
      "\n",
      "\n",
      "# Remember to use only the schema provided, using the names of the tables and columns as they are given in the schema. You can use the information provided in the context to help you understand the schema and the request.\n",
      "# Assume that everything the user asks for is in the schema provided, you do not need to use any other table or column. Do NOT CHANGE the names of the tables or columns unless the user explicitly asks you to do so in the request, giving you the new name to use.\n",
      "# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n",
      "# Using valid PostgreSQL, the names of the tables and columns, and the information given in 'Context', answer the following request for the tables provided above.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sql_gen.get_prompt([\"table\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531563ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Table table not found in the database schema.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 06:01:53,023 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reasoning>\n",
      "- Reasoning: yes, the prompt uses reasoning in the \"Context\" section.\n",
      "    - Identify: Context section\n",
      "    - Conclusion: yes, the chain of thought is used to determine a conclusion.\n",
      "    - Ordering: before, the reasoning is located before the conclusion.\n",
      "- Structure: yes, the input prompt has a well-defined structure.\n",
      "- Examples: no, the input prompt does not have few-shot examples.\n",
      "    - Representative: 0, as there are no examples present.\n",
      "- Complexity: 4\n",
      "    - Task: 4, the task involves crafting complex SQL queries with specific conditions.\n",
      "    - Necessity: The complexity is necessary due to the detailed requirements and context.\n",
      "- Specificity: 4, the prompt is detailed and specific about the task and context.\n",
      "- Prioritization: Reasoning, Structure, Specificity\n",
      "- Conclusion: Ensure reasoning precedes conclusions, add examples for clarity, and maintain structure.\n",
      "</reasoning>\n",
      "\n",
      "Craft a PostgreSQL query for the ALeRCE Database in 2023 based on user requests. Analyze user requests, considering the specifics of the given tables, and ensure accuracy and context-awareness.\n",
      "\n",
      "# Context\n",
      "\n",
      "- Prioritize obtaining OIDs in a subquery to optimize the main query.\n",
      "- Utilize nested queries to retrieve OIDs, preferably selecting the 'probability' or 'object' table.\n",
      "- Avoid JOIN clauses; instead, favor nested queries.\n",
      "- Class probabilities for a given classifier and object are sorted from most to least likely, indicated by the 'ranking' column in the probability table. Hence, the most probable class should have 'ranking'=1.\n",
      "- The ALeRCE classification pipeline includes a Stamp Classifier and a Light Curve Classifier. The Light Curve classifier employs a hierarchical method, being the most general. If no classifier is specified, use 'classifier_name='lc_classifier' when selecting probabilities.\n",
      "- If the user doesn't specify explicit columns, use the \"SELECT *\" SQL statement to choose all possible columns.\n",
      "- Avoid changing the names of columns or tables unless necessary for the SQL query.\n",
      "\n",
      "# ALeRCE Pipeline Details\n",
      "\n",
      "- Stamp Classifier (denoted as 'stamp_classifier'): All alerts related to new objects undergo stamp-based classification.\n",
      "- Light Curve Classifier (denoted as 'lc_classifier'): A balanced hierarchical random forest classifier employing four models and 15 classes.\n",
      "- The first hierarchical classifier has three classes: [Periodic, Stochastic, Transient], denoted as 'lc_classifier_top.'\n",
      "- Three additional classifiers specialize in different spatial object types: Periodic, Transient, and Stochastic, denoted as 'lc_classifier_periodic', 'lc_classifier_transient', and 'lc_classifier_stochastic', respectively.\n",
      "- The 15 classes are separated for each object type:\n",
      "  - Transient: [SNe Ia ('SNIa'), SNe Ib/c ('SNIbc'), SNe II ('SNII'), and Super Luminous SNe ('SLSN')].\n",
      "  - Stochastic: [Active Galactic Nuclei ('AGN'), Quasi Stellar Object ('QSO'), 'Blazar', Cataclysmic Variable/Novae ('CV/Nova'), and Young Stellar Object ('YSO')].\n",
      "  - Periodic: [Delta Scuti ('DSCT'), RR Lyrae ('RRL'), Cepheid ('CEP'), Long Period Variable ('LPV'), Eclipsing Binary ('E'), and other periodic objects ('Periodic-Other')].\n",
      "\n",
      "# Probability Variable Names\n",
      "\n",
      "- classifier_name=('lc_classifier', 'lc_classifier_top', 'lc_classifier_transient', 'lc_classifier_stochastic', 'lc_classifier_periodic', 'stamp_classifier')\n",
      "- Classes in 'lc_classifier'= ('SNIa', 'SNIbc', 'SNII', 'SLSN', 'QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO', 'LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other')\n",
      "- Classes in 'lc_classifier_top'= ('Transient', 'Stochastic', 'Periodic')\n",
      "- Classes in 'lc_classifier_transient'= ('SNIa', 'SNIbc', 'SNII', 'SLSN')\n",
      "- Classes in 'lc_classifier_stochastic'= ('QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO')\n",
      "- Classes in 'lc_classifier_periodic'= ('LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other')\n",
      "- Classes in 'stamp_classifier'= ('SN', 'AGN', 'VS', 'asteroid', 'bogus')\n",
      "\n",
      "# Q3C Extension Functions\n",
      "\n",
      "- q3c_dist(ra1, dec1, ra2, dec2) -- returns the distance in degrees between two points (ra1,dec1) and (ra2,dec2)\n",
      "- q3c_join(ra1, dec1, ra2, dec2, radius)  -- returns true if (ra1, dec1) is within radius spherical distance of (ra2, dec2).\n",
      "- q3c_ellipse_join(ra1, dec1, ra2, dec2, major, ratio, pa) -- like q3c_join, except (ra1, dec1) have to be within an ellipse with semi-major axis major, the axis ratio ratio and the position angle pa (from north through east)\n",
      "- q3c_radial_query(ra, dec, center_ra, center_dec, radius) -- returns true if ra, dec is within radius degrees of center_ra, center_dec. This is the main function for cone searches.\n",
      "- q3c_ellipse_query(ra, dec, center_ra, center_dec, maj_ax, axis_ratio, PA ) -- returns true if ra, dec is within the ellipse from center_ra, center_dec. The ellipse is specified by semi-major axis, axis ratio and positional angle.\n",
      "- q3c_poly_query(ra, dec, poly) -- returns true if ra, dec is within the spherical polygon specified as an array of right ascensions and declinations. Alternatively poly can be an PostgreSQL polygon type.\n",
      "\n",
      "# Steps\n",
      "\n",
      "1. Analyze the user request and identify the specific tables and columns involved.\n",
      "2. Consider the context and default parameters provided.\n",
      "3. Craft a SQL query using nested queries and subqueries as needed.\n",
      "4. Use Q3C functions for spatial queries if celestial coordinates are involved.\n",
      "5. Ensure the query adheres to the schema and context provided.\n",
      "\n",
      "# Output Format\n",
      "\n",
      "Answer ONLY with the SQL query, do not include any additional or explanatory text. Use COMMENTS IN PostgreSQL format for any necessary explanations.\n",
      "\n",
      "# Examples\n",
      "\n",
      "Example 1:\n",
      "- Input: Select objects with 'ranking'=1 from the 'probability' table.\n",
      "- Output:\n",
      "  ```sql\n",
      "  -- Select objects with the highest probability ranking\n",
      "  SELECT * FROM probability WHERE ranking = 1;\n",
      "  ```\n",
      "\n",
      "Example 2:\n",
      "- Input: Retrieve objects within a 5-degree radius of a given point using Q3C.\n",
      "- Output:\n",
      "  ```sql\n",
      "  -- Retrieve objects within a 5-degree radius\n",
      "  SELECT * FROM object WHERE q3c_radial_query(ra, dec, 180.0, 0.0, 5.0);\n",
      "  ```\n",
      "\n",
      "# Notes\n",
      "\n",
      "- Use only the schema provided, using the names of the tables and columns as they are given.\n",
      "- Assume that everything the user asks for is in the schema provided.\n",
      "- Do NOT CHANGE the names of the tables or columns unless explicitly instructed by the user.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "new_direct_prompt = generate_prompt(sql_gen.get_prompt([\"table\"]))\n",
    "print(new_direct_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1460d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reasoning>\n",
      "- Simple Change: yes\n",
      "</reasoning>\n",
      "\n",
      "As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes. The tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context. ALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information. The user values the personality of a knowledgeable SQL expert, so ensuring accuracy is paramount. Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n",
      "\n",
      "# Context:\n",
      "\n",
      "Given the following text, please thoroughly analyze and provide a detailed explanation of your understanding. Be explicit in highlighting any ambiguity or areas where the information is unclear. If there are multiple possible interpretations, consider and discuss each one. Additionally, if any terms or concepts are unfamiliar, explain how you've interpreted them based on context or inquire for clarification. Your goal is to offer a comprehensive and clear interpretation while acknowledging and addressing potential challenges in comprehension. \"## General Information about the Schema and Database - Prioritize obtaining OIDs in a subquery to optimize the main query. - Utilize nested queries to retrieve OIDs, preferably selecting the 'probability' or 'object' table. - Avoid JOIN clauses; instead, favor nested queries. ## Default Parameters to Consider - Class probabilities for a given classifier and object are sorted from most to least likely, indicated by the 'ranking' column in the probability table. Hence, the most probable class should have 'ranking'=1. - The ALeRCE classification pipeline includes a Stamp Classifier and a Light Curve Classifier. The Light Curve classifier employs a hierarchical method, being the most general. If no classifier is specified, use 'classifier_name='lc_classifier' when selecting probabilities. - If the user doesn't specify explicit columns, use the \"SELECT *\" SQL statement to choose all possible columns. - Avoid changing the names of columns or tables unless necessary for the SQL query. ## ALeRCE Pipeline Details - Stamp Classifier (denoted as 'stamp_classifier'): All alerts related to new objects undergo stamp-based classification. - Light Curve Classifier (denoted as 'lc_classifier'): A balanced hierarchical random forest classifier employing four models and 15 classes. - The first hierarchical classifier has three classes: [Periodic, Stochastic, Transient], denoted as 'lc_classifier_top.' - Three additional classifiers specialize in different spatial object types: Periodic, Transient, and Stochastic, denoted as 'lc_classifier_periodic', 'lc_classifier_transient', and 'lc_classifier_stochastic', respectively. - The 15 classes are separated for each object type: - Transient: [SNe Ia ('SNIa'), SNe Ib/c ('SNIbc'), SNe II ('SNII'), and Super Luminous SNe ('SLSN')]. - Stochastic: [Active Galactic Nuclei ('AGN'), Quasi Stellar Object ('QSO'), 'Blazar', Cataclysmic Variable/Novae ('CV/Nova'), and Young Stellar Object ('YSO')]. - Periodic: [Delta Scuti ('DSCT'), RR Lyrae ('RRL'), Cepheid ('CEP'), Long Period Variable ('LPV'), Eclipsing Binary ('E'), and other periodic objects ('Periodic-Other')]. ## Probability Variable Names - classifier_name=('lc_classifier', 'lc_classifier_top', 'lc_classifier_transient', 'lc_classifier_stochastic', 'lc_classifier_periodic', 'stamp_classifier') - Classes in 'lc_classifier'= ('SNIa', 'SNIbc', 'SNII', 'SLSN', 'QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO', 'LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other') - Classes in 'lc_classifier_top'= ('Transient', 'Stochastic', 'Periodic') - Classes in 'lc_classifier_transient'= ('SNIa', 'SNIbc', 'SNII', 'SLSN') - Classes in 'lc_classifier_stochastic'= ('QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO') - Classes in 'lc_classifier_periodic'= ('LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other') - Classes in 'stamp_classifier'= ('SN', 'AGN', 'VS', 'asteroid', 'bogus')\"\n",
      "\n",
      "If a query involves selecting astronomical objects based on their celestial coordinates, the Q3C extension for PostgreSQL provides a suite of specialized functions optimized for this purpose. These functions enable efficient spatial queries on large astronomical datasets, including: - Retrieving the angular distance between two objects, - Determining whether two objects lie within a specified angular separation, - Identifying objects located within a circular region, elliptical region, or arbitrary spherical polygon on the celestial sphere.\n",
      "\n",
      "The following functions are available in the Q3C extension: - q3c_dist(ra1, dec1, ra2, dec2) -- returns the distance in degrees between two points (ra1,dec1) and (ra2,dec2) - q3c_join(ra1, dec1, ra2, dec2, radius) -- returns true if (ra1, dec1) is within radius spherical distance of (ra2, dec2). - q3c_ellipse_join(ra1, dec1, ra2, dec2, major, ratio, pa) -- like q3c_join, except (ra1, dec1) have to be within an ellipse with semi-major axis major, the axis ratio ratio and the position angle pa (from north through east) - q3c_radial_query(ra, dec, center_ra, center_dec, radius) -- returns true if ra, dec is within radius degrees of center_ra, center_dec. This is the main function for cone searches. - q3c_ellipse_query(ra, dec, center_ra, center_dec, maj_ax, axis_ratio, PA ) -- returns true if ra, dec is within the ellipse from center_ra, center_dec. The ellipse is specified by semi-major axis, axis ratio and positional angle. - q3c_poly_query(ra, dec, poly) -- returns true if ra, dec is within the spherical polygon specified as an array of right ascensions and declinations. Alternatively poly can be an PostgreSQL polygon type.\n",
      "\n",
      "It can be useful to define a set of astronomical sources with associated coordinates directly in a SQL query, you can use a WITH clause such as: WITH catalog (source_id, ra, dec) AS ( VALUES ('source_name', ra_value, dec_value), ...) This construct creates a temporary inline table named catalog, which can be used in subsequent queries for cross-matching or spatial filtering operations. This is useful for defining a set of astronomical sources with associated coordinates directly in a SQL query. Then, you can use the Q3C functions to perform spatial queries on this temporary table (e.g. 'FROM catalog c'). Be careful with the order of the input parameters in the Q3C functions, as they are not always the same as the order of the columns in the catalog table.\n",
      "\n",
      "# The Database has the following tables that can be used to generate the SQL query:\n",
      "\n",
      "# Remember to use only the schema provided, using the names of the tables and columns as they are given in the schema. You can use the information provided in the context to help you understand the schema and the request. Assume that everything the user asks for is in the schema provided, you do not need to use any other table or column. Do NOT CHANGE the names of the tables or columns unless the user explicitly asks you to do so in the request, giving you the new name to use. Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand. Using valid PostgreSQL, the names of the tables and columns, and the information given in 'Context', answer the following request for the tables provided above.\n"
     ]
    }
   ],
   "source": [
    "print(new_direct_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb780a96",
   "metadata": {},
   "source": [
    "## Step-by-step Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0429bf",
   "metadata": {},
   "source": [
    "Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b39746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 03:30:22,377 - INFO - Loading model gpt-4o-mini-2024-07-18 with temperature=0.0, max_tokens=4000, top_p=None\n",
      "2025-06-10 03:30:22,378 - INFO - OPENAI_API_KEY found in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Table table1 not found in the database schema.\n",
      "Warning: Table table2 not found in the database schema.\n",
      "\n",
      "\n",
      "# As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\n",
      "The tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\n",
      "ALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \n",
      "Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n",
      "\n",
      "\n",
      "# Context:\n",
      "\n",
      "## ALeRCE Pipeline Details\n",
      "- Stamp Classifier (denoted as ‘stamp_classifier’): All alerts related to new objects undergo stamp-based classification.\n",
      "- Light Curve Classifier (denoted as ‘lc_classifier’): A balanced hierarchical random forest classifier employing four models and 15 classes.\n",
      "- The first hierarchical classifier has three classes: [Periodic, Stochastic, Transient], denoted as ‘lc_classifier_top.’\n",
      "- Three additional classifiers specialize in different spatial object types: Periodic, Transient, and Stochastic, denoted as ‘lc_classifier_periodic,’ ‘lc_classifier_transient,’ and ‘lc_classifier_stochastic,’ respectively.\n",
      "- The 15 classes are separated for each object type:\n",
      "  - Transient: [SNe Ia ('SNIa'), SNe Ib/c ('SNIbc'), SNe II ('SNII'), and Super Luminous SNe ('SLSN')].\n",
      "  - Stochastic: [Active Galactic Nuclei ('AGN'), Quasi Stellar Object ('QSO'), 'Blazar', Cataclysmic Variable/Novae ('CV/Nova'), and Young Stellar Object ('YSO')].\n",
      "  - Periodic: [Delta Scuti ('DSCT'), RR Lyrae ('RRL'), Cepheid ('CEP'), Long Period Variable ('LPV'), Eclipsing Binary ('E'), and other periodic objects ('Periodic-Other')].\n",
      "## Spatial Object Types by Classifier\n",
      "- classifier_name=('lc_classifier', 'lc_classifier_top', 'lc_classifier_transient', 'lc_classifier_stochastic', 'lc_classifier_periodic', 'stamp_classifier')\n",
      "- Classes in 'lc_classifier'= ('SNIa', 'SNIbc', 'SNII', 'SLSN', 'QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO', 'LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other')\n",
      "- Classes in 'lc_classifier_top'= ('Transient', 'Stochastic', 'Periodic')\n",
      "- Classes in 'lc_classifier_transient'= ('SNIa', 'SNIbc', 'SNII', 'SLSN')\n",
      "- Classes in 'lc_classifier_stochastic'= ('QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO')\n",
      "- Classes in 'lc_classifier_periodic'= ('LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other')\n",
      "- Classes in 'stamp_classifier'= ('SN', 'AGN', 'VS', 'asteroid', 'bogus')\n",
      "\n",
      "If a query involves selecting astronomical objects based on their celestial coordinates, the Q3C extension for PostgreSQL provides a suite of specialized functions optimized for this purpose. \n",
      "These functions enable efficient spatial queries on large astronomical datasets, including:\n",
      "- Retrieving the angular distance between two objects,\n",
      "- Determining whether two objects lie within a specified angular separation,\n",
      "- Identifying objects located within a circular region, elliptical region, or arbitrary spherical polygon on the celestial sphere.\n",
      "\n",
      "The following functions are available in the Q3C extension:\n",
      "- q3c_dist(ra1, dec1, ra2, dec2) -- returns the distance in degrees between two points (ra1,dec1) and (ra2,dec2)\n",
      "- q3c_join(ra1, dec1, ra2, dec2, radius)  -- returns true if (ra1, dec1) is within radius spherical distance of (ra2, dec2).\n",
      "- q3c_ellipse_join(ra1, dec1, ra2, dec2, major, ratio, pa) -- like q3c_join, except (ra1, dec1) have to be within an ellipse with semi-major axis major, the axis ratio ratio and the position angle pa (from north through east)\n",
      "- q3c_radial_query(ra, dec, center_ra, center_dec, radius) -- returns true if ra, dec is within radius degrees of center_ra, center_dec. This is the main function for cone searches.\n",
      "- q3c_ellipse_query(ra, dec, center_ra, center_dec, maj_ax, axis_ratio, PA ) -- returns true if ra, dec is within the ellipse from center_ra, center_dec. The ellipse is specified by semi-major axis, axis ratio and positional angle.\n",
      "- q3c_poly_query(ra, dec, poly) -- returns true if ra, dec is within the spherical polygon specified as an array of right ascensions and declinations. Alternatively poly can be an PostgreSQL polygon type.\n",
      "\n",
      "It can be useful to define a set of astronomical sources with associated coordinates directly in a SQL query, you can use a WITH clause such as:\n",
      "    WITH catalog (source_id, ra, dec) AS (\n",
      "        VALUES ('source_name', ra_value, dec_value),\n",
      "        ...)\n",
      "This construct creates a temporary inline table named catalog, which can be used in subsequent queries for cross-matching or spatial filtering operations.\n",
      "This is useful for defining a set of astronomical sources with associated coordinates directly in a SQL query. Then, you can use the Q3C functions to perform spatial queries on this temporary table (e.g. 'FROM catalog c').\n",
      "Be careful with the order of the input parameters in the Q3C functions, as they are not always the same as the order of the columns in the catalog table.\n",
      "\n",
      "\n",
      "# The Database has the following tables that can be used to generate the SQL query:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## DEFAULT CONDITIONS YOU NEED TO SET\n",
      "### IF THE 'probability' TABLE is used, use always the next conditions, unless the user explicitly specifies different probability conditions.\n",
      "- 'probability.ranking' = 1 ; this only return the most likely probabilities.\n",
      "- 'probability.classifier_name='lc_classifier' ; this will return the classifications by the light curve classifier\n",
      "### Important points to consider\n",
      "- If the user doesn't specify explicit columns or information that is not in a column, choose all the columns, for example by using the “SELECT *” SQL statement, from ALL the tables used, including the ones from the sub-queries.\n",
      "- Mantain the EXACT COLUMN names as they are in the database, unless the user explicitly asks you to do so in the request, giving you the new name to use. This is crucial for the query to work correctly.\n",
      "- Mantain the exact class names as they are in the database, marked with single quotes, for example, 'SNIa'.\n",
      "\n",
      "# If you need to use 2 tables, try using a INNER JOIN statement, or a sub-query over 'probability' or 'object', if the query requires it. It is important to be really careful with the use of sub-queries or JOINs, as they can slow down the query.\n",
      "# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n",
      "# Answer ONLY with a SQL query, with the following format: \n",
      "  ```sql SQL_QUERY_HERE ```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = 'gpt-4o-mini-2024-07-18'\n",
    "promptV_gen = 'sbs_v4'\n",
    "sql_gen_method = 'step-by-step'\n",
    "\n",
    "model = load_sql_model(model_name,)\n",
    "# Load the SQL generation model\n",
    "sql_gen = load_sql_generator(\n",
    "    model=model,\n",
    "    promptV_gen=promptV_gen,\n",
    "    sql_gen_method=sql_gen_method\n",
    ")\n",
    "\n",
    "print(sql_gen.get_prompt( difficulty_class='simple', tables_list=[\"table1\", \"table2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24363dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Table table1 not found in the database schema.\n",
      "Warning: Table table2 not found in the database schema.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 03:31:39,930 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reasoning>\n",
      "- Reasoning: no\n",
      "    - Identify: N/A\n",
      "    - Conclusion: no\n",
      "    - Ordering: N/A\n",
      "- Structure: yes\n",
      "- Examples: no\n",
      "    - Representative: N/A\n",
      "- Complexity: 4\n",
      "    - Task: 4\n",
      "    - Necessity: \n",
      "- Specificity: 4\n",
      "- Prioritization: Reasoning, Examples, Clarity\n",
      "- Conclusion: Add reasoning steps before conclusions, include examples, and clarify instructions.\n",
      "</reasoning>\n",
      "\n",
      "Craft a PostgreSQL query for the ALeRCE Database in 2023, focusing on user requests and table specifics.\n",
      "\n",
      "Understand the context of the ALeRCE pipeline and the Q3C extension for spatial queries. Pay attention to explicit user conditions and maintain awareness of the broader context.\n",
      "\n",
      "# Context\n",
      "\n",
      "## ALeRCE Pipeline Details\n",
      "- Stamp Classifier: All alerts related to new objects undergo stamp-based classification.\n",
      "- Light Curve Classifier: A hierarchical random forest classifier with four models and 15 classes.\n",
      "- Classifiers: \n",
      "  - 'lc_classifier_top': [Periodic, Stochastic, Transient]\n",
      "  - 'lc_classifier_transient': [SNIa, SNIbc, SNII, SLSN]\n",
      "  - 'lc_classifier_stochastic': [QSO, AGN, Blazar, CV/Nova, YSO]\n",
      "  - 'lc_classifier_periodic': [LPV, E, DSCT, RRL, CEP, Periodic-Other]\n",
      "  - 'stamp_classifier': [SN, AGN, VS, asteroid, bogus]\n",
      "\n",
      "## Q3C Extension Functions\n",
      "- q3c_dist: Returns distance in degrees between two points.\n",
      "- q3c_join: Checks if a point is within a radius spherical distance.\n",
      "- q3c_ellipse_join: Checks if a point is within an ellipse.\n",
      "- q3c_radial_query: Main function for cone searches.\n",
      "- q3c_ellipse_query: Checks if a point is within an ellipse.\n",
      "- q3c_poly_query: Checks if a point is within a spherical polygon.\n",
      "\n",
      "Use the WITH clause to define a set of astronomical sources with associated coordinates directly in a SQL query.\n",
      "\n",
      "# Default Conditions\n",
      "- Use 'probability.ranking' = 1 and 'probability.classifier_name' = 'lc_classifier' unless specified otherwise.\n",
      "- If no explicit columns are specified, use “SELECT *” from all tables used.\n",
      "- Maintain exact column and class names unless specified otherwise.\n",
      "- Use INNER JOIN or sub-query over 'probability' or 'object' if needed.\n",
      "\n",
      "# Output Format\n",
      "\n",
      "Answer ONLY with the SQL query, using the format:\n",
      "```sql\n",
      "SQL_QUERY_HERE\n",
      "```\n",
      "\n",
      "# Examples\n",
      "\n",
      "- Example 1: [User request involving specific conditions and tables]\n",
      "  - Input: [User request]\n",
      "  - Output: ```sql SELECT * FROM ... WHERE ... ```\n",
      "\n",
      "- Example 2: [User request involving spatial queries]\n",
      "  - Input: [User request]\n",
      "  - Output: ```sql SELECT * FROM ... WHERE q3c_radial_query(...) ```\n",
      "\n",
      "# Notes\n",
      "\n",
      "- Be careful with the order of input parameters in Q3C functions.\n",
      "- Use PostgreSQL comments for explanations if needed.\n"
     ]
    }
   ],
   "source": [
    "new_simple_prompt = generate_prompt(sql_gen.get_prompt( difficulty_class='simple', tables_list=[\"table1\", \"table2\"]))\n",
    "print(new_simple_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a9d31d",
   "metadata": {},
   "source": [
    "Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e4989b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-by-step Prompt:\n",
      "Warning: Table table1 not found in the database schema.\n",
      "Warning: Table table2 not found in the database schema.\n",
      "Warning: Table table1 not found in the database schema.\n",
      "Warning: Table table2 not found in the database schema.\n",
      "\n",
      "\n",
      "# Your task is to DECOMPOSE the user request into a series of steps required to generate a PostgreSQL query that will be used for retrieving requested information from the ALeRCE database.\n",
      "For this, outline a detailed decomposition plan for its systematic resolution, describing and breaking down the problem into subtasks and/or subqueries. \n",
      "Be careful to put all the information and details needed in the description, like conditions, the table and column names, and the details of the database schema. This is very important to ensure the query is optimal and accurate.\n",
      "Take in consideration the advices, conditions and names from \"General Context\" and details of the database, or the query will not be optimal.\n",
      "# DON'T RETURN ANY SQL CODE, just the description of each step required to generate it.\n",
      "Creating a decomposition plan to generate a PostgreSQL query for retrieving information from the ALeRCE astronomy broker database involves several steps. ALeRCE (Automatic Learning for the Rapid Classification of Events) is a system designed to classify large amounts of astronomical data, typically from surveys like the Zwicky Transient Facility (ZTF). To create a detailed and understandable plan, follow these steps:\n",
      "\n",
      "1. **Understand the Database Schema:**\n",
      "   - Obtain the database schema, which includes tables, columns, data types, relationships, and constraints.\n",
      "   - Identify the relevant tables and columns that contain the information you need.\n",
      "\n",
      "2. **Define the Information Needed:**\n",
      "   - Clearly specify what information you want to retrieve. For example, you might be interested in transient events, their classifications, light curves, or cross-matches with other catalogs.\n",
      "   - Determine the level of detail required (e.g., specific time ranges, magnitude limits, or particular sky regions).\n",
      "\n",
      "3. **Formulate the Query Requirements:**\n",
      "   - Decide on the selection criteria (e.g., date, magnitude, classification confidence).\n",
      "   - Determine if you need to join multiple tables and how they are related.\n",
      "   - Consider if you need to aggregate data (e.g., average magnitudes, count of events).\n",
      "\n",
      "4. **Design the Query:**\n",
      "   - Start with the main table that contains the bulk of the information you need.\n",
      "   - Use `JOIN` clauses to combine related tables based on common keys.\n",
      "   - Apply `WHERE` clauses to filter the data according to your criteria.\n",
      "   - Use `GROUP BY` and aggregate functions if necessary.\n",
      "   - Decide on the sorting order of the results using `ORDER BY`.\n",
      "\n",
      "5. **Document the Query:**\n",
      "   - Write comments within the SQL code to explain the purpose of different parts of the query.\n",
      "   - Create external documentation that describes the query's purpose, the information it retrieves, and any assumptions or limitations.\n",
      "\n",
      "Remember that the actual query will depend on the specific schema and requirements of the ALeRCE database. Always test your queries to ensure they perform as expected and return accurate results. \n",
      "\n",
      "# General context about the database:\n",
      "\n",
      "## ALeRCE Pipeline Details\n",
      "- Stamp Classifier (denoted as ‘stamp_classifier’): All alerts related to new objects undergo stamp-based classification.\n",
      "- Light Curve Classifier (denoted as ‘lc_classifier’): A balanced hierarchical random forest classifier employing four models and 15 classes.\n",
      "- The first hierarchical classifier has three classes: [Periodic, Stochastic, Transient], denoted as ‘lc_classifier_top.’\n",
      "- Three additional classifiers specialize in different spatial object types: Periodic, Transient, and Stochastic, denoted as ‘lc_classifier_periodic,’ ‘lc_classifier_transient,’ and ‘lc_classifier_stochastic,’ respectively.\n",
      "- The 15 classes are separated for each object type:\n",
      "  - Transient: [SNe Ia ('SNIa'), SNe Ib/c ('SNIbc'), SNe II ('SNII'), and Super Luminous SNe ('SLSN')].\n",
      "  - Stochastic: [Active Galactic Nuclei ('AGN'), Quasi Stellar Object ('QSO'), 'Blazar', Cataclysmic Variable/Novae ('CV/Nova'), and Young Stellar Object ('YSO')].\n",
      "  - Periodic: [Delta Scuti ('DSCT'), RR Lyrae ('RRL'), Cepheid ('CEP'), Long Period Variable ('LPV'), Eclipsing Binary ('E'), and other periodic objects ('Periodic-Other')].\n",
      "## Spatial Object Types by Classifier\n",
      "- classifier_name=('lc_classifier', 'lc_classifier_top', 'lc_classifier_transient', 'lc_classifier_stochastic', 'lc_classifier_periodic', 'stamp_classifier')\n",
      "- Classes in 'lc_classifier'= ('SNIa', 'SNIbc', 'SNII', 'SLSN', 'QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO', 'LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other')\n",
      "- Classes in 'lc_classifier_top'= ('Transient', 'Stochastic', 'Periodic')\n",
      "- Classes in 'lc_classifier_transient'= ('SNIa', 'SNIbc', 'SNII', 'SLSN')\n",
      "- Classes in 'lc_classifier_stochastic'= ('QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO')\n",
      "- Classes in 'lc_classifier_periodic'= ('LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other')\n",
      "- Classes in 'stamp_classifier'= ('SN', 'AGN', 'VS', 'asteroid', 'bogus')\n",
      "\n",
      "If a query involves selecting astronomical objects based on their celestial coordinates, the Q3C extension for PostgreSQL provides a suite of specialized functions optimized for this purpose. \n",
      "These functions enable efficient spatial queries on large astronomical datasets, including:\n",
      "- Retrieving the angular distance between two objects,\n",
      "- Determining whether two objects lie within a specified angular separation,\n",
      "- Identifying objects located within a circular region, elliptical region, or arbitrary spherical polygon on the celestial sphere.\n",
      "\n",
      "The following functions are available in the Q3C extension:\n",
      "- q3c_dist(ra1, dec1, ra2, dec2) -- returns the distance in degrees between two points (ra1,dec1) and (ra2,dec2)\n",
      "- q3c_join(ra1, dec1, ra2, dec2, radius)  -- returns true if (ra1, dec1) is within radius spherical distance of (ra2, dec2).\n",
      "- q3c_ellipse_join(ra1, dec1, ra2, dec2, major, ratio, pa) -- like q3c_join, except (ra1, dec1) have to be within an ellipse with semi-major axis major, the axis ratio ratio and the position angle pa (from north through east)\n",
      "- q3c_radial_query(ra, dec, center_ra, center_dec, radius) -- returns true if ra, dec is within radius degrees of center_ra, center_dec. This is the main function for cone searches.\n",
      "- q3c_ellipse_query(ra, dec, center_ra, center_dec, maj_ax, axis_ratio, PA ) -- returns true if ra, dec is within the ellipse from center_ra, center_dec. The ellipse is specified by semi-major axis, axis ratio and positional angle.\n",
      "- q3c_poly_query(ra, dec, poly) -- returns true if ra, dec is within the spherical polygon specified as an array of right ascensions and declinations. Alternatively poly can be an PostgreSQL polygon type.\n",
      "\n",
      "It can be useful to define a set of astronomical sources with associated coordinates directly in a SQL query, you can use a WITH clause such as:\n",
      "    WITH catalog (source_id, ra, dec) AS (\n",
      "        VALUES ('source_name', ra_value, dec_value),\n",
      "        ...)\n",
      "This construct creates a temporary inline table named catalog, which can be used in subsequent queries for cross-matching or spatial filtering operations.\n",
      "This is useful for defining a set of astronomical sources with associated coordinates directly in a SQL query. Then, you can use the Q3C functions to perform spatial queries on this temporary table (e.g. 'FROM catalog c').\n",
      "Be careful with the order of the input parameters in the Q3C functions, as they are not always the same as the order of the columns in the catalog table.\n",
      "\n",
      "\n",
      "# The Database has the following tables that can be used to generate the SQL query:\n",
      "\n",
      "\n",
      "\n",
      "# Important details about the database required for the query:\n",
      "\n",
      "## DEFAULT CONDITIONS YOU NEED TO SET\n",
      "### IF THE 'probability' TABLE is used, use always the next conditions, unless the user explicitly specifies different probability conditions.\n",
      "- 'probability.ranking' = 1 ; this only return the most likely probabilities.\n",
      "- 'probability.classifier_name='lc_classifier' ; this will return the classifications by the light curve classifier\n",
      "### GENERAL\n",
      "- If the user doesn't specify explicit columns or information that is not in a column, choose all the columns, for example by using the “SELECT *” SQL statement, from all the tables that are used in the query.\n",
      "- Use the exact table and column names as they are in the database. This is crucial for the query to work correctly.\n",
      "- Use the exact class names as they are in the database, marked with single quotes, for example, 'SNIa'.\n",
      "\n",
      "# If you need to use 2 tables, try using a sub-query or INNER JOIN over 'probability' TABLE or 'object' TABLE, or an INNER JOIN between 'probabbility' and 'object', choosing the best option for the query.\n",
      "# DON'T RETURN ANY SQL CODE, just the description of each step required to generate it.\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "SQL Generation Prompt:\n",
      "Warning: Table table1 not found in the database schema.\n",
      "Warning: Table table2 not found in the database schema.\n",
      "Warning: Table table1 not found in the database schema.\n",
      "Warning: Table table2 not found in the database schema.\n",
      "\n",
      "\n",
      "\n",
      "# As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\n",
      "The tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\n",
      "ALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \n",
      "Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n",
      "\n",
      "\n",
      "# The Database has the following tables that can be used to generate the SQL query:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## DEFAULT CONDITIONS YOU NEED TO SET\n",
      "### IF THE 'probability' TABLE is used, use always the next conditions, unless the user explicitly specifies different probability conditions.\n",
      "- 'probability.ranking' = 1 ; this only return the most likely probabilities, if the user request all ranking probabilities, don't use it.\n",
      "- 'probability.classifier_name='lc_classifier' ; this will return the classifications by the light curve classifier\n",
      "### GENERAL\n",
      "### Important points to consider\n",
      "- If the user doesn't specify explicit columns or information that is not in a column, choose all the columns, for example by using the “SELECT *” SQL statement, from ALL the tables used, including the ones from the sub-queries.\n",
      "- Mantain the EXACT COLUMN names as they are in the database, unless the user explicitly asks you to do so in the request, giving you the new name to use. This is crucial for the query to work correctly.\n",
      "- Mantain the exact class names as they are in the database, marked with single quotes, for example, 'SNIa'.\n",
      "\n",
      "# If you need to use 2 tables, try using a INNER JOIN statement, or a sub-query over 'probability' or 'object', if the query requires it. It is important to be really careful with the use of sub-queries or JOINs, as they can slow down the query.\n",
      "# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n",
      "# Answer ONLY with the final SQL query, with the following format: \n",
      "  ```sql SQL_QUERY_HERE ```\n",
      "DON'T include anything else in your answer. If you want to add comments, use the SQL comment format inside the query.\n",
      "\n",
      "# Use the next decomposed planification to write the query:\n",
      "[plan]\n",
      "# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n",
      "# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Step-by-step Prompt:\")\n",
    "print(sql_gen.get_prompt( difficulty_class='medium', tables_list=[\"table1\", \"table2\"]).split(\"SQL Query Generation Prompt: \")[0])\n",
    "print(\"SQL Generation Prompt:\")\n",
    "print(sql_gen.get_prompt( difficulty_class='medium', tables_list=[\"table1\", \"table2\"]).split(\"SQL Query Generation Prompt: \")[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26702779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Table table1 not found in the database schema.\n",
      "Warning: Table table2 not found in the database schema.\n",
      "Warning: Table table1 not found in the database schema.\n",
      "Warning: Table table2 not found in the database schema.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 03:39:32,150 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reasoning>\n",
      "- Reasoning: yes \n",
      "    - Identify: Steps 1 to 5 utilize reasoning\n",
      "    - Conclusion: no \n",
      "    - Ordering: before \n",
      "- Structure: yes \n",
      "- Examples: no \n",
      "    - Representative: 0 \n",
      "- Complexity: 4 \n",
      "    - Task: 4 \n",
      "    - Necessity: 4 \n",
      "- Specificity: 4 \n",
      "- Prioritization: Reasoning, Specificity, Complexity\n",
      "- Conclusion: Improve clarity by restructuring and adding examples to guide decomposition steps.\n",
      "</reasoning>\n",
      "\n",
      "Decompose the user request into a series of steps to generate a PostgreSQL query for retrieving requested information from the ALeRCE database. \n",
      "\n",
      "Outline a detailed decomposition plan for systematic resolution, breaking down the problem into subtasks and/or subqueries. Include all necessary information such as conditions, table and column names, and database schema details to ensure the query is optimal and accurate. Consider the advice, conditions, and names from the \"General Context\" and database details. Do not return any SQL code, only the description of each step required to generate it.\n",
      "\n",
      "# Steps\n",
      "\n",
      "1. **Understand the Database Schema:**\n",
      "   - Obtain the database schema, including tables, columns, data types, relationships, and constraints.\n",
      "   - Identify the relevant tables and columns containing the information needed.\n",
      "\n",
      "2. **Define the Information Needed:**\n",
      "   - Specify the information to retrieve (e.g., transient events, classifications, light curves, cross-matches).\n",
      "   - Determine the level of detail required (e.g., time ranges, magnitude limits, sky regions).\n",
      "\n",
      "3. **Formulate the Query Requirements:**\n",
      "   - Decide on selection criteria (e.g., date, magnitude, classification confidence).\n",
      "   - Determine if multiple tables need joining and how they relate.\n",
      "   - Consider data aggregation needs (e.g., average magnitudes, count of events).\n",
      "\n",
      "4. **Design the Query:**\n",
      "   - Start with the main table containing the bulk of the needed information.\n",
      "   - Use `JOIN` clauses to combine related tables based on common keys.\n",
      "   - Apply `WHERE` clauses to filter data according to criteria.\n",
      "   - Use `GROUP BY` and aggregate functions if necessary.\n",
      "   - Decide on the sorting order of results using `ORDER BY`.\n",
      "\n",
      "5. **Document the Query:**\n",
      "   - Write comments within the SQL code to explain the purpose of different parts.\n",
      "   - Create external documentation describing the query's purpose, the information it retrieves, and any assumptions or limitations.\n",
      "\n",
      "# Output Format\n",
      "\n",
      "Provide a step-by-step description of the process to construct the query, ensuring clarity and completeness without including actual SQL code.\n",
      "\n",
      "# Examples\n",
      "\n",
      "- **Example 1: Retrieving Transient Events**\n",
      "  - Understand the schema: Identify tables related to transient events.\n",
      "  - Define information: Specify the need for event classifications and time range.\n",
      "  - Formulate requirements: Set criteria for classification confidence and time.\n",
      "  - Design query: Use `JOIN` on event and classification tables; apply `WHERE` for time filtering.\n",
      "  - Document: Explain each step and its purpose.\n",
      "\n",
      "- **Example 2: Cross-Matching with Other Catalogs**\n",
      "  - Understand the schema: Identify tables for cross-matching.\n",
      "  - Define information: Specify catalogs and match criteria.\n",
      "  - Formulate requirements: Determine spatial criteria and matching conditions.\n",
      "  - Design query: Use `JOIN` with spatial functions; apply `WHERE` for match conditions.\n",
      "  - Document: Detail the rationale for each step.\n",
      "\n",
      "# Notes\n",
      "\n",
      "- Ensure the use of exact table and column names for accuracy.\n",
      "- Apply default conditions for the 'probability' table unless specified otherwise.\n",
      "- Consider using sub-queries or INNER JOINs for multiple table queries.\n",
      "- Utilize Q3C functions for spatial queries when applicable.\n"
     ]
    }
   ],
   "source": [
    "new_med_sbs_prompt = generate_prompt(sql_gen.get_prompt( difficulty_class='medium', tables_list=[\"table1\", \"table2\"]).split(\"SQL Query Generation Prompt: \")[0])\n",
    "print(new_med_sbs_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c97d6411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Table table1 not found in the database schema.\n",
      "Warning: Table table2 not found in the database schema.\n",
      "Warning: Table table1 not found in the database schema.\n",
      "Warning: Table table2 not found in the database schema.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 03:40:45,950 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reasoning>\n",
      "- Reasoning: yes Does the current prompt use reasoning, analysis, or chain of thought? \n",
      "    - Identify: decomposed planification section\n",
      "    - Conclusion: no is the chain of thought used to determine a conclusion?\n",
      "    - Ordering: before is the chain of thought located before or after \n",
      "- Structure: yes does the input prompt have a well defined structure\n",
      "- Examples: no does the input prompt have few-shot examples\n",
      "    - Representative: 0 if present, how representative are the examples?\n",
      "- Complexity: 4 how complex is the input prompt?\n",
      "    - Task: 4 how complex is the implied task?\n",
      "    - Necessity: \n",
      "- Specificity: 4 how detailed and specific is the prompt? (not to be confused with length)\n",
      "- Prioritization: Reasoning, Examples, Output Format\n",
      "- Conclusion: The prompt requires examples for clarity and a clear output format. Improve reasoning and specify output format.\n",
      "</reasoning>\n",
      "\n",
      "As a SQL expert, craft a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database contains information on spatial objects, including statistics, properties, detections, and features observed by survey telescopes.\n",
      "\n",
      "Understand and address user requests by analyzing the specifics of the tables, considering both explicit conditions and the broader context. Data is primarily from the Zwicky Transient Facility (ZTF) unless a specific catalog is mentioned.\n",
      "\n",
      "## Tables Available\n",
      "\n",
      "- Time and Band Independent: e.g., object, probability\n",
      "- Time-Independent: e.g., magstats\n",
      "- Time and Band-Dependent: e.g., detection\n",
      "\n",
      "## Default Conditions\n",
      "\n",
      "- **Probability Table**: \n",
      "  - Use `'probability.ranking' = 1` unless all ranking probabilities are requested.\n",
      "  - Use `'probability.classifier_name='lc_classifier'`.\n",
      "\n",
      "## General Guidelines\n",
      "\n",
      "- Select all columns if none are specified using `SELECT *`.\n",
      "- Maintain exact column and class names unless explicitly instructed otherwise.\n",
      "- Use `INNER JOIN` or sub-queries judiciously to avoid slowing down the query.\n",
      "\n",
      "## Output Format\n",
      "\n",
      "Answer ONLY with the SQL query in the following format:\n",
      "```sql\n",
      "SQL_QUERY_HERE\n",
      "```\n",
      "Include comments using SQL format within the query if necessary.\n",
      "\n",
      "# Steps\n",
      "\n",
      "- Use existing SQL code as a reference, adapting conditions as needed.\n",
      "- Join steps using the same table unless separate sub-queries are necessary.\n",
      "\n",
      "# Examples\n",
      "\n",
      "[Example 1: User Request]\n",
      "- User wants data from the 'object' and 'probability' tables with specific conditions.\n",
      "\n",
      "[Example 1: SQL Query]\n",
      "```sql\n",
      "SELECT * FROM object\n",
      "INNER JOIN probability ON object.id = probability.object_id\n",
      "WHERE probability.ranking = 1\n",
      "AND probability.classifier_name = 'lc_classifier';\n",
      "```\n",
      "\n",
      "# Notes\n",
      "\n",
      "- Be precise with conditions and aware of the table relationships.\n",
      "- Ensure the query is optimized for performance.\n"
     ]
    }
   ],
   "source": [
    "new_med_gen_prompt = generate_prompt(sql_gen.get_prompt( difficulty_class='medium', tables_list=[\"table1\", \"table2\"]).split(\"SQL Query Generation Prompt: \")[1])\n",
    "print(new_med_gen_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2eb8a6",
   "metadata": {},
   "source": [
    "Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cc3be83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Table table1 not found in the database schema.\n",
      "Warning: Table table2 not found in the database schema.\n",
      "Warning: Table table1 not found in the database schema.\n",
      "Warning: Table table2 not found in the database schema.\n",
      "\n",
      "\n",
      "# Your task is to DECOMPOSE the user request into a series of steps required to generate a PostgreSQL query that will be used for retrieving requested information from the ALeRCE database.\n",
      "For this, outline a detailed decomposition plan for its systematic resolution, describing and breaking down the problem into subtasks and/or subqueries. \n",
      "Be careful to put all the information and details needed in the description, like conditions, the table and column names, and the details of the database schema. This is very important to ensure the query is optimal and accurate.\n",
      "Take in consideration the advices, conditions and names from \"General Context\" and details of the database, or the query will not be optimal.\n",
      "The request is a very difficult and advanced query, so you will need to use JOINs, INTERSECTs and UNIONs statements, together with Nested queries. It is very important that you give every possible detail in each step, describing the statements and the nested-queries that are required.\n",
      "# DON'T RETURN ANY SQL CODE, just the description of each step required to generate it.\n",
      "Creating a decomposition plan to generate a PostgreSQL query for retrieving information from the ALeRCE astronomy broker database involves several steps. ALeRCE (Automatic Learning for the Rapid Classification of Events) is a system designed to classify large amounts of astronomical data, typically from surveys like the Zwicky Transient Facility (ZTF). To create a detailed and understandable plan, follow these steps:\n",
      "\n",
      "1. **Understand the Database Schema:**\n",
      "   - Obtain the database schema, which includes tables, columns, data types, relationships, and constraints.\n",
      "   - Identify the relevant tables and columns that contain the information you need.\n",
      "\n",
      "2. **Define the Information Needed:**\n",
      "   - Clearly specify what information you want to retrieve. For example, you might be interested in transient events, their classifications, light curves, or cross-matches with other catalogs.\n",
      "   - Determine the level of detail required (e.g., specific time ranges, magnitude limits, or particular sky regions).\n",
      "\n",
      "3. **Formulate the Query Requirements:**\n",
      "   - Decide on the selection criteria (e.g., date, magnitude, classification confidence).\n",
      "   - Determine if you need to join multiple tables and how they are related.\n",
      "   - Consider if you need to aggregate data (e.g., average magnitudes, count of events).\n",
      "\n",
      "4. **Design the Query:**\n",
      "   - Start with the main table that contains the bulk of the information you need.\n",
      "   - Use `JOIN` clauses to combine related tables based on common keys.\n",
      "   - Apply `WHERE` clauses to filter the data according to your criteria.\n",
      "   - Use `GROUP BY` and aggregate functions if necessary.\n",
      "   - Decide on the sorting order of the results using `ORDER BY`.\n",
      "\n",
      "5. **Document the Query:**\n",
      "   - Write comments within the SQL code to explain the purpose of different parts of the query.\n",
      "   - Create external documentation that describes the query's purpose, the information it retrieves, and any assumptions or limitations.\n",
      "\n",
      "Remember that the actual query will depend on the specific schema and requirements of the ALeRCE database. Always test your queries to ensure they perform as expected and return accurate results. \n",
      "\n",
      "# General context about the database:\n",
      "\n",
      "## ALeRCE Pipeline Details\n",
      "- Stamp Classifier (denoted as ‘stamp_classifier’): All alerts related to new objects undergo stamp-based classification.\n",
      "- Light Curve Classifier (denoted as ‘lc_classifier’): A balanced hierarchical random forest classifier employing four models and 15 classes.\n",
      "- The first hierarchical classifier has three classes: [Periodic, Stochastic, Transient], denoted as ‘lc_classifier_top.’\n",
      "- Three additional classifiers specialize in different spatial object types: Periodic, Transient, and Stochastic, denoted as ‘lc_classifier_periodic,’ ‘lc_classifier_transient,’ and ‘lc_classifier_stochastic,’ respectively.\n",
      "- The 15 classes are separated for each object type:\n",
      "  - Transient: [SNe Ia ('SNIa'), SNe Ib/c ('SNIbc'), SNe II ('SNII'), and Super Luminous SNe ('SLSN')].\n",
      "  - Stochastic: [Active Galactic Nuclei ('AGN'), Quasi Stellar Object ('QSO'), 'Blazar', Cataclysmic Variable/Novae ('CV/Nova'), and Young Stellar Object ('YSO')].\n",
      "  - Periodic: [Delta Scuti ('DSCT'), RR Lyrae ('RRL'), Cepheid ('CEP'), Long Period Variable ('LPV'), Eclipsing Binary ('E'), and other periodic objects ('Periodic-Other')].\n",
      "## Spatial Object Types by Classifier\n",
      "- classifier_name=('lc_classifier', 'lc_classifier_top', 'lc_classifier_transient', 'lc_classifier_stochastic', 'lc_classifier_periodic', 'stamp_classifier')\n",
      "- Classes in 'lc_classifier'= ('SNIa', 'SNIbc', 'SNII', 'SLSN', 'QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO', 'LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other')\n",
      "- Classes in 'lc_classifier_top'= ('Transient', 'Stochastic', 'Periodic')\n",
      "- Classes in 'lc_classifier_transient'= ('SNIa', 'SNIbc', 'SNII', 'SLSN')\n",
      "- Classes in 'lc_classifier_stochastic'= ('QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO')\n",
      "- Classes in 'lc_classifier_periodic'= ('LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other')\n",
      "- Classes in 'stamp_classifier'= ('SN', 'AGN', 'VS', 'asteroid', 'bogus')\n",
      "\n",
      "If a query involves selecting astronomical objects based on their celestial coordinates, the Q3C extension for PostgreSQL provides a suite of specialized functions optimized for this purpose. \n",
      "These functions enable efficient spatial queries on large astronomical datasets, including:\n",
      "- Retrieving the angular distance between two objects,\n",
      "- Determining whether two objects lie within a specified angular separation,\n",
      "- Identifying objects located within a circular region, elliptical region, or arbitrary spherical polygon on the celestial sphere.\n",
      "\n",
      "The following functions are available in the Q3C extension:\n",
      "- q3c_dist(ra1, dec1, ra2, dec2) -- returns the distance in degrees between two points (ra1,dec1) and (ra2,dec2)\n",
      "- q3c_join(ra1, dec1, ra2, dec2, radius)  -- returns true if (ra1, dec1) is within radius spherical distance of (ra2, dec2).\n",
      "- q3c_ellipse_join(ra1, dec1, ra2, dec2, major, ratio, pa) -- like q3c_join, except (ra1, dec1) have to be within an ellipse with semi-major axis major, the axis ratio ratio and the position angle pa (from north through east)\n",
      "- q3c_radial_query(ra, dec, center_ra, center_dec, radius) -- returns true if ra, dec is within radius degrees of center_ra, center_dec. This is the main function for cone searches.\n",
      "- q3c_ellipse_query(ra, dec, center_ra, center_dec, maj_ax, axis_ratio, PA ) -- returns true if ra, dec is within the ellipse from center_ra, center_dec. The ellipse is specified by semi-major axis, axis ratio and positional angle.\n",
      "- q3c_poly_query(ra, dec, poly) -- returns true if ra, dec is within the spherical polygon specified as an array of right ascensions and declinations. Alternatively poly can be an PostgreSQL polygon type.\n",
      "\n",
      "It can be useful to define a set of astronomical sources with associated coordinates directly in a SQL query, you can use a WITH clause such as:\n",
      "    WITH catalog (source_id, ra, dec) AS (\n",
      "        VALUES ('source_name', ra_value, dec_value),\n",
      "        ...)\n",
      "This construct creates a temporary inline table named catalog, which can be used in subsequent queries for cross-matching or spatial filtering operations.\n",
      "This is useful for defining a set of astronomical sources with associated coordinates directly in a SQL query. Then, you can use the Q3C functions to perform spatial queries on this temporary table (e.g. 'FROM catalog c').\n",
      "Be careful with the order of the input parameters in the Q3C functions, as they are not always the same as the order of the columns in the catalog table.\n",
      "\n",
      "\n",
      "# The Database has the following tables that can be used to generate the SQL query:\n",
      "\n",
      "\n",
      "\n",
      "# Important details about the database required for the query:\n",
      "\n",
      "## DEFAULT CONDITIONS YOU NEED TO SET\n",
      "### IF THE 'probability' TABLE is used, use always the next conditions, unless the user explicitly specifies different probability conditions.\n",
      "- 'probability.ranking' = 1 ; this only return the most likely probabilities.\n",
      "- 'probability.classifier_name='lc_classifier'\n",
      "### IF THE 'feature' TABLE is used with 2 or more features, you need to take the following steps, because it is a transposed table (each feature is in a different row).\n",
      "I. Create a sub-query using the 'probability' TABLE filtering the desired objects.\n",
      "II. For each feature, you have to make a sub-query retrieving the specific feature adding the condition of its value, taking only the oids in the 'probability' sub-query with an INNER JOIN inside each 'feature' sub-query to retrieve only the features associated with the desired spatial objects.\n",
      "III. Make an UNION between the sub-queries of each feature from step II\n",
      "IV. Make an INTERSECT between the sub-queries of each feature from step II\n",
      "V. Filter the 'UNION' query from step III selecting only the 'oids' that are in the 'INTERSECT' query from step IV\n",
      "VI. Add the remaining conditions to the final result of step V, using the 'probability' sub-query from step I.\n",
      "### GENERAL\n",
      "- If the user doesn't specify explicit columns or information that is not in a column, choose all the columns, for example by using the “SELECT *” SQL statement.\n",
      "- Use the exact table and column names as they are in the database. This is crucial for the query to work correctly.\n",
      "- Use the exact class names as they are in the database, marked with single quotes, for example, 'SNIa'.\n",
      "\n",
      "# If you need to use 2 or 3 tables, try using a sub-query or INNER JOIN over 'probability' TABLE or 'object' TABLE, or an INNER JOIN between 'probabbility' and 'object', or over an INNER JOIN between 'probability', 'object' and 'magstat', if it is necessary (priority in this order).\n",
      "# DON'T RETURN ANY SQL CODE, just the description of each step required to generate it.\n",
      "\n",
      "\n",
      "\n",
      " SQL Query Generation Prompt: \n",
      "\n",
      "\n",
      "# As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\n",
      "The tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\n",
      "ALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \n",
      "Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n",
      "\n",
      "\n",
      "# The Database has the following tables that can be used to generate the SQL query:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## DEFAULT CONDITIONS YOU NEED TO SET\n",
      "### IF THE 'probability' TABLE is used, use always the next conditions, unless the user explicitly specifies different probability conditions.\n",
      "- 'probability.ranking' = 1 ; this only return the most likely probabilities, if the user request all ranking probabilities, don't use it.\n",
      "- 'probability.classifier_name='lc_classifier'\n",
      "### IF THE 'feature' TABLE is used with 2 or more features, you need to take the following steps, because it is a transposed table (each feature is in a different row).\n",
      "I. Create a sub-query using the 'probability' TABLE filtering the desired objects.\n",
      "II. For each feature, you have to make a sub-query retrieving the specific feature adding the condition of its value, taking only the oids in the 'probability' sub-query with an INNER JOIN inside each 'feature' sub-query to retrieve only the features associated with the desired spatial objects.\n",
      "III. Make an UNION between the sub-queries of each feature from step II\n",
      "IV. Make an INTERSECT between the sub-queries of each feature from step II\n",
      "V. Filter the 'UNION' query from step III selecting only the 'oids' that are in the 'INTERSECT' query from step IV\n",
      "VI. Add the remaining conditions to the final result of step V, using the 'probability' sub-query from step I.\n",
      "### GENERAL\n",
      "### Important points to consider\n",
      "- If the user doesn't specify explicit columns or information that is not in a column, choose all the columns, for example by using the “SELECT *” SQL statement, from ALL the tables used, including the ones from the sub-queries.\n",
      "- Mantain the EXACT COLUMN names as they are in the database, unless the user explicitly asks you to do so in the request, giving you the new name to use. This is crucial for the query to work correctly.\n",
      "- Mantain the exact class names as they are in the database, marked with single quotes, for example, 'SNIa'.\n",
      "\n",
      "\n",
      "# If you need to use 2 or 3 tables, try using a sub-query or INNER JOIN over 'probability' TABLE or 'object' TABLE, or an INNER JOIN between 'probabbility' and 'object', or over an INNER JOIN between 'probability', 'object' and 'magstat', if it is necessary (priority in this order).\n",
      "# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n",
      "# Finally, join all the steps in a final query, with the following format: \n",
      "```sql [FINAL QUERY HERE] ```\n",
      "DON'T include anything else inside and after your FINAL answer.\n",
      "\n",
      "# Use the next decomposed planification to write the query:\n",
      "[plan]\n",
      "# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n",
      "# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sql_gen.get_prompt( difficulty_class='advanced', tables_list=[\"table1\", \"table2\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81a71f3",
   "metadata": {},
   "source": [
    "# Schema Linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0727fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "OPENAI_APIKEY = \"your_api_key_here\"\n",
    "# set API key\n",
    "client = OpenAI(api_key=OPENAI_APIKEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9837be7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/jespejo/miniconda3/envs/llm:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "openai                    1.97.1                   pypi_0    pypi\n",
      "# packages in environment at /home/jespejo/miniconda3/envs/llm:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "httpx                     0.28.1                   pypi_0    pypi\n"
     ]
    }
   ],
   "source": [
    "!conda list openai\n",
    "!conda list httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d0e4ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==1.97.1\n",
      "  Using cached openai-1.97.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.97.1) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.97.1) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.97.1) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.97.1) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.97.1) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.97.1) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.97.1) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.97.1) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai==1.97.1) (3.10)\n",
      "Requirement already satisfied: certifi in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.97.1) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.97.1) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.97.1) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.97.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.97.1) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.97.1) (0.4.1)\n",
      "Using cached openai-1.97.1-py3-none-any.whl (764 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.55.0\n",
      "    Uninstalling openai-1.55.0:\n",
      "      Successfully uninstalled openai-1.55.0\n",
      "Successfully installed openai-1.97.1\n",
      "Collecting httpx==0.28.1\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: anyio in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpx==0.28.1) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpx==0.28.1) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpx==0.28.1) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpx==0.28.1) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpcore==1.*->httpx==0.28.1) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from anyio->httpx==0.28.1) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from anyio->httpx==0.28.1) (4.14.1)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Installing collected packages: httpx\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.2\n",
      "    Uninstalling httpx-0.27.2:\n",
      "      Successfully uninstalled httpx-0.27.2\n",
      "Successfully installed httpx-0.28.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==1.97.1\n",
    "!pip install httpx==0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad065ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pred_tables(schema_prompt: str, query: str, prompting: str):\n",
    "    if prompting == 'user':\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-11-20\",\n",
    "            temperature=0.0,\n",
    "            max_tokens=1000,\n",
    "            messages=[\n",
    "                # {\n",
    "                #     \"role\": \"system\",\n",
    "                #     \"content\": schema_prompt,\n",
    "                # },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": schema_prompt + \\\n",
    "                f\"\\nThe user request is the following: {query}\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "    elif prompting == 'system':\n",
    "        completion = client.chat.completions.create(\n",
    "                model=\"gpt-4o-2024-11-20\",\n",
    "                temperature=0.0,\n",
    "                max_tokens=1000,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": schema_prompt,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": query,\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "    elif prompting == 'both':\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-11-20\",\n",
    "            temperature=0.0,\n",
    "            messages=[\n",
    "                {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": schema_prompt + \\\n",
    "                    f\"\\nThe user request is the following: {query}\",\n",
    "                },\n",
    "                {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": query,\n",
    "                    },\n",
    "                ],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompting type\")\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0c20a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Given the user request, select the tables needed to generate a SQL query\n",
      "# The Database has the following tables:\n",
      "\n",
      "TABLE \"object\": contains basic filter and time–aggregated statistics such as location, number of observations, and the times of first and last detection.\n",
      "TABLE \"probability\": classification probabilities associated to a given object, classifier, and class. Contain the object classification probabilities, including those from the stamp and light curve classifiers, and from different versions of these classifiers.\n",
      "TABLE \"feature\": contains the object light curve statistics and other features used for ML classification and which are stored as json files in our database.\n",
      "TABLE \"magstat\": contains time–aggregated statistics separated by filter, such as the average magnitude, the initial magnitude change rate, number of detections, etc.\n",
      "TABLE \"non_detection\": contains the limiting magnitudes of previous non–detections separated by filter.\n",
      "TABLE \"detection\": contains the object light curves including their difference and corrected magnitudes and associated errors separated by filter.\n",
      "TABLE \"step\": contains the different pipeline steps and their versions.\n",
      "TABLE \"taxonomy\": contains details about the different taxonomies used in our stamp and light curve classifiers, which can evolve with time.\n",
      "TABLE \"feature_version\": contains the version of the feature extraction and preprocessing steps used to generate the features.\n",
      "TABLE \"xmatch\": contains the object cross–matches and associated cross–match catalogs.\n",
      "TABLE \"allwise\": contains the AllWISE catalog information for the objects.\n",
      "TABLE \"dataquality\": detailed object information regarding the quality of the data\n",
      "TABLE \"gaia_ztf\": GAIA objects near detected ZTF objects\n",
      "TABLE \"ss_ztf\": known solar system objects near detected objects\n",
      "TABLE \"ps1_ztf\": PanSTARRS objects near detected ZTF objects\n",
      "TABLE \"reference\": properties of the reference images used to build templates\n",
      "TABLE \"pipeline\": information about the different pipeline steps and their versions\n",
      "TABLE \"information_schema.tables\": information about the database tables and columns\n",
      "TABLE \"forced_photometry\": contains the forced photometry measurements for each object, including the object position, magnitude, and associated errors, and the photometry of the reference image.\n",
      "\n",
      "\n",
      "\n",
      "# Give ONLY the TABLES that are needed to generate the SQL query.\n",
      "# Give the answer in the following format: ['table1', 'table2', 'table3', ...]. For example, if the TABLES needed for the user request are TABLE object and TABLE taxonomy, then you should type: ['object', 'taxonomy']\n",
      "# Just give the tables and ignore any other task given in the request given as \"request\".\n",
      "# Remember to use the exact name of the TABLES, as they are written in the DATABASE SCHEMA. Do NOT create table names.\n",
      "# If you think that no table mentioned above is needed, then type: \"\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prompts.SchemaLinkingPrompts import sl_final_instructions_v2, prompt_schema_linking_v0\n",
    "from prompts.DBSchemaPrompts import alerce_tables_desc\n",
    "\n",
    "schema_linking_prompt = prompt_schema_linking_v0.format(db_schema=alerce_tables_desc, final_instructions=sl_final_instructions_v2)\n",
    "print(schema_linking_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe56fb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Given the user request, select the tables needed to generate a SQL query\\n# The Database has the following tables:\\n\\nTABLE \"object\": contains basic filter and time–aggregated statistics such as location, number of observations, and the times of first and last detection.\\nTABLE \"probability\": classification probabilities associated to a given object, classifier, and class. Contain the object classification probabilities, including those from the stamp and light curve classifiers, and from different versions of these classifiers.\\nTABLE \"feature\": contains the object light curve statistics and other features used for ML classification and which are stored as json files in our database.\\nTABLE \"magstat\": contains time–aggregated statistics separated by filter, such as the average magnitude, the initial magnitude change rate, number of detections, etc.\\nTABLE \"non_detection\": contains the limiting magnitudes of previous non–detections separated by filter.\\nTABLE \"detection\": contains the object light curves including their difference and corrected magnitudes and associated errors separated by filter.\\nTABLE \"step\": contains the different pipeline steps and their versions.\\nTABLE \"taxonomy\": contains details about the different taxonomies used in our stamp and light curve classifiers, which can evolve with time.\\nTABLE \"feature_version\": contains the version of the feature extraction and preprocessing steps used to generate the features.\\nTABLE \"xmatch\": contains the object cross–matches and associated cross–match catalogs.\\nTABLE \"allwise\": contains the AllWISE catalog information for the objects.\\nTABLE \"dataquality\": detailed object information regarding the quality of the data\\nTABLE \"gaia_ztf\": GAIA objects near detected ZTF objects\\nTABLE \"ss_ztf\": known solar system objects near detected objects\\nTABLE \"ps1_ztf\": PanSTARRS objects near detected ZTF objects\\nTABLE \"reference\": properties of the reference images used to build templates\\nTABLE \"pipeline\": information about the different pipeline steps and their versions\\nTABLE \"information_schema.tables\": information about the database tables and columns\\nTABLE \"forced_photometry\": contains the forced photometry measurements for each object, including the object position, magnitude, and associated errors, and the photometry of the reference image.\\n\\n\\n\\n# Give ONLY the TABLES that are needed to generate the SQL query.\\n# Give the answer in the following format: [\\'table1\\', \\'table2\\', \\'table3\\', ...]. For example, if the TABLES needed for the user request are TABLE object and TABLE taxonomy, then you should type: [\\'object\\', \\'taxonomy\\']\\n# Just give the tables and ignore any other task given in the request given as \"request\".\\n# Remember to use the exact name of the TABLES, as they are written in the DATABASE SCHEMA. Do NOT create table names.\\n# If you think that no table mentioned above is needed, then type: \"\"\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_linking_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4916e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_linking_prompt_V2 = '''\n",
    "# Given the user request, select the tables needed to generate a SQL query\n",
    "# The Database has the following tables:\n",
    "TABLE \"object\": contains basic filter and time-aggregated statistics such as location, number of observations, and the times of first and last detection.\n",
    "TABLE \"probability\": classification probabilities associated to a given object, classifier, and class. Contain the object classification probabilities, including those from the stamp and light curve classifiers, and from different versions of these classifiers.\n",
    "TABLE \"feature\": contains the object light curve statistics and other features used for ML classification and which are stored as json files in our database.\n",
    "TABLE \"magstat\": contains time-aggregated statistics separated by filter, such as the average magnitude, the initial magnitude change rate, number of detections, etc.\n",
    "TABLE \"non_detection\": contains the limiting magnitudes of previous non-detections separated by filter.\n",
    "TABLE \"detection\": contains the object light curves including their difference and corrected magnitudes and associated errors separated by filter.\n",
    "TABLE \"step\": contains the different pipeline steps and their versions.\n",
    "TABLE \"taxonomy\": contains details about the different taxonomies used in our stamp and light curve classifiers, which can evolve with time.\n",
    "TABLE \"feature_version\": contains the version of the feature extraction and preprocessing steps used to generate the features.\n",
    "TABLE \"xmatch\": contains the object cross-matches and associated cross-match catalogs.\n",
    "TABLE \"allwise\": contains the AllWISE catalog information for the objects.\n",
    "TABLE \"dataquality\": detailed object information regarding the quality of the data\n",
    "TABLE \"gaia_ztf\": GAIA objects near detected ZTF objects\n",
    "TABLE \"ss_ztf\": known solar system objects near detected objects\n",
    "TABLE \"ps1_ztf\": PanSTARRS objects near detected ZTF objects\n",
    "TABLE \"reference\": properties of the reference images used to build templates\n",
    "TABLE \"pipeline\": information about the different pipeline steps and their versions\n",
    "TABLE \"information_schema.tables\": information about the database tables and columns\n",
    "TABLE \"forced_photometry\": contains the forced photometry measurements for each object, including the object position, magnitude, and associated errors, and the photometry of the reference image.\n",
    "\n",
    "# Give ONLY the TABLES that are needed to generate the SQL query, nothing more\n",
    "# Give the answer in the following format: ['table1', 'table2', ...]\n",
    "# For example, if the TABLES needed for the user request are TABLE object and TABLE taxonomy, then you should type: ['object', 'taxonomy']\n",
    "# Remember to use the exact name of the TABLES, as they are written in the DATABASE SCHEMA\n",
    "# Just give the tables and ignore any other task given in the request given as \"request\".\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4402b9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line:   # Given the user request, select the tables needed to generate a SQL query\n",
      "Line:   # The Database has the following tables:\n",
      "Line: - \n",
      "Line: - TABLE \"object\": contains basic filter and time–aggregated statistics such as location, number of observations, and the times of first and last detection.\n",
      "Line: ?                                               ^\n",
      "\n",
      "Line: + TABLE \"object\": contains basic filter and time-aggregated statistics such as location, number of observations, and the times of first and last detection.\n",
      "Line: ?                                               ^\n",
      "\n",
      "Line:   TABLE \"probability\": classification probabilities associated to a given object, classifier, and class. Contain the object classification probabilities, including those from the stamp and light curve classifiers, and from different versions of these classifiers.\n",
      "Line:   TABLE \"feature\": contains the object light curve statistics and other features used for ML classification and which are stored as json files in our database.\n",
      "Line: - TABLE \"magstat\": contains time–aggregated statistics separated by filter, such as the average magnitude, the initial magnitude change rate, number of detections, etc.\n",
      "Line: ?                               ^\n",
      "\n",
      "Line: + TABLE \"magstat\": contains time-aggregated statistics separated by filter, such as the average magnitude, the initial magnitude change rate, number of detections, etc.\n",
      "Line: ?                               ^\n",
      "\n",
      "Line: - TABLE \"non_detection\": contains the limiting magnitudes of previous non–detections separated by filter.\n",
      "Line: ?                                                                        ^\n",
      "\n",
      "Line: + TABLE \"non_detection\": contains the limiting magnitudes of previous non-detections separated by filter.\n",
      "Line: ?                                                                        ^\n",
      "\n",
      "Line:   TABLE \"detection\": contains the object light curves including their difference and corrected magnitudes and associated errors separated by filter.\n",
      "Line:   TABLE \"step\": contains the different pipeline steps and their versions.\n",
      "Line:   TABLE \"taxonomy\": contains details about the different taxonomies used in our stamp and light curve classifiers, which can evolve with time.\n",
      "Line:   TABLE \"feature_version\": contains the version of the feature extraction and preprocessing steps used to generate the features.\n",
      "Line: - TABLE \"xmatch\": contains the object cross–matches and associated cross–match catalogs.\n",
      "Line: ?                                          ^                            ^\n",
      "\n",
      "Line: + TABLE \"xmatch\": contains the object cross-matches and associated cross-match catalogs.\n",
      "Line: ?                                          ^                            ^\n",
      "\n",
      "Line:   TABLE \"allwise\": contains the AllWISE catalog information for the objects.\n",
      "Line:   TABLE \"dataquality\": detailed object information regarding the quality of the data\n",
      "Line:   TABLE \"gaia_ztf\": GAIA objects near detected ZTF objects\n",
      "Line:   TABLE \"ss_ztf\": known solar system objects near detected objects\n",
      "Line:   TABLE \"ps1_ztf\": PanSTARRS objects near detected ZTF objects\n",
      "Line:   TABLE \"reference\": properties of the reference images used to build templates\n",
      "Line:   TABLE \"pipeline\": information about the different pipeline steps and their versions\n",
      "Line:   TABLE \"information_schema.tables\": information about the database tables and columns\n",
      "Line:   TABLE \"forced_photometry\": contains the forced photometry measurements for each object, including the object position, magnitude, and associated errors, and the photometry of the reference image.\n",
      "Line:   \n",
      "Line: - \n",
      "Line: - \n",
      "Line: - # Give ONLY the TABLES that are needed to generate the SQL query.\n",
      "Line: ?                                                                 ^\n",
      "\n",
      "Line: + # Give ONLY the TABLES that are needed to generate the SQL query, nothing more\n",
      "Line: ?                                                                 ^^^^^^^^^^^^^^\n",
      "\n",
      "Line: + # Give the answer in the following format: ['table1', 'table2', ...]\n",
      "Line: - # Give the answer in the following format: ['table1', 'table2', 'table3', ...]. For example, if the TABLES needed for the user request are TABLE object and TABLE taxonomy, then you should type: ['object', 'taxonomy']\n",
      "Line: ?  ------------------------------------------------------------------------------\n",
      "\n",
      "Line: + # For example, if the TABLES needed for the user request are TABLE object and TABLE taxonomy, then you should type: ['object', 'taxonomy']\n",
      "Line: + # Remember to use the exact name of the TABLES, as they are written in the DATABASE SCHEMA\n",
      "Line:   # Just give the tables and ignore any other task given in the request given as \"request\".\n",
      "Line: - # Remember to use the exact name of the TABLES, as they are written in the DATABASE SCHEMA. Do NOT create table names.\n",
      "Line: - # If you think that no table mentioned above is needed, then type: \"\"\n"
     ]
    }
   ],
   "source": [
    "# Check difference between prompts\n",
    "import difflib\n",
    "\n",
    "d = difflib.Differ()\n",
    "diff = list(d.compare(schema_linking_prompt.strip().splitlines(), tables_linking_prompt_V2.strip().splitlines()))\n",
    "\n",
    "for line in diff:\n",
    "    print(\"Line:\", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef135255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 14\n",
    "query='''For the next list of oids: ['ZTF23aavzgjg' 'ZTF23aaynzyk' 'ZTF23aavqxos' 'ZTF23aaknyni'\\n 'ZTF23aavsdtc' 'ZTF18aandkua' 'ZTF23aaxfewt' 'ZTF23aavshwi'\\n 'ZTF22aawasao' 'ZTF23aaxgvnt'], return the unique object identifier, candidate identifier, filter identifier, modified julian date, magnitud, magnitud error, whether the object has stamps, deep learning real bogus score, the star galaxy score of the nearest object, and the distance to the nearest source in panstarrs for objects that have a deep learning real bogus score greater than 0.5 and that either have a star galaxy score less than 0.5 or a distance to the nearest panstarrs source smaller than 1 arcsec.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "784cc3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['detection', 'ps1_ztf']\n",
      "Second try:  ['detection', 'ps1_ztf']\n",
      "Third try:  ['detection', 'ps1_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f647925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'detection', 'probability', 'ps1_ztf']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second try:  ['object', 'detection', 'probability', 'ps1_ztf']\n",
      "Third try:  ['object', 'detection', 'probability', 'ps1_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "742857e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'detection', 'probability', 'ps1_ztf']\n",
      "Second try:  ['object', 'detection', 'probability', 'ps1_ztf']\n",
      "Third try:  ['object', 'detection', 'probability', 'ps1_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54b20106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'detection', 'probability', 'ps1_ztf']\n",
      "Second try:  ['object', 'detection', 'probability', 'ps1_ztf']\n",
      "Third try:  ['object', 'detection', 'probability', 'ps1_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebb6ba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'detection', 'probability', 'ps1_ztf']\n",
      "Second try:  ['object', 'detection', 'probability', 'ps1_ztf']\n",
      "Third try:  ['object', 'detection', 'probability', 'ps1_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff1290de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['detection', 'probability', 'ps1_ztf']\n",
      "Second try:  ['detection', 'ps1_ztf']\n",
      "Third try:  ['detection', 'probability', 'ps1_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d773ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same final instructions\n",
    "tables_linking_prompt_same = '''\n",
    "# Given the user request, select the tables needed to generate a SQL query\n",
    "# The Database has the following tables:\n",
    "TABLE \"object\": contains basic filter and time-aggregated statistics such as location, number of observations, and the times of first and last detection.\n",
    "TABLE \"probability\": classification probabilities associated to a given object, classifier, and class. Contain the object classification probabilities, including those from the stamp and light curve classifiers, and from different versions of these classifiers.\n",
    "TABLE \"feature\": contains the object light curve statistics and other features used for ML classification and which are stored as json files in our database.\n",
    "TABLE \"magstat\": contains time-aggregated statistics separated by filter, such as the average magnitude, the initial magnitude change rate, number of detections, etc.\n",
    "TABLE \"non_detection\": contains the limiting magnitudes of previous non-detections separated by filter.\n",
    "TABLE \"detection\": contains the object light curves including their difference and corrected magnitudes and associated errors separated by filter.\n",
    "TABLE \"step\": contains the different pipeline steps and their versions.\n",
    "TABLE \"taxonomy\": contains details about the different taxonomies used in our stamp and light curve classifiers, which can evolve with time.\n",
    "TABLE \"feature_version\": contains the version of the feature extraction and preprocessing steps used to generate the features.\n",
    "TABLE \"xmatch\": contains the object cross-matches and associated cross-match catalogs.\n",
    "TABLE \"allwise\": contains the AllWISE catalog information for the objects.\n",
    "TABLE \"dataquality\": detailed object information regarding the quality of the data\n",
    "TABLE \"gaia_ztf\": GAIA objects near detected ZTF objects\n",
    "TABLE \"ss_ztf\": known solar system objects near detected objects\n",
    "TABLE \"ps1_ztf\": PanSTARRS objects near detected ZTF objects\n",
    "TABLE \"reference\": properties of the reference images used to build templates\n",
    "TABLE \"pipeline\": information about the different pipeline steps and their versions\n",
    "TABLE \"information_schema.tables\": information about the database tables and columns\n",
    "TABLE \"forced_photometry\": contains the forced photometry measurements for each object, including the object position, magnitude, and associated errors, and the photometry of the reference image.\n",
    "\n",
    "# Give ONLY the TABLES that are needed to generate the SQL query.\n",
    "# Give the answer in the following format: ['table1', 'table2', 'table3', ...]. For example, if the TABLES needed for the user request are TABLE object and TABLE taxonomy, then you should type: ['object', 'taxonomy']\n",
    "# Just give the tables and ignore any other task given in the request given as \"request\".\n",
    "# Remember to use the exact name of the TABLES, as they are written in the DATABASE SCHEMA. Do NOT create table names.\n",
    "# If you think that no table mentioned above is needed, then type: \"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b84aca47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'detection', 'probability', 'ps1_ztf']\n",
      "Second try:  ['object', 'detection', 'probability', 'ps1_ztf']\n",
      "Third try:  ['object', 'detection', 'probability', 'ps1_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd82e4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'detection', 'probability', 'ps1_ztf']\n",
      "Second try:  ['detection', 'object', 'probability', 'ps1_ztf']\n",
      "Third try:  ['detection', 'object', 'probability', 'ps1_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'user'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c282ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 19\n",
    "query='''Write a script in PostgreSQL that return objects which appeared between march 1st and april 1st of 2021, which have at most one detection, and which are classified as asteroids by the stamp classifier with a probability greater than 0.7. The query must return the columns oid, meanra, meandec, ndet, firstMJD, class name and probability.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7818c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability']\n",
      "Second try:  ['object', 'detection', 'probability', 'taxonomy']\n",
      "Third try:  ['object', 'probability']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "845cb6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability']\n",
      "Second try:  ['object', 'probability']\n",
      "Third try:  ['object', 'probability']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c37e09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability']\n",
      "Second try:  ['object', 'probability']\n",
      "Third try:  ['object', 'probability']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4fffe586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability']\n",
      "Second try:  ['object', 'probability']\n",
      "Third try:  ['object', 'probability']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c339abff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 12\n",
    "query='''Give me all the SNe that first occurred between february 13 and september 10 and that are within the polygon defined by the following coordinates '((-20, -20), (-2, -20), (20, 1), (10, 10))'::polygon  . Return the oids, the mean ra and dec.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da5dcf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object']\n",
      "Second try:  ['object']\n",
      "Third try:  ['object']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b993f432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object']\n",
      "Second try:  ['object']\n",
      "Third try:  ['object']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c393864a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object']\n",
      "Second try:  ['object']\n",
      "Third try:  ['object']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60edb840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object']\n",
      "Second try:  ['object']\n",
      "Third try:  ['object']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "364ec8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 104\n",
    "query='''Find at most 30 ZTF objects that have a probability larger than 0.9 of being an asteroid in the stamp classifier version 'stamp_classifier_1.0.4'. Return the following columns: identifier of the ZTF and candidate, distance from the nearest Solar System object, MPC archive magnitude and name. Include also the following columns related to each candidate in the output table: filter identifier, FWHM from SExtractor, number of PS1 calibrators used, and exposure time'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40352640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['probability', 'ss_ztf', 'detection']\n",
      "Second try:  ['probability', 'ss_ztf', 'detection']\n",
      "Third try:  ['probability', 'ss_ztf', 'detection']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b679ab1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'ss_ztf', 'detection']\n",
      "Second try:  ['object', 'probability', 'ss_ztf', 'detection']\n",
      "Third try:  ['object', 'probability', 'ss_ztf', 'detection']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc0a9108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'ss_ztf', 'detection']\n",
      "Second try:  ['object', 'probability', 'ss_ztf', 'detection']\n",
      "Third try:  ['object', 'probability', 'ss_ztf', 'detection']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "005872bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'ss_ztf', 'detection']\n",
      "Second try:  ['object', 'probability', 'ss_ztf', 'detection']\n",
      "Third try:  ['object', 'probability', 'ss_ztf', 'detection']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0965c71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['probability', 'ss_ztf', 'detection']\n",
      "Second try:  ['probability', 'ss_ztf', 'detection']\n",
      "Third try:  ['probability', 'ss_ztf', 'detection']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef1989d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'ss_ztf', 'detection']\n",
      "Second try:  ['object', 'probability', 'ss_ztf', 'detection']\n",
      "Third try:  ['object', 'probability', 'ss_ztf', 'detection']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'user'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb120e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 90\n",
    "query='''Find at most 100 ZTF objects that have a multiband period lower than 5 days in the 'lc_classifier_1.2.1-P' feature version. Return all columns from the 'probability' table for such objects, including only data for the light curve classifier, with rankings either 1 or 2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7029888d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['probability', 'feature_version']\n",
      "Second try:  ['probability', 'feature_version']\n",
      "Third try:  ['probability', 'feature', 'feature_version']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d766361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['feature', 'feature_version', 'probability']\n",
      "Second try:  ['feature', 'feature_version', 'probability']\n",
      "Third try:  ['feature', 'feature_version', 'probability']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6c37048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['probability', 'feature', 'feature_version']\n",
      "Second try:  ['feature', 'feature_version', 'probability']\n",
      "Third try:  ['feature', 'feature_version', 'probability']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "23d6f914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['feature', 'feature_version', 'probability']\n",
      "Second try:  ['object', 'feature', 'feature_version', 'probability']\n",
      "Third try:  ['object', 'feature', 'feature_version', 'probability']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1c4f67fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'feature', 'feature_version', 'probability']\n",
      "Second try:  ['object', 'feature', 'feature_version', 'probability']\n",
      "Third try:  ['object', 'feature', 'feature_version', 'probability']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6894623a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['probability', 'feature', 'feature_version']\n",
      "Second try:  ['probability', 'feature', 'feature_version']\n",
      "Third try:  ['probability', 'feature', 'feature_version']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46bb0ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['feature', 'feature_version', 'probability']\n",
      "Second try:  ['feature', 'feature_version', 'probability']\n",
      "Third try:  ['feature', 'feature_version', 'probability']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'user'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fdd0d5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 34\n",
    "query='''Given this list of oids ['ZTF17aaadpsi' 'ZTF19aaduncs' 'ZTF18abnvehl' 'ZTF19abrqsxy' 'ZTF19aaduodl' 'ZTF19aadovdv' 'ZTF18aammkke' 'ZTF18abtriul' 'ZTF17aabwtky' 'ZTF18abwjpfy'], write a PostgresSQL script that returns the information of the features 'Amplitude' or 'Multiband_period' associated with the objects in the list.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6fcfdc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['feature']\n",
      "Second try:  ['feature']\n",
      "Third try:  ['feature']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b2cfdc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['feature']\n",
      "Second try:  ['feature']\n",
      "Third try:  ['feature']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1d5e14f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['feature']\n",
      "Second try:  ['feature']\n",
      "Third try:  ['object', 'feature']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9fabebf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'feature']\n",
      "Second try:  ['object', 'feature']\n",
      "Third try:  ['object', 'feature']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c56e2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['feature']\n",
      "Second try:  ['feature']\n",
      "Third try:  ['feature']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9acc3228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['feature']\n",
      "Second try:  ['feature']\n",
      "Third try:  ['feature']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0d0dcc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['feature']\n",
      "Second try:  ['feature']\n",
      "Third try:  ['feature']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_same, query= query, prompting= 'user'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e73ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 27\n",
    "query='''Return the oids, meanra, meandec, ndet, firstmjd, deltajd, g_r_max, the classifier and class name, the ranking and the probability columns for each class of each object classified by the lc_classifier, with 100 or more detections that are most likely to be cepheid, with a probability larger than 0.76. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55e0ab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'taxonomy']\n",
      "Second try:  ['object', 'probability', 'taxonomy']\n",
      "Third try:  ['object', 'probability', 'taxonomy']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d318cf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability']\n",
      "Second try:  ['object', 'probability']\n",
      "Third try:  ['object', 'probability']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce9b01bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'taxonomy']\n",
      "Second try:  ['object', 'probability', 'taxonomy']\n",
      "Third try:  ['object', 'probability', 'taxonomy']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cc82b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability']\n",
      "Second try:  ['object', 'probability']\n",
      "Third try:  ['object', 'probability']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cb1af41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 67\n",
    "query='''For objects with ZTF identifiers 'ZTF18acxlskz', 'ZTF22aanppbi' and 'ZTF22abunrft', find all rows in the 'gaia_ztf' table where the nearest Gaia source lies within 1.5 arcsec of the ZTF object detection'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43177d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['gaia_ztf']\n",
      "Second try:  ['gaia_ztf']\n",
      "Third try:  ['gaia_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b828dcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['gaia_ztf']\n",
      "Second try:  ['gaia_ztf']\n",
      "Third try:  ['gaia_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f27726e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['gaia_ztf']\n",
      "Second try:  ['gaia_ztf']\n",
      "Third try:  ['gaia_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "178c30b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['gaia_ztf']\n",
      "Second try:  ['gaia_ztf']\n",
      "Third try:  ['gaia_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ae471f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['gaia_ztf']\n",
      "Second try:  ['gaia_ztf']\n",
      "Third try:  ['gaia_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'both'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'both'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'both'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9689aebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'gaia_ztf']\n",
      "Second try:  ['object', 'gaia_ztf']\n",
      "Third try:  ['object', 'gaia_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4dd7fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 83\n",
    "query='''Get all columns in the 'allwise' table for the ZTF object 'ZTF21aazqwxv' '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c5a8a03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['allwise']\n",
      "Second try:  ['allwise']\n",
      "Third try:  ['allwise']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "10281cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['allwise']\n",
      "Second try:  ['allwise']\n",
      "Third try:  ['allwise']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5547d5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['allwise']\n",
      "Second try:  ['allwise']\n",
      "Third try:  ['allwise']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2389afe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['allwise']\n",
      "Second try:  ['allwise']\n",
      "Third try:  ['allwise']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "41f3fac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['allwise']\n",
      "Second try:  ['allwise']\n",
      "Third try:  ['allwise']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 84\n",
    "query='''For the ZTF object 'ZTF19aascdol' get the following information about its ALLWISE match(es): identifier in ZTF and in the catalog, distance between counterparts, and magnitudes in filters WISE W1 to W4'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f35c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['allwise']\n",
      "Second try:  ['allwise']\n",
      "Third try:  ['allwise']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c239f6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'allwise']\n",
      "Second try:  ['object', 'allwise']\n",
      "Third try:  ['object', 'allwise']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639ccdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'allwise']\n",
      "Second try:  ['object', 'allwise']\n",
      "Third try:  ['object', 'allwise']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300649ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'allwise']\n",
      "Second try:  ['object', 'allwise']\n",
      "Third try:  ['object', 'allwise']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03208014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'allwise']\n",
      "Second try:  ['object', 'allwise']\n",
      "Third try:  ['object', 'allwise']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fa5b22",
   "metadata": {},
   "source": [
    "### simon openai version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42527d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/jespejo/miniconda3/envs/llm:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "openai                    1.55.0                   pypi_0    pypi\n",
      "# packages in environment at /home/jespejo/miniconda3/envs/llm:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "httpx                     0.27.2                   pypi_0    pypi\n"
     ]
    }
   ],
   "source": [
    "!conda list openai\n",
    "!conda list httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1242bdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==1.55.0 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (1.55.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.55.0) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.55.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.55.0) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.55.0) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.55.0) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.55.0) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.55.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from openai==1.55.0) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai==1.55.0) (3.10)\n",
      "Requirement already satisfied: certifi in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.55.0) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.55.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.55.0) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.55.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.55.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.55.0) (0.4.1)\n",
      "Collecting httpx==0.27.2\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: anyio in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpx==0.27.2) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpx==0.27.2) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpx==0.27.2) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpx==0.27.2) (3.10)\n",
      "Requirement already satisfied: sniffio in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpx==0.27.2) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.16 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from httpcore==1.*->httpx==0.27.2) (0.16.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /home/jespejo/miniconda3/envs/llm/lib/python3.11/site-packages (from anyio->httpx==0.27.2) (4.14.1)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: httpx\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "Successfully installed httpx-0.27.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==1.55.0\n",
    "!pip install httpx==0.27.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e5dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "OPENAI_APIKEY = \"your_api_key_here\"\n",
    "# set API key\n",
    "client = OpenAI(api_key=OPENAI_APIKEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa8dc98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pred_tables(schema_prompt: str, query: str, prompting: str):\n",
    "    if prompting == 'user':\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-11-20\",\n",
    "            temperature=0.0,\n",
    "            max_tokens=1000,\n",
    "            messages=[\n",
    "                # {\n",
    "                #     \"role\": \"system\",\n",
    "                #     \"content\": schema_prompt,\n",
    "                # },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": schema_prompt + \\\n",
    "                f\"\\nThe user request is the following: {query}\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "    elif prompting == 'system':\n",
    "        completion = client.chat.completions.create(\n",
    "                model=\"gpt-4o-2024-11-20\",\n",
    "                temperature=0.0,\n",
    "                max_tokens=1000,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": schema_prompt,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": query,\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "    elif prompting == 'both':\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-11-20\",\n",
    "            temperature=0.0,\n",
    "            messages=[\n",
    "                {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": schema_prompt + \\\n",
    "                    f\"\\nThe user request is the following: {query}\",\n",
    "                },\n",
    "                {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": query,\n",
    "                    },\n",
    "                ],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompting type\")\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf38c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 67\n",
    "query='''For objects with ZTF identifiers 'ZTF18acxlskz', 'ZTF22aanppbi' and 'ZTF22abunrft', find all rows in the 'gaia_ztf' table where the nearest Gaia source lies within 1.5 arcsec of the ZTF object detection'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b89bb5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['gaia_ztf']\n",
      "Second try:  ['gaia_ztf']\n",
      "Third try:  ['gaia_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c093ff20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['gaia_ztf']\n",
      "Second try:  ['gaia_ztf']\n",
      "Third try:  ['gaia_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3e978e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['gaia_ztf']\n",
      "Second try:  ['gaia_ztf']\n",
      "Third try:  ['gaia_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57761efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['gaia_ztf']\n",
      "Second try:  ['gaia_ztf']\n",
      "Third try:  ['gaia_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbfcb352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['gaia_ztf']\n",
      "Second try:  ['gaia_ztf']\n",
      "Third try:  ['gaia_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'both'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'both'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'both'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "079f53ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'gaia_ztf']\n",
      "Second try:  ['object', 'gaia_ztf']\n",
      "Third try:  ['object', 'gaia_ztf']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3518e113",
   "metadata": {},
   "source": [
    "### gpt-4.1-2025-04-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c17dbe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pred_tables(schema_prompt: str, query: str, prompting: str, model: str = \"gpt-4o-2024-11-20\"):\n",
    "    if prompting == 'user':\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "            max_tokens=4000,\n",
    "            messages=[\n",
    "                # {\n",
    "                #     \"role\": \"system\",\n",
    "                #     \"content\": schema_prompt,\n",
    "                # },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": schema_prompt + \\\n",
    "                f\"\\nThe user request is the following: {query}\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "    elif prompting == 'system':\n",
    "        completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                temperature=0.0,\n",
    "                max_tokens=4000,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": schema_prompt,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": query,\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "    elif prompting == 'both':\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "            messages=[\n",
    "                {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": schema_prompt + \\\n",
    "                    f\"\\nThe user request is the following: {query}\",\n",
    "                },\n",
    "                {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": query,\n",
    "                    },\n",
    "                ],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompting type\")\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "841dc064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 34\n",
    "query='''Given this list of oids ['ZTF17aaadpsi' 'ZTF19aaduncs' 'ZTF18abnvehl' 'ZTF19abrqsxy' 'ZTF19aaduodl' 'ZTF19aadovdv' 'ZTF18aammkke' 'ZTF18abtriul' 'ZTF17aabwtky' 'ZTF18abwjpfy'], write a PostgresSQL script that returns the information of the features 'Amplitude' or 'Multiband_period' associated with the objects in the list.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4d434f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['feature']\n",
      "Second try:  ['feature']\n",
      "Third try:  ['feature']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6c50ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['feature']\n",
      "Second try:  ['feature']\n",
      "Third try:  ['feature']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "31b1ec1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['feature']\n",
      "Second try:  ['feature']\n",
      "Third try:  ['feature']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "88bf2102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['feature']\n",
      "Second try:  ['feature']\n",
      "Third try:  ['feature']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "988864c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['feature']\n",
      "Second try:  ['feature']\n",
      "Third try:  ['feature']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0aa7a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 27\n",
    "query='''Return the oids, meanra, meandec, ndet, firstmjd, deltajd, g_r_max, the classifier and class name, the ranking and the probability columns for each class of each object classified by the lc_classifier, with 100 or more detections that are most likely to be cepheid, with a probability larger than 0.76. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b977cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'taxonomy']\n",
      "Second try:  ['object', 'probability', 'taxonomy']\n",
      "Third try:  ['object', 'probability', 'taxonomy']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f532871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability']\n",
      "Second try:  ['object', 'probability']\n",
      "Third try:  ['object', 'probability']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a27ac75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'taxonomy']\n",
      "Second try:  ['object', 'probability', 'taxonomy']\n",
      "Third try:  ['object', 'probability', 'taxonomy']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb6bc7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'taxonomy']\n",
      "Second try:  ['object', 'probability', 'taxonomy']\n",
      "Third try:  ['object', 'probability', 'taxonomy']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3cecbe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'taxonomy']\n",
      "Second try:  ['object', 'probability']\n",
      "Third try:  ['object', 'probability']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ee26f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 19\n",
    "query='''Write a script in PostgreSQL that return objects which appeared between march 1st and april 1st of 2021, which have at most one detection, and which are classified as asteroids by the stamp classifier with a probability greater than 0.7. The query must return the columns oid, meanra, meandec, ndet, firstMJD, class name and probability.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e6b011b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'detection', 'probability', 'taxonomy']\n",
      "Second try:  ['object', 'probability', 'taxonomy']\n",
      "Third try:  ['object', 'detection', 'probability', 'taxonomy']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0748182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'detection', 'probability', 'taxonomy']\n",
      "Second try:  ['object', 'detection', 'probability', 'taxonomy']\n",
      "Third try:  ['object', 'probability', 'taxonomy']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "938c73ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'taxonomy']\n",
      "Second try:  ['object', 'detection', 'probability', 'taxonomy']\n",
      "Third try:  ['object', 'probability', 'taxonomy']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d498a0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'taxonomy']\n",
      "Second try:  ['object', 'probability', 'taxonomy']\n",
      "Third try:  ['object', 'probability', 'taxonomy']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83cb82db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'taxonomy']\n",
      "Second try:  ['object', 'probability', 'taxonomy']\n",
      "Third try:  ['object', 'probability', 'taxonomy']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da7eec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 30\n",
    "query='''Return a Sql query to get the predicted class for each object according to the light curve classifier. The query should return the oid, the class name and the probability of the class for each object.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ececb147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'taxonomy']\n",
      "Second try:  ['probability', 'taxonomy']\n",
      "Third try:  ['probability', 'taxonomy']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "448d1905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'taxonomy']\n",
      "Second try:  ['object', 'probability', 'taxonomy']\n",
      "Third try:  ['object', 'probability', 'taxonomy']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= schema_linking_prompt, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2aead4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'taxonomy']\n",
      "Second try:  ['object', 'probability', 'taxonomy']\n",
      "Third try:  ['object', 'probability', 'taxonomy']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'system', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a4f20a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['object', 'probability', 'taxonomy']\n",
      "Second try:  ['object', 'probability', 'taxonomy']\n",
      "Third try:  ['object', 'probability', 'taxonomy']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'user', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc95dd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try:  ['probability', 'taxonomy']\n",
      "Second try:  ['probability', 'taxonomy']\n",
      "Third try:  ['probability', 'taxonomy']\n"
     ]
    }
   ],
   "source": [
    "print(\"First try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Second try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both', model='gpt-4.1-2025-04-14'))\n",
    "print(\"Third try: \", pred_tables(schema_prompt= tables_linking_prompt_V2, query= query, prompting= 'both', model='gpt-4.1-2025-04-14'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cfba33",
   "metadata": {},
   "source": [
    "# Direct Generation prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669badb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_task_simon='''As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\n",
    "The tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\n",
    "ALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \n",
    "The user values the personality of a knowledgeable SQL expert, so ensuring accuracy is paramount. Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n",
    "'''\n",
    "general_context_simon='''Given the following text, please thoroughly analyze and provide a detailed explanation of your understanding. Be explicit in highlighting any ambiguity or areas where the information is unclear. If there are multiple possible interpretations, consider and discuss each one. Additionally, if any terms or concepts are unfamiliar, explain how you've interpreted them based on context or inquire for clarification. Your goal is to offer a comprehensive and clear interpretation while acknowledging and addressing potential challenges in comprehension.\n",
    "\"## General Information about the Schema and Database\n",
    "- Prioritize obtaining OIDs in a subquery to optimize the main query.\n",
    "- Utilize nested queries to retrieve OIDs, preferably selecting the 'probability' or 'object' table.\n",
    "- Avoid JOIN clauses; instead, favor nested queries.\n",
    "## Default Parameters to Consider\n",
    "- Class probabilities for a given classifier and object are sorted from most to least likely, indicated by the 'ranking' column in the probability table. Hence, the most probable class should have 'ranking'=1.\n",
    "- The ALeRCE classification pipeline includes a Stamp Classifier and a Light Curve Classifier. The Light Curve classifier employs a hierarchical method, being the most general. If no classifier is specified, use 'classifier_name='lc_classifier' when selecting probabilities.\n",
    "- If the user doesn't specify explicit columns, use the \"SELECT *\" SQL statement to choose all possible columns.\n",
    "- Avoid changing the names of columns or tables unless necessary for the SQL query.\n",
    "## ALeRCE Pipeline Details\n",
    "- Stamp Classifier (denoted as 'stamp_classifier'): All alerts related to new objects undergo stamp-based classification.\n",
    "- Light Curve Classifier (denoted as 'lc_classifier'): A balanced hierarchical random forest classifier employing four models and 15 classes.\n",
    "- The first hierarchical classifier has three classes: [periodic, stochastic, transient], denoted as 'lc_classifier_top.'\n",
    "- Three additional classifiers specialize in different spatial object types: Periodic, Transient, and Stochastic, denoted as 'lc_classifier_periodic', 'lc_classifier_transient', and 'lc_classifier_stochastic', respectively.\n",
    "- The 15 classes are separated for each object type:\n",
    "  - Transient: [SNe Ia ('SNIa'), SNe Ib/c ('SNIbc'), SNe II ('SNII'), and Super Luminous SNe ('SLSN')].\n",
    "  - Stochastic: [Active Galactic Nuclei ('AGN'), Quasi Stellar Object ('QSO'), 'Blazar', Cataclysmic Variable/Novae ('CV/Nova'), and Young Stellar Object ('YSO')].\n",
    "  - Periodic: [Delta Scuti ('DSCT'), RR Lyrae ('RRL'), Cepheid ('CEP'), Long Period Variable ('LPV'), Eclipsing Binary ('E'), and other periodic objects ('Periodic-Other')].\n",
    "## Probability Variable Names\n",
    "- classifier_name=('lc_classifier', 'lc_classifier_top', 'lc_classifier_transient', 'lc_classifier_stochastic', 'lc_classifier_periodic', 'stamp_classifier')\n",
    "- Classes in 'lc_classifier'= ('SNIa', 'SNIbc', 'SNII', 'SLSN', 'QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO', 'LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other')\n",
    "- Classes in 'lc_classifier_top'= ('transient', 'stochastic', 'periodic')\n",
    "- Classes in 'lc_classifier_transient'= ('SNIa', 'SNIbc', 'SNII', 'SLSN')\n",
    "- Classes in 'lc_classifier_stochastic'= ('QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO')\n",
    "- Classes in 'lc_classifier_periodic'= ('LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other')\n",
    "- Classes in 'stamp_classifier'= ('SN', 'AGN', 'VS', 'asteroid', 'bogus')\n",
    "\n",
    "If a query involves selecting astronomical objects based on their celestial coordinates, the Q3C extension for PostgreSQL provides a suite of specialized functions optimized for this purpose. \n",
    "These functions enable efficient spatial queries on large astronomical datasets, including:\n",
    "- Retrieving the angular distance between two objects,\n",
    "- Determining whether two objects lie within a specified angular separation,\n",
    "- Identifying objects located within a circular region, elliptical region, or arbitrary spherical polygon on the celestial sphere.\n",
    "\n",
    "The following functions are available in the Q3C extension:\n",
    "- q3c_dist(ra1, dec1, ra2, dec2) -- returns the distance in degrees between two points (ra1,dec1) and (ra2,dec2)\n",
    "- q3c_join(ra1, dec1, ra2, dec2, radius)  -- returns true if (ra1, dec1) is within radius spherical distance of (ra2, dec2).\n",
    "- q3c_ellipse_join(ra1, dec1, ra2, dec2, major, ratio, pa) -- like q3c_join, except (ra1, dec1) have to be within an ellipse with semi-major axis major, the axis ratio ratio and the position angle pa (from north through east)\n",
    "- q3c_radial_query(ra, dec, center_ra, center_dec, radius) -- returns true if ra, dec is within radius degrees of center_ra, center_dec. This is the main function for cone searches.\n",
    "- q3c_ellipse_query(ra, dec, center_ra, center_dec, maj_ax, axis_ratio, PA ) -- returns true if ra, dec is within the ellipse from center_ra, center_dec. The ellipse is specified by semi-major axis, axis ratio and positional angle.\n",
    "- q3c_poly_query(ra, dec, poly) -- returns true if ra, dec is within the spherical polygon specified as an array of right ascensions and declinations. Alternatively poly can be an PostgreSQL polygon type.\n",
    "\n",
    "It can be useful to define a set of astronomical sources with associated coordinates directly in a SQL query, you can use a WITH clause such as:\n",
    "    WITH catalog (source_id, ra, dec) AS (\n",
    "        VALUES ('source_name', ra_value, dec_value),\n",
    "        ...)\n",
    "This construct creates a temporary inline table named catalog, which can be used in subsequent queries for cross-matching or spatial filtering operations.\n",
    "This is useful for defining a set of astronomical sources with associated coordinates directly in a SQL query. Then, you can use the Q3C functions to perform spatial queries on this temporary table (e.g. 'FROM catalog c').\n",
    "Be careful with the order of the input parameters in the Q3C functions, as they are not always the same as the order of the columns in the catalog table.\n",
    "'''\n",
    "\n",
    "final_instructions_simon='''\n",
    "# Remember to use only the schema provided, using the names of the tables and columns as they are given in the schema. You can use the information provided in the context to help you understand the schema and the request.\n",
    "# Assume that everything the user asks for is in the schema provided, you do not need to use any other table or column. Do NOT CHANGE the names of the tables or columns unless the user explicitly asks you to do so in the request, giving you the new name to use.\n",
    "# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n",
    "# Using valid PostgreSQL, the names of the tables and columns, and the information given in 'Context', answer the following request for the tables provided above.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad496b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts.DirectSQLGenPrompts import prompt_gen_task_v8, prompt_gen_context_v15, q3c_info, final_instructions_sql_gen_v19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "433e7985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line:   Given the following text, please thoroughly analyze and provide a detailed explanation of your understanding. Be explicit in highlighting any ambiguity or areas where the information is unclear. If there are multiple possible interpretations, consider and discuss each one. Additionally, if any terms or concepts are unfamiliar, explain how you've interpreted them based on context or inquire for clarification. Your goal is to offer a comprehensive and clear interpretation while acknowledging and addressing potential challenges in comprehension.\n",
      "Line:   \"## General Information about the Schema and Database\n",
      "Line:   - Prioritize obtaining OIDs in a subquery to optimize the main query.\n",
      "Line:   - Utilize nested queries to retrieve OIDs, preferably selecting the 'probability' or 'object' table.\n",
      "Line:   - Avoid JOIN clauses; instead, favor nested queries.\n",
      "Line:   ## Default Parameters to Consider\n",
      "Line:   - Class probabilities for a given classifier and object are sorted from most to least likely, indicated by the 'ranking' column in the probability table. Hence, the most probable class should have 'ranking'=1.\n",
      "Line:   - The ALeRCE classification pipeline includes a Stamp Classifier and a Light Curve Classifier. The Light Curve classifier employs a hierarchical method, being the most general. If no classifier is specified, use 'classifier_name='lc_classifier' when selecting probabilities.\n",
      "Line:   - If the user doesn't specify explicit columns, use the \"SELECT *\" SQL statement to choose all possible columns.\n",
      "Line:   - Avoid changing the names of columns or tables unless necessary for the SQL query.\n",
      "Line:   ## ALeRCE Pipeline Details\n",
      "Line:   - Stamp Classifier (denoted as 'stamp_classifier'): All alerts related to new objects undergo stamp-based classification.\n",
      "Line:   - Light Curve Classifier (denoted as 'lc_classifier'): A balanced hierarchical random forest classifier employing four models and 15 classes.\n",
      "Line:   - The first hierarchical classifier has three classes: [Periodic, Stochastic, Transient], denoted as 'lc_classifier_top.'\n",
      "Line:   - Three additional classifiers specialize in different spatial object types: Periodic, Transient, and Stochastic, denoted as 'lc_classifier_periodic', 'lc_classifier_transient', and 'lc_classifier_stochastic', respectively.\n",
      "Line:   - The 15 classes are separated for each object type:\n",
      "Line:     - Transient: [SNe Ia ('SNIa'), SNe Ib/c ('SNIbc'), SNe II ('SNII'), and Super Luminous SNe ('SLSN')].\n",
      "Line:     - Stochastic: [Active Galactic Nuclei ('AGN'), Quasi Stellar Object ('QSO'), 'Blazar', Cataclysmic Variable/Novae ('CV/Nova'), and Young Stellar Object ('YSO')].\n",
      "Line:     - Periodic: [Delta Scuti ('DSCT'), RR Lyrae ('RRL'), Cepheid ('CEP'), Long Period Variable ('LPV'), Eclipsing Binary ('E'), and other periodic objects ('Periodic-Other')].\n",
      "Line:   ## Probability Variable Names\n",
      "Line:   - classifier_name=('lc_classifier', 'lc_classifier_top', 'lc_classifier_transient', 'lc_classifier_stochastic', 'lc_classifier_periodic', 'stamp_classifier')\n",
      "Line:   - Classes in 'lc_classifier'= ('SNIa', 'SNIbc', 'SNII', 'SLSN', 'QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO', 'LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other')\n",
      "Line:   - Classes in 'lc_classifier_top'= ('Transient', 'Stochastic', 'Periodic')\n",
      "Line:   - Classes in 'lc_classifier_transient'= ('SNIa', 'SNIbc', 'SNII', 'SLSN')\n",
      "Line:   - Classes in 'lc_classifier_stochastic'= ('QSO', 'AGN', 'Blazar', 'CV/Nova', 'YSO')\n",
      "Line:   - Classes in 'lc_classifier_periodic'= ('LPV', 'E', 'DSCT', 'RRL', 'CEP', 'Periodic-Other')\n",
      "Line:   - Classes in 'stamp_classifier'= ('SN', 'AGN', 'VS', 'asteroid', 'bogus')\n",
      "Line: + \"\n",
      "Line:   \n",
      "Line:   If a query involves selecting astronomical objects based on their celestial coordinates, the Q3C extension for PostgreSQL provides a suite of specialized functions optimized for this purpose. \n",
      "Line:   These functions enable efficient spatial queries on large astronomical datasets, including:\n",
      "Line:   - Retrieving the angular distance between two objects,\n",
      "Line:   - Determining whether two objects lie within a specified angular separation,\n",
      "Line:   - Identifying objects located within a circular region, elliptical region, or arbitrary spherical polygon on the celestial sphere.\n",
      "Line:   \n",
      "Line:   The following functions are available in the Q3C extension:\n",
      "Line:   - q3c_dist(ra1, dec1, ra2, dec2) -- returns the distance in degrees between two points (ra1,dec1) and (ra2,dec2)\n",
      "Line:   - q3c_join(ra1, dec1, ra2, dec2, radius)  -- returns true if (ra1, dec1) is within radius spherical distance of (ra2, dec2).\n",
      "Line:   - q3c_ellipse_join(ra1, dec1, ra2, dec2, major, ratio, pa) -- like q3c_join, except (ra1, dec1) have to be within an ellipse with semi-major axis major, the axis ratio ratio and the position angle pa (from north through east)\n",
      "Line:   - q3c_radial_query(ra, dec, center_ra, center_dec, radius) -- returns true if ra, dec is within radius degrees of center_ra, center_dec. This is the main function for cone searches.\n",
      "Line:   - q3c_ellipse_query(ra, dec, center_ra, center_dec, maj_ax, axis_ratio, PA ) -- returns true if ra, dec is within the ellipse from center_ra, center_dec. The ellipse is specified by semi-major axis, axis ratio and positional angle.\n",
      "Line:   - q3c_poly_query(ra, dec, poly) -- returns true if ra, dec is within the spherical polygon specified as an array of right ascensions and declinations. Alternatively poly can be an PostgreSQL polygon type.\n",
      "Line:   \n",
      "Line:   It can be useful to define a set of astronomical sources with associated coordinates directly in a SQL query, you can use a WITH clause such as:\n",
      "Line:       WITH catalog (source_id, ra, dec) AS (\n",
      "Line:           VALUES ('source_name', ra_value, dec_value),\n",
      "Line:           ...)\n",
      "Line:   This construct creates a temporary inline table named catalog, which can be used in subsequent queries for cross-matching or spatial filtering operations.\n",
      "Line:   This is useful for defining a set of astronomical sources with associated coordinates directly in a SQL query. Then, you can use the Q3C functions to perform spatial queries on this temporary table (e.g. 'FROM catalog c').\n",
      "Line:   Be careful with the order of the input parameters in the Q3C functions, as they are not always the same as the order of the columns in the catalog table.\n"
     ]
    }
   ],
   "source": [
    "# Check difference between prompts\n",
    "import difflib\n",
    "\n",
    "d = difflib.Differ()\n",
    "diff_gen_task = list(d.compare(general_task_simon.strip().splitlines(), prompt_gen_task_v8.strip().splitlines()))\n",
    "diff_gen_cntx = list(d.compare(general_context_simon.strip().splitlines(), (prompt_gen_context_v15 + q3c_info).strip().splitlines()))\n",
    "diff_final_inst = list(d.compare(final_instructions_simon.strip().splitlines(), final_instructions_sql_gen_v19.strip().splitlines()))\n",
    "\n",
    "for line in diff_gen_cntx:\n",
    "    print(\"Line:\", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "529f45f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line:   As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\n",
      "Line:   The tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\n",
      "Line:   ALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \n",
      "Line:   The user values the personality of a knowledgeable SQL expert, so ensuring accuracy is paramount. Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n"
     ]
    }
   ],
   "source": [
    "for line in diff_gen_task:\n",
    "    print(\"Line:\", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f10564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line:   # Remember to use only the schema provided, using the names of the tables and columns as they are given in the schema. You can use the information provided in the context to help you understand the schema and the request.\n",
      "Line:   # Assume that everything the user asks for is in the schema provided, you do not need to use any other table or column. Do NOT CHANGE the names of the tables or columns unless the user explicitly asks you to do so in the request, giving you the new name to use.\n",
      "Line:   # Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n",
      "Line:   # Using valid PostgreSQL, the names of the tables and columns, and the information given in 'Context', answer the following request for the tables provided above.\n"
     ]
    }
   ],
   "source": [
    "for line in diff_final_inst:\n",
    "    print(\"Line:\", line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
