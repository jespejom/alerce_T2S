{"custom_id": "86-0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\n# As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nBe thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\n# The Database has the following tables that can be used to generate the SQL query:\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE INDEX ix_object_ndet ON object USING btree (ndet);\nCREATE INDEX ix_object_firstmjd ON object USING btree (firstmjd);\nCREATE INDEX ix_object_g_r_max ON object USING btree (g_r_max);\nCREATE INDEX ix_object_g_r_mean_corr ON object USING btree (g_r_mean_corr);\nCREATE INDEX ix_object_meanra ON object USING btree (meanra);\nCREATE INDEX ix_object_meandec ON object USING btree (meandec);\n\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\nCREATE INDEX ix_probabilities_oid ON probability USING hash (oid);\nCREATE INDEX ix_probabilities_probability ON probability USING btree (probability);\nCREATE INDEX ix_probabilities_ranking ON probability USING btree (ranking);\nCREATE INDEX ix_classification_rank1 ON probability USING btree (ranking) WHERE (ranking = 1);\n\n\n\n\n## DEFAULT CONDITIONS YOU NEED TO SET\n### IF THE 'probability' TABLE is used, use always the next conditions, unless the user explicitly specifies different probability conditions.\n- 'probability.ranking' = 1 ; this only return the most likely probabilities, if the user request all ranking probabilities, don't use it.\n- 'probability.classifier_name='lc_classifier' ; this will return the classifications by the light curve classifier\n### GENERAL\n### Important points to consider\n- If the user doesn't specify explicit columns or information that is not in a column, choose all the columns, for example by using the \u201cSELECT *\u201d SQL statement, from ALL the tables used, including the ones from the sub-queries.\n- Mantain the EXACT COLUMN names as they are in the database, unless the user explicitly asks you to do so in the request, giving you the new name to use. This is crucial for the query to work correctly.\n- Mantain the exact class names as they are in the database, marked with single quotes, for example, 'SNIa'.\n\n# If you need to use 2 tables, try using a INNER JOIN statement, or a sub-query over 'probability' or 'object', if the query requires it. It is important to be really careful with the use of sub-queries or JOINs, as they can slow down the query.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Answer ONLY with the final SQL query, with the following format: \n  ```sql SQL_QUERY_HERE ```\nDON'T include anything else in your answer. If you want to add comments, use the SQL comment format inside the query.\n\n# Use the next decomposed planification to write the query:\nTo decompose the user request into a systematic plan for generating the PostgreSQL query, follow these detailed steps:\n\n1. **Identify the relevant tables and columns:**\n   - The 'probability' table contains classification probabilities, with columns: 'oid', 'class_name', 'classifier_name', 'classifier_version', 'probability', and others.\n   - The 'ps1_ztf' table contains information about the closest PS1 sources, with columns: 'oid', 'sgscore1', 'distpsnr1', and other positional and photometric data.\n   \n2. **Determine the filtering criteria:**\n   - For the 'probability' table:\n     - Filter for 'classifier_name' equal to 'lc_classifier'.\n     - Filter for 'classifier_version' equal to 'hierarchical_rf_1.1.0'.\n     - Filter for 'class_name' being one of 'Cepheid', 'DSCT', or 'RRL'.\n     - Filter for 'probability' greater than 0.6.\n     - Filter for 'ranking' equal to 1 to ensure the most probable classification.\n   \n   - For the 'ps1_ztf' table:\n     - Filter for 'sgscore1' greater than 0.8, indicating a high star/galaxy score.\n   \n3. **Establish the join conditions:**\n   - The 'probability' and 'ps1_ztf' tables are linked via the 'oid' column, which uniquely identifies objects.\n   - Use an INNER JOIN on 'oid' to combine the classification probabilities with the PS1 source information.\n   \n4. **Construct the selection criteria:**\n   - Apply the filters on 'probability' table for the specified classes, probability threshold, classifier name, version, and ranking.\n   - Apply the filter on 'ps1_ztf' for the star/galaxy score.\n   \n5. **Determine the output columns:**\n   - Select all columns from the 'probability' table.\n   - Select all columns from the 'ps1_ztf' table.\n   - Since the user requests \"all columns\" from both tables, specify these explicitly or use a wildcard if appropriate.\n   \n6. **Limit the number of results:**\n   - Use a LIMIT clause to restrict the output to at most 100 rows.\n   \n7. **Order the results (optional but recommended):**\n   - To make the output more meaningful, consider ordering by 'probability' descending or by 'oid'.\n   \n8. **Summarize the query structure:**\n   - The query will be a SELECT statement with a JOIN between 'probability' and 'ps1_ztf' on 'oid'.\n   - The WHERE clause will include all the filtering conditions.\n   - The LIMIT clause will restrict the output to 100 rows.\n   \n9. **Ensure adherence to database conventions:**\n   - Use exact table and column names.\n   - Use the specified classifier name, version, class names, and probability thresholds.\n   - Use the 'ranking' filter to select only the top-ranked classification.\n   \n10. **Final step:**\n    - Prepare the query with all the above conditions, ready for execution, ensuring it is optimized for performance by using appropriate indexes (e.g., on 'oid', 'probability', 'classifier_name', 'classifier_version', 'class_name', 'sgscore1').\n\nThis systematic decomposition ensures the resulting query accurately retrieves the desired objects with the specified conditions, combining classification probabilities and PS1 source scores, limited to 100 results.\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n"}, {"role": "user", "content": "\n # User Request: ''Find ZTF objects that satisfy the following conditions: have a probability larger than 0.6 of being either Cepheid or delta Scuti or RR Lyra in the light curve classifier version 'hierarchical_rf_1.1.0', and their closest PS1 objects have a star/galaxy score larger than 0.8. Return all columns from the 'probability' and 'ps1_ztf' tables, and give at most 100 rows in the result''"}], "max_tokens": 4000, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "86-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\n# As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nBe thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\n# The Database has the following tables that can be used to generate the SQL query:\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE INDEX ix_object_ndet ON object USING btree (ndet);\nCREATE INDEX ix_object_firstmjd ON object USING btree (firstmjd);\nCREATE INDEX ix_object_g_r_max ON object USING btree (g_r_max);\nCREATE INDEX ix_object_g_r_mean_corr ON object USING btree (g_r_mean_corr);\nCREATE INDEX ix_object_meanra ON object USING btree (meanra);\nCREATE INDEX ix_object_meandec ON object USING btree (meandec);\n\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\nCREATE INDEX ix_probabilities_oid ON probability USING hash (oid);\nCREATE INDEX ix_probabilities_probability ON probability USING btree (probability);\nCREATE INDEX ix_probabilities_ranking ON probability USING btree (ranking);\nCREATE INDEX ix_classification_rank1 ON probability USING btree (ranking) WHERE (ranking = 1);\n\n\n\n\n## DEFAULT CONDITIONS YOU NEED TO SET\n### IF THE 'probability' TABLE is used, use always the next conditions, unless the user explicitly specifies different probability conditions.\n- 'probability.ranking' = 1 ; this only return the most likely probabilities, if the user request all ranking probabilities, don't use it.\n- 'probability.classifier_name='lc_classifier' ; this will return the classifications by the light curve classifier\n### GENERAL\n### Important points to consider\n- If the user doesn't specify explicit columns or information that is not in a column, choose all the columns, for example by using the \u201cSELECT *\u201d SQL statement, from ALL the tables used, including the ones from the sub-queries.\n- Mantain the EXACT COLUMN names as they are in the database, unless the user explicitly asks you to do so in the request, giving you the new name to use. This is crucial for the query to work correctly.\n- Mantain the exact class names as they are in the database, marked with single quotes, for example, 'SNIa'.\n\n# If you need to use 2 tables, try using a INNER JOIN statement, or a sub-query over 'probability' or 'object', if the query requires it. It is important to be really careful with the use of sub-queries or JOINs, as they can slow down the query.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Answer ONLY with the final SQL query, with the following format: \n  ```sql SQL_QUERY_HERE ```\nDON'T include anything else in your answer. If you want to add comments, use the SQL comment format inside the query.\n\n# Use the next decomposed planification to write the query:\nTo decompose the user request into a systematic plan for generating the PostgreSQL query, follow these detailed steps:\n\n1. **Identify the relevant tables and columns:**\n   - The 'probability' table contains classification probabilities, with columns: 'oid', 'class_name', 'classifier_name', 'classifier_version', 'probability', and others.\n   - The 'ps1_ztf' table contains information about the closest PS1 sources, with columns: 'oid', 'sgscore1', and other positional and magnitude data.\n   \n2. **Determine the filtering criteria:**\n   - For the 'probability' table:\n     - Filter for 'classifier_name' = 'lc_classifier'\n     - Filter for 'classifier_version' = 'hierarchical_rf_1.1.0'\n     - Filter for 'ranking' = 1 (to get the most probable class)\n     - Filter for 'probability' > 0.6\n     - Filter for 'class_name' being one of 'Cepheid', 'DSCT', or 'RRL'\n   \n   - For the 'ps1_ztf' table:\n     - Filter for 'sgscore1' > 0.8 (star/galaxy score)\n   \n3. **Establish the join condition:**\n   - The 'probability' table and 'ps1_ztf' table are linked via the 'oid' column, which uniquely identifies objects.\n   - Use an INNER JOIN on 'oid' to combine the classification probabilities with the PS1 source information.\n   \n4. **Construct the selection:**\n   - Select all columns from both 'probability' and 'ps1_ztf' tables.\n   - Ensure that the join includes only objects satisfying all the above filters.\n   \n5. **Limit the result:**\n   - Restrict the output to at most 100 rows using a LIMIT clause.\n   \n6. **Order the results (optional but recommended):**\n   - To make the output more meaningful, consider ordering by 'probability' descending, so the most confident classifications appear first.\n   \n7. **Final considerations:**\n   - Confirm that the filters for class names are correctly specified with exact string matches.\n   - Use the exact table and column names as specified.\n   - Ensure the query is efficient by applying filters before joins where possible.\n   \nThis systematic decomposition ensures the query will accurately retrieve ZTF objects with high probability of being Cepheid, delta Scuti, or RR Lyrae, and with high-confidence PS1 star/galaxy scores, returning all relevant data from both tables, limited to 100 results.\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n"}, {"role": "user", "content": "\n # User Request: ''Find ZTF objects that satisfy the following conditions: have a probability larger than 0.6 of being either Cepheid or delta Scuti or RR Lyra in the light curve classifier version 'hierarchical_rf_1.1.0', and their closest PS1 objects have a star/galaxy score larger than 0.8. Return all columns from the 'probability' and 'ps1_ztf' tables, and give at most 100 rows in the result''"}], "max_tokens": 4000, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "36-0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\n# As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nBe thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\n# The Database has the following tables that can be used to generate the SQL query:\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE INDEX ix_object_ndet ON object USING btree (ndet);\nCREATE INDEX ix_object_firstmjd ON object USING btree (firstmjd);\nCREATE INDEX ix_object_g_r_max ON object USING btree (g_r_max);\nCREATE INDEX ix_object_g_r_mean_corr ON object USING btree (g_r_mean_corr);\nCREATE INDEX ix_object_meanra ON object USING btree (meanra);\nCREATE INDEX ix_object_meandec ON object USING btree (meandec);\n\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\nCREATE INDEX ix_probabilities_oid ON probability USING hash (oid);\nCREATE INDEX ix_probabilities_probability ON probability USING btree (probability);\nCREATE INDEX ix_probabilities_ranking ON probability USING btree (ranking);\nCREATE INDEX ix_classification_rank1 ON probability USING btree (ranking) WHERE (ranking = 1);\n\n\n\n\n## DEFAULT CONDITIONS YOU NEED TO SET\n### IF THE 'probability' TABLE is used, use always the next conditions, unless the user explicitly specifies different probability conditions.\n- 'probability.ranking' = 1 ; this only return the most likely probabilities, if the user request all ranking probabilities, don't use it.\n- 'probability.classifier_name='lc_classifier' ; this will return the classifications by the light curve classifier\n### GENERAL\n### Important points to consider\n- If the user doesn't specify explicit columns or information that is not in a column, choose all the columns, for example by using the \u201cSELECT *\u201d SQL statement, from ALL the tables used, including the ones from the sub-queries.\n- Mantain the EXACT COLUMN names as they are in the database, unless the user explicitly asks you to do so in the request, giving you the new name to use. This is crucial for the query to work correctly.\n- Mantain the exact class names as they are in the database, marked with single quotes, for example, 'SNIa'.\n\n# If you need to use 2 tables, try using a INNER JOIN statement, or a sub-query over 'probability' or 'object', if the query requires it. It is important to be really careful with the use of sub-queries or JOINs, as they can slow down the query.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Answer ONLY with the final SQL query, with the following format: \n  ```sql SQL_QUERY_HERE ```\nDON'T include anything else in your answer. If you want to add comments, use the SQL comment format inside the query.\n\n# Use the next decomposed planification to write the query:\nTo fulfill the user request of retrieving feature names and values for the light curves of the object with the specified oid ('ZTF20aaelulu'), along with the oid column, the decomposition plan should include the following steps:\n\n1. **Identify the main table containing object information:**\n   - Recognize that the 'object' table contains the primary identifier (oid) and general statistics about each astronomical object.\n   - Confirm that the 'oid' value 'ZTF20aaelulu' exists in this table.\n\n2. **Locate the table with feature data:**\n   - Determine that the 'feature' table stores various features related to each object, with columns for 'oid', 'name' (feature name), and 'value' (feature value).\n   - Understand that each row in 'feature' corresponds to a specific feature for a particular object.\n\n3. **Establish the filtering criteria:**\n   - Filter the 'feature' table to include only rows where 'oid' matches 'ZTF20aaelulu'.\n   - Ensure that the 'name' and 'value' columns are selected to show feature names and their corresponding values.\n\n4. **Include the object identifier in the output:**\n   - Make sure to include the 'oid' column in the SELECT statement to explicitly show the object identifier alongside the features.\n\n5. **Determine the output structure:**\n   - The result should be a list of feature names ('name') and their associated values ('value') for the specified object.\n   - The output should also include the 'oid' for clarity and completeness.\n\n6. **Optional considerations:**\n   - Decide whether to retrieve all features or filter by specific feature names if specified (not in this case).\n   - Confirm that the query is optimized by filtering directly on 'oid' and selecting only necessary columns.\n\n7. **Construct the query logic:**\n   - Use a simple SELECT statement from the 'feature' table.\n   - Apply a WHERE clause to filter by 'oid' = 'ZTF20aaelulu'.\n   - Select the 'oid', 'name', and 'value' columns.\n\n8. **Verify the query correctness:**\n   - Ensure that the 'oid' value matches exactly the user-specified object ID.\n   - Confirm that the feature names and values are correctly retrieved and displayed.\n\nBy following these steps, the resulting query will accurately and efficiently retrieve all feature names and values associated with the specified object, including the object identifier.\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n"}, {"role": "user", "content": "\n # User Request: ''Show the feature names and values of the light curves of the object with the oid ZTF20aaelulu. Return also the oid column.''"}], "max_tokens": 4000, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "36-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\n# As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nBe thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\n# The Database has the following tables that can be used to generate the SQL query:\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE INDEX ix_object_ndet ON object USING btree (ndet);\nCREATE INDEX ix_object_firstmjd ON object USING btree (firstmjd);\nCREATE INDEX ix_object_g_r_max ON object USING btree (g_r_max);\nCREATE INDEX ix_object_g_r_mean_corr ON object USING btree (g_r_mean_corr);\nCREATE INDEX ix_object_meanra ON object USING btree (meanra);\nCREATE INDEX ix_object_meandec ON object USING btree (meandec);\n\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\nCREATE INDEX ix_probabilities_oid ON probability USING hash (oid);\nCREATE INDEX ix_probabilities_probability ON probability USING btree (probability);\nCREATE INDEX ix_probabilities_ranking ON probability USING btree (ranking);\nCREATE INDEX ix_classification_rank1 ON probability USING btree (ranking) WHERE (ranking = 1);\n\n\n\n\n## DEFAULT CONDITIONS YOU NEED TO SET\n### IF THE 'probability' TABLE is used, use always the next conditions, unless the user explicitly specifies different probability conditions.\n- 'probability.ranking' = 1 ; this only return the most likely probabilities, if the user request all ranking probabilities, don't use it.\n- 'probability.classifier_name='lc_classifier' ; this will return the classifications by the light curve classifier\n### GENERAL\n### Important points to consider\n- If the user doesn't specify explicit columns or information that is not in a column, choose all the columns, for example by using the \u201cSELECT *\u201d SQL statement, from ALL the tables used, including the ones from the sub-queries.\n- Mantain the EXACT COLUMN names as they are in the database, unless the user explicitly asks you to do so in the request, giving you the new name to use. This is crucial for the query to work correctly.\n- Mantain the exact class names as they are in the database, marked with single quotes, for example, 'SNIa'.\n\n# If you need to use 2 tables, try using a INNER JOIN statement, or a sub-query over 'probability' or 'object', if the query requires it. It is important to be really careful with the use of sub-queries or JOINs, as they can slow down the query.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Answer ONLY with the final SQL query, with the following format: \n  ```sql SQL_QUERY_HERE ```\nDON'T include anything else in your answer. If you want to add comments, use the SQL comment format inside the query.\n\n# Use the next decomposed planification to write the query:\nTo fulfill the user request of retrieving feature names and values of the light curves for the object with oid 'ZTF20aaelulu', along with the oid column, the decomposition plan should include the following steps:\n\n1. **Identify the main table containing object information:**\n   - Recognize that the 'object' table contains the primary statistics and identifiers for each astronomical object, including the 'oid' column.\n   - Confirm that the 'oid' value 'ZTF20aaelulu' exists in this table.\n\n2. **Determine the table storing feature data:**\n   - Understand that the 'feature' table contains various features associated with each object, with columns: 'oid', 'name', 'value', 'fid', 'version'.\n   - Recognize that each row in 'feature' corresponds to a specific feature name and its value for a given object.\n\n3. **Establish the filtering criteria:**\n   - Filter the 'feature' table to include only rows where 'oid' matches 'ZTF20aaelulu'.\n   - Ensure that the 'oid' in the 'feature' table corresponds to the object of interest.\n\n4. **Select relevant columns:**\n   - Retrieve the 'oid' column to confirm the object identity.\n   - Retrieve the 'name' column to get feature names.\n   - Retrieve the 'value' column to get feature values.\n\n5. **Order or group the results:**\n   - Decide whether to order the features alphabetically by 'name' for clarity.\n   - Consider grouping if necessary, but typically a simple selection suffices.\n\n6. **Construct the query structure:**\n   - Use a SELECT statement to fetch 'oid', 'name', and 'value' from the 'feature' table.\n   - Apply a WHERE clause to filter by 'oid' = 'ZTF20aaelulu'.\n\n7. **Optional: Include additional filtering or sorting:**\n   - If needed, order by 'name' to organize feature names.\n   - Confirm that the query returns all features associated with the object.\n\n8. **Verify the presence of features:**\n   - Ensure that the object has associated features in the 'feature' table.\n   - Handle cases where no features are available, if relevant.\n\n9. **Prepare for execution:**\n   - Confirm that the query is optimized by using indexes on 'oid' in the 'feature' table.\n   - Ensure that the query is straightforward and efficient.\n\n10. **Document the query purpose:**\n    - Clearly state that the query retrieves all feature names and their corresponding values for the specified object, including the object identifier.\n\nBy following these steps, the resulting query will accurately and efficiently retrieve the desired feature information for the object with oid 'ZTF20aaelulu'.\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n"}, {"role": "user", "content": "\n # User Request: ''Show the feature names and values of the light curves of the object with the oid ZTF20aaelulu. Return also the oid column.''"}], "max_tokens": 4000, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "0-0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\n# As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nBe thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\n# The Database has the following tables that can be used to generate the SQL query:\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE INDEX ix_object_ndet ON object USING btree (ndet);\nCREATE INDEX ix_object_firstmjd ON object USING btree (firstmjd);\nCREATE INDEX ix_object_g_r_max ON object USING btree (g_r_max);\nCREATE INDEX ix_object_g_r_mean_corr ON object USING btree (g_r_mean_corr);\nCREATE INDEX ix_object_meanra ON object USING btree (meanra);\nCREATE INDEX ix_object_meandec ON object USING btree (meandec);\n\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\nCREATE INDEX ix_probabilities_oid ON probability USING hash (oid);\nCREATE INDEX ix_probabilities_probability ON probability USING btree (probability);\nCREATE INDEX ix_probabilities_ranking ON probability USING btree (ranking);\nCREATE INDEX ix_classification_rank1 ON probability USING btree (ranking) WHERE (ranking = 1);\n\n\n\n\n## DEFAULT CONDITIONS YOU NEED TO SET\n### IF THE 'probability' TABLE is used, use always the next conditions, unless the user explicitly specifies different probability conditions.\n- 'probability.ranking' = 1 ; this only return the most likely probabilities, if the user request all ranking probabilities, don't use it.\n- 'probability.classifier_name='lc_classifier' ; this will return the classifications by the light curve classifier\n### GENERAL\n### Important points to consider\n- If the user doesn't specify explicit columns or information that is not in a column, choose all the columns, for example by using the \u201cSELECT *\u201d SQL statement, from ALL the tables used, including the ones from the sub-queries.\n- Mantain the EXACT COLUMN names as they are in the database, unless the user explicitly asks you to do so in the request, giving you the new name to use. This is crucial for the query to work correctly.\n- Mantain the exact class names as they are in the database, marked with single quotes, for example, 'SNIa'.\n\n# If you need to use 2 tables, try using a INNER JOIN statement, or a sub-query over 'probability' or 'object', if the query requires it. It is important to be really careful with the use of sub-queries or JOINs, as they can slow down the query.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Answer ONLY with the final SQL query, with the following format: \n  ```sql SQL_QUERY_HERE ```\nDON'T include anything else in your answer. If you want to add comments, use the SQL comment format inside the query.\n\n# Use the next decomposed planification to write the query:\nTo generate a PostgreSQL query that retrieves objects likely to be Young Stellar Objects (YSOs) with the specified conditions from the ALeRCE database, follow these detailed steps:\n\n1. **Identify the relevant tables and columns:**\n   - Use the 'object' table to access object identifiers and basic properties.\n   - Use the 'probability' table to filter objects with a high probability (> 0.7) of being YSOs, specifically where 'class_name' is 'YSO' and 'ranking' is 1.\n   - Use the 'feature' table to access photometric features, including period, amplitude, feature name, feature value, feature version, and filter id.\n\n2. **Filter objects based on classification probability:**\n   - Select objects where the 'probability' table indicates a 'YSO' classification with a probability greater than 0.7.\n   - Ensure that the 'classifier_name' is 'lc_classifier' and 'classifier_version' matches the latest or relevant version if specified.\n   - Filter for the top-ranked probability ('ranking' = 1).\n\n3. **Identify objects with periodicity:**\n   - From the 'feature' table, select features where 'name' indicates a period-related feature (e.g., 'Multiband_period' or similar period features).\n   - Filter for features where 'value' (the period) is between 3 and 10 days.\n\n4. **Filter objects based on amplitude:**\n   - From the 'feature' table, select features where 'name' indicates amplitude (e.g., 'Amplitude').\n   - Filter for features where 'value' (the amplitude) is greater than 0.5 mag.\n\n5. **Join the tables:**\n   - Join the 'object' table with the 'probability' table on 'oid' to filter by classification probability.\n   - Join the resulting set with the 'feature' table on 'oid' to access period and amplitude features.\n   - Ensure that the features are from the same object and match the criteria for period and amplitude.\n\n6. **Select the required columns:**\n   - Object identifier ('oid').\n   - The probability value ('probability').\n   - The feature value ('value') for period and amplitude.\n   - The feature name ('name') to identify the feature.\n   - The filter id ('fid') associated with the feature.\n   - The feature version ('version').\n\n7. **Order the results:**\n   - Sort the final list by the probability ('probability') in descending order to prioritize the most likely YSOs.\n\n8. **Ensure the query returns all relevant features:**\n   - For each object, include the features for period and amplitude, possibly as separate rows or combined, depending on the desired output format.\n\n9. **Optional:**\n   - Limit the number of results if needed.\n   - Add any additional filters or conditions based on user preferences or data constraints.\n\nBy following these steps, the query will systematically filter objects classified as likely YSOs, with the specified periodicity and amplitude characteristics, sorted by their probability of being YSOs, and will include all necessary details such as object ID, probability, feature values, feature names, filter IDs, and feature versions.\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n"}, {"role": "user", "content": "\n # User Request: ''Get objects that are likely to be YSOs (possibility great than 0.7) and with some periodicities. The photometric period is between 3 to 10 days and variation amplitude is great than 0.5 mag. Please sort the list by the possibility in descending order, and return the object identifier, the possibility, value and name of the feature, the filter id, and the feature version.''"}], "max_tokens": 4000, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "0-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\n# As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nBe thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\n# The Database has the following tables that can be used to generate the SQL query:\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE INDEX ix_object_ndet ON object USING btree (ndet);\nCREATE INDEX ix_object_firstmjd ON object USING btree (firstmjd);\nCREATE INDEX ix_object_g_r_max ON object USING btree (g_r_max);\nCREATE INDEX ix_object_g_r_mean_corr ON object USING btree (g_r_mean_corr);\nCREATE INDEX ix_object_meanra ON object USING btree (meanra);\nCREATE INDEX ix_object_meandec ON object USING btree (meandec);\n\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\nCREATE INDEX ix_probabilities_oid ON probability USING hash (oid);\nCREATE INDEX ix_probabilities_probability ON probability USING btree (probability);\nCREATE INDEX ix_probabilities_ranking ON probability USING btree (ranking);\nCREATE INDEX ix_classification_rank1 ON probability USING btree (ranking) WHERE (ranking = 1);\n\n\n\n\n## DEFAULT CONDITIONS YOU NEED TO SET\n### IF THE 'probability' TABLE is used, use always the next conditions, unless the user explicitly specifies different probability conditions.\n- 'probability.ranking' = 1 ; this only return the most likely probabilities, if the user request all ranking probabilities, don't use it.\n- 'probability.classifier_name='lc_classifier' ; this will return the classifications by the light curve classifier\n### GENERAL\n### Important points to consider\n- If the user doesn't specify explicit columns or information that is not in a column, choose all the columns, for example by using the \u201cSELECT *\u201d SQL statement, from ALL the tables used, including the ones from the sub-queries.\n- Mantain the EXACT COLUMN names as they are in the database, unless the user explicitly asks you to do so in the request, giving you the new name to use. This is crucial for the query to work correctly.\n- Mantain the exact class names as they are in the database, marked with single quotes, for example, 'SNIa'.\n\n# If you need to use 2 tables, try using a INNER JOIN statement, or a sub-query over 'probability' or 'object', if the query requires it. It is important to be really careful with the use of sub-queries or JOINs, as they can slow down the query.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Answer ONLY with the final SQL query, with the following format: \n  ```sql SQL_QUERY_HERE ```\nDON'T include anything else in your answer. If you want to add comments, use the SQL comment format inside the query.\n\n# Use the next decomposed planification to write the query:\nTo decompose the user request into a systematic series of steps for generating the appropriate PostgreSQL query, follow these detailed steps:\n\n1. **Identify the main tables involved:**\n   - Use the `probability` table to filter objects based on their classification probability.\n   - Use the `object` table to retrieve object identifiers and associated metadata.\n   - Use the `feature` table to access photometric features such as period, amplitude, filter ID, feature name, and feature version.\n\n2. **Filter objects based on YSO likelihood:**\n   - Select entries from the `probability` table where:\n     - `class_name` corresponds to 'YSO'.\n     - `probability` value is greater than 0.7.\n     - `classifier_name` is set to 'lc_classifier' (as per default conditions).\n     - `ranking` is 1 (to ensure the most probable classification).\n\n3. **Identify objects with some periodicity:**\n   - From the `feature` table, filter for features where:\n     - `name` indicates a period-related feature (e.g., 'Multiband_period' or other period features).\n     - `value` (the feature value) indicates a period between 3 and 10 days.\n     - `fid` (filter ID) corresponds to the filter used for the period measurement.\n     - `version` indicates the feature version.\n\n4. **Filter objects with variation amplitude greater than 0.5 mag:**\n   - From the `feature` table, select features where:\n     - `name` corresponds to an amplitude feature (e.g., 'Amplitude').\n     - `value` exceeds 0.5.\n\n5. **Combine filters:**\n   - Use JOIN operations to link the `probability` table with the `object` table via `oid`.\n   - Join the `feature` table twice or more if needed:\n     - Once for the period feature.\n     - Once for the amplitude feature.\n   - Ensure that the features are associated with the same object (`oid`).\n\n6. **Apply sorting:**\n   - Order the final results by the `probability` value in descending order.\n\n7. **Select specific columns for output:**\n   - Object identifier (`oid`).\n   - The probability value.\n   - The feature value (period or amplitude).\n   - The feature name.\n   - The filter ID (`fid`).\n   - The feature version.\n\n8. **Ensure the query returns all relevant features:**\n   - For each object, include the object ID, probability, and the features (period and amplitude) with their respective details.\n\n9. **Optional: Limit or paginate results if needed (not specified in user request).**\n\n10. **Test the query to verify correctness:**\n    - Confirm that all filters are correctly applied.\n    - Check that the join conditions correctly match features to objects.\n    - Validate that the sorting order is correct.\n\nThis systematic decomposition ensures the query accurately retrieves objects that are likely YSOs with the specified periodicity and amplitude characteristics, sorted by their likelihood in descending order, including all requested feature details.\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n"}, {"role": "user", "content": "\n # User Request: ''Get objects that are likely to be YSOs (possibility great than 0.7) and with some periodicities. The photometric period is between 3 to 10 days and variation amplitude is great than 0.5 mag. Please sort the list by the possibility in descending order, and return the object identifier, the possibility, value and name of the feature, the filter id, and the feature version.''"}], "max_tokens": 4000, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "4-0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\n# As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nBe thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\n# The Database has the following tables that can be used to generate the SQL query:\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE INDEX ix_object_ndet ON object USING btree (ndet);\nCREATE INDEX ix_object_firstmjd ON object USING btree (firstmjd);\nCREATE INDEX ix_object_g_r_max ON object USING btree (g_r_max);\nCREATE INDEX ix_object_g_r_mean_corr ON object USING btree (g_r_mean_corr);\nCREATE INDEX ix_object_meanra ON object USING btree (meanra);\nCREATE INDEX ix_object_meandec ON object USING btree (meandec);\n\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\nCREATE INDEX ix_probabilities_oid ON probability USING hash (oid);\nCREATE INDEX ix_probabilities_probability ON probability USING btree (probability);\nCREATE INDEX ix_probabilities_ranking ON probability USING btree (ranking);\nCREATE INDEX ix_classification_rank1 ON probability USING btree (ranking) WHERE (ranking = 1);\n\n\n\n\n## DEFAULT CONDITIONS YOU NEED TO SET\n### IF THE 'probability' TABLE is used, use always the next conditions, unless the user explicitly specifies different probability conditions.\n- 'probability.ranking' = 1 ; this only return the most likely probabilities, if the user request all ranking probabilities, don't use it.\n- 'probability.classifier_name='lc_classifier'\n### IF THE 'feature' TABLE is used with 2 or more features, you need to take the following steps, because it is a transposed table (each feature is in a different row).\nI. Create a sub-query using the 'probability' TABLE filtering the desired objects.\nII. For each feature, you have to make a sub-query retrieving the specific feature adding the condition of its value, taking only the oids in the 'probability' sub-query with an INNER JOIN inside each 'feature' sub-query to retrieve only the features associated with the desired spatial objects.\nIII. Make an UNION between the sub-queries of each feature from step II\nIV. Make an INTERSECT between the sub-queries of each feature from step II\nV. Filter the 'UNION' query from step III selecting only the 'oids' that are in the 'INTERSECT' query from step IV\nVI. Add the remaining conditions to the final result of step V, using the 'probability' sub-query from step I.\n### GENERAL\n### Important points to consider\n- If the user doesn't specify explicit columns or information that is not in a column, choose all the columns, for example by using the \u201cSELECT *\u201d SQL statement, from ALL the tables used, including the ones from the sub-queries.\n- Mantain the EXACT COLUMN names as they are in the database, unless the user explicitly asks you to do so in the request, giving you the new name to use. This is crucial for the query to work correctly.\n- Mantain the exact class names as they are in the database, marked with single quotes, for example, 'SNIa'.\n\n\n# If you need to use 2 or 3 tables, try using a sub-query or INNER JOIN over 'probability' TABLE or 'object' TABLE, or an INNER JOIN between 'probabbility' and 'object', or over an INNER JOIN between 'probability', 'object' and 'magstat', if it is necessary (priority in this order).\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Finally, join all the steps in a final query, with the following format: \n```sql [FINAL QUERY HERE] ```\nDON'T include anything else inside and after your FINAL answer.\n\n# Use the next decomposed planification to write the query:\nTo decompose the user request into a systematic plan for generating the appropriate PostgreSQL query, follow these detailed steps:\n\n1. **Identify the main tables involved:**\n   - The `object` table, which contains object identifiers (`oid`), mean right ascension (`meanra`), mean declination (`meandec`), and other static object properties.\n   - The `probability` table, which provides classification probabilities, including the class name (`class_name`), classifier name (`classifier_name`), probability (`probability`), and ranking.\n   - The `detection` table, which records individual detections with time (`mjd`), magnitudes (`magpsf`), magnitude errors (`sigmapsf`), band identifiers (`fid`), and candidate identifiers (`candid`).\n   - The `magstat` table, which contains statistics per object per band, including initial rise estimates (`dmdt_first`), and possibly other relevant parameters.\n   \n2. **Filter objects classified as 'SN II' with probability > 0.6:**\n   - Use the `probability` table filtered by `class_name='SNII'`, `classifier_name='lc_classifier'`, and `probability > 0.6`.\n   - Ensure to select only the entries with `ranking=1` to get the most probable classification.\n   - Use an `INNER JOIN` between `probability` and `object` on `oid` to link classification probabilities with object identifiers.\n\n3. **Restrict the classification to the date range 2019-2022:**\n   - Use the `object` table's `firstmjd` and `lastmjd` columns to filter objects that have observations within this period.\n   - Convert the date range into Julian days (MJD). For example, 2019-01-01 corresponds approximately to MJD 58549, and 2022-12-31 to MJD 59910.\n   - Filter objects where `firstmjd` \u2264 2022-12-31 (or the upper MJD limit) and `lastmjd` \u2265 2019-01-01 (or the lower MJD limit).\n\n4. **Apply the detection filters:**\n   - Select detections from the `detection` table where:\n     - `mjd` falls within 2019-2022 MJDs.\n     - `fid` corresponds to the g-band (`fid=1`) and r-band (`fid=2`).\n     - The number of detections per object exceeds 50 (`ndet > 50`), which can be checked via a sub-query or aggregate count grouped by `oid`.\n   \n5. **Filter objects with initial rise rate > 0.5 mag/day:**\n   - Use the `magstat` table, filtering for:\n     - `dmdt_first > 0.5` (initial rise estimate).\n     - Corresponding to the same objects (`oid`) filtered previously.\n     - Filter for `fid=1` (g-band) and `fid=2` (r-band) separately, or combine as needed.\n   \n6. **Retrieve the required information:**\n   - For each detection (or for each object over time), extract:\n     - Object identifier (`oid`).\n     - Candidate identifier (`candid`).\n     - Magnitude (`magpsf`).\n     - Magnitude error (`sigmapsf`).\n     - Initial rise estimate (`dmdt_first`) from `magstat`.\n     - Band identifier (`fid`).\n     - Observation time (`mjd`).\n   \n7. **Organize the data as a function of time:**\n   - Order the results by `mjd` to reflect the temporal evolution.\n   - Possibly include the band name or convert `fid` to band labels ('g' or 'r') for clarity.\n   \n8. **Combine all filters and conditions:**\n   - Use `JOIN`s to connect `object`, `probability`, `detection`, and `magstat` tables.\n   - Use sub-queries or `INTERSECT`s to ensure objects meet all criteria simultaneously:\n     - Classification as SN II with probability > 0.6.\n     - Observed within 2019-2022.\n     - More than 50 detections.\n     - Initial rise rate > 0.5 mag/day in g and r bands.\n   \n9. **Apply any additional constraints or ordering:**\n   - Order the final results by `mjd` ascending.\n   - Select all relevant columns, or specify only those requested.\n\n10. **Ensure the query is optimized:**\n    - Use indexes on `oid`, `mjd`, `fid`, and `probability` columns.\n    - Limit the result set if necessary for performance during testing.\n\nThis systematic decomposition ensures that the final SQL query will accurately retrieve the object identifiers, candidate IDs, magnitudes, errors, initial rise estimates, and band information as a function of time, satisfying all the specified conditions for objects classified as SN II in the specified date range with the given probability and rise rate constraints.\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n"}, {"role": "user", "content": "\n # User Request: ''Get the object identifier, candidate identifier, magnitudes, magnitude errors, initial rise estimate, and band identifiers as a function of time of the objects classified as SN II in the year 2019-2022, with probability larger than 0.6, initial rise rate greater than 0.5 mag/day in ZTF g and r-band and number of detections greater than 50.''"}], "max_tokens": 4000, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "4-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\n# As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nBe thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\n# The Database has the following tables that can be used to generate the SQL query:\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE INDEX ix_object_ndet ON object USING btree (ndet);\nCREATE INDEX ix_object_firstmjd ON object USING btree (firstmjd);\nCREATE INDEX ix_object_g_r_max ON object USING btree (g_r_max);\nCREATE INDEX ix_object_g_r_mean_corr ON object USING btree (g_r_mean_corr);\nCREATE INDEX ix_object_meanra ON object USING btree (meanra);\nCREATE INDEX ix_object_meandec ON object USING btree (meandec);\n\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\nCREATE INDEX ix_probabilities_oid ON probability USING hash (oid);\nCREATE INDEX ix_probabilities_probability ON probability USING btree (probability);\nCREATE INDEX ix_probabilities_ranking ON probability USING btree (ranking);\nCREATE INDEX ix_classification_rank1 ON probability USING btree (ranking) WHERE (ranking = 1);\n\n\n\n\n## DEFAULT CONDITIONS YOU NEED TO SET\n### IF THE 'probability' TABLE is used, use always the next conditions, unless the user explicitly specifies different probability conditions.\n- 'probability.ranking' = 1 ; this only return the most likely probabilities, if the user request all ranking probabilities, don't use it.\n- 'probability.classifier_name='lc_classifier'\n### IF THE 'feature' TABLE is used with 2 or more features, you need to take the following steps, because it is a transposed table (each feature is in a different row).\nI. Create a sub-query using the 'probability' TABLE filtering the desired objects.\nII. For each feature, you have to make a sub-query retrieving the specific feature adding the condition of its value, taking only the oids in the 'probability' sub-query with an INNER JOIN inside each 'feature' sub-query to retrieve only the features associated with the desired spatial objects.\nIII. Make an UNION between the sub-queries of each feature from step II\nIV. Make an INTERSECT between the sub-queries of each feature from step II\nV. Filter the 'UNION' query from step III selecting only the 'oids' that are in the 'INTERSECT' query from step IV\nVI. Add the remaining conditions to the final result of step V, using the 'probability' sub-query from step I.\n### GENERAL\n### Important points to consider\n- If the user doesn't specify explicit columns or information that is not in a column, choose all the columns, for example by using the \u201cSELECT *\u201d SQL statement, from ALL the tables used, including the ones from the sub-queries.\n- Mantain the EXACT COLUMN names as they are in the database, unless the user explicitly asks you to do so in the request, giving you the new name to use. This is crucial for the query to work correctly.\n- Mantain the exact class names as they are in the database, marked with single quotes, for example, 'SNIa'.\n\n\n# If you need to use 2 or 3 tables, try using a sub-query or INNER JOIN over 'probability' TABLE or 'object' TABLE, or an INNER JOIN between 'probabbility' and 'object', or over an INNER JOIN between 'probability', 'object' and 'magstat', if it is necessary (priority in this order).\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Finally, join all the steps in a final query, with the following format: \n```sql [FINAL QUERY HERE] ```\nDON'T include anything else inside and after your FINAL answer.\n\n# Use the next decomposed planification to write the query:\nTo decompose the user request into a systematic plan for generating the PostgreSQL query, follow these detailed steps:\n\n1. **Identify the main tables involved:**\n   - The `object` table for object identifiers and general statistics.\n   - The `detection` table for time-series photometric data, magnitudes, errors, band identifiers, and detection times.\n   - The `probability` table for classification probabilities.\n   - The `magstat` table for initial rise estimates and other light curve statistics.\n\n2. **Define the filtering criteria based on the user request:**\n   - Classification as SN II: filter `probability` entries where `class_name='SNII'` and `classifier_name='lc_classifier'`.\n   - Probability threshold: `probability.probability > 0.6`.\n   - Initial rise rate: `magstat.dmdt_first > 0.5` (assuming `dmdt_first` represents the initial rise rate in mag/day).\n   - Time range: objects observed between 2019 and 2022, i.e., `firstmjd` and `lastmjd` within this period.\n   - Number of detections: `object.ndet > 50`.\n   - Object classification: only objects classified as SN II with high probability.\n\n3. **Construct a sub-query to filter objects based on classification probability:**\n   - Filter `probability` table for `class_name='SNII'`, `classifier_name='lc_classifier'`, and `probability > 0.6`.\n   - Select `oid` values from this filtered set.\n\n4. **Filter objects based on the time range:**\n   - Use `object.firstmjd` and `object.lastmjd` to select objects with detection dates within 2019-2022.\n   - Convert calendar years to Julian date ranges, e.g., 2019-01-01 to 2022-12-31, and filter accordingly.\n\n5. **Filter objects with sufficient detections:**\n   - Use `object.ndet > 50` as a criterion.\n\n6. **Join the filtered objects with the classification probabilities:**\n   - Use an INNER JOIN between `object` and the filtered `probability` sub-query on `oid`.\n\n7. **Retrieve the relevant object information:**\n   - Object identifier (`object.oid`).\n   - Candidate identifier (`detection.candid`) for each detection.\n   - Magnitudes (`detection.magpsf`) and magnitude errors (`detection.sigmapsf`).\n   - Band identifiers (`detection.fid`).\n   - Detection time (`detection.mjd`).\n\n8. **Incorporate the initial rise estimate:**\n   - Use `magstat.dmdt_first` for each object to get the initial rise rate.\n   - Filter for `dmdt_first > 0.5`.\n\n9. **Ensure the data is ordered as a function of time:**\n   - Order the results by `detection.mjd` ascending.\n\n10. **Combine all filters and conditions:**\n    - Use nested sub-queries or CTEs (WITH clauses) to organize the filtering steps.\n    - Use `JOIN`s to connect `object`, `detection`, `probability`, and `magstat` tables.\n    - Apply the filters for probability, classification, time range, detection count, and initial rise rate.\n\n11. **Final selection:**\n    - Select the object identifier, candidate identifier, magnitude, magnitude error, initial rise estimate, and band identifier.\n    - Ensure the output is ordered chronologically by detection time.\n\n12. **Additional considerations:**\n    - Use the `q3c_radial_query` or similar spatial functions if spatial filtering is needed (not specified here).\n    - Confirm that only the most probable classification (`ranking=1`) is used if multiple probabilities exist.\n    - Use `WHERE` clauses to enforce all the above conditions.\n\nThis systematic plan ensures that the query will accurately retrieve the desired data for objects classified as SN II in the specified period, with the specified probability and light curve characteristics, ordered as a function of time.\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n"}, {"role": "user", "content": "\n # User Request: ''Get the object identifier, candidate identifier, magnitudes, magnitude errors, initial rise estimate, and band identifiers as a function of time of the objects classified as SN II in the year 2019-2022, with probability larger than 0.6, initial rise rate greater than 0.5 mag/day in ZTF g and r-band and number of detections greater than 50.''"}], "max_tokens": 4000, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "97-0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\n# As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nBe thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\n# The Database has the following tables that can be used to generate the SQL query:\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE INDEX ix_object_ndet ON object USING btree (ndet);\nCREATE INDEX ix_object_firstmjd ON object USING btree (firstmjd);\nCREATE INDEX ix_object_g_r_max ON object USING btree (g_r_max);\nCREATE INDEX ix_object_g_r_mean_corr ON object USING btree (g_r_mean_corr);\nCREATE INDEX ix_object_meanra ON object USING btree (meanra);\nCREATE INDEX ix_object_meandec ON object USING btree (meandec);\n\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\nCREATE INDEX ix_probabilities_oid ON probability USING hash (oid);\nCREATE INDEX ix_probabilities_probability ON probability USING btree (probability);\nCREATE INDEX ix_probabilities_ranking ON probability USING btree (ranking);\nCREATE INDEX ix_classification_rank1 ON probability USING btree (ranking) WHERE (ranking = 1);\n\n\n\n\n## DEFAULT CONDITIONS YOU NEED TO SET\n### IF THE 'probability' TABLE is used, use always the next conditions, unless the user explicitly specifies different probability conditions.\n- 'probability.ranking' = 1 ; this only return the most likely probabilities, if the user request all ranking probabilities, don't use it.\n- 'probability.classifier_name='lc_classifier' ; this will return the classifications by the light curve classifier\n### GENERAL\n### Important points to consider\n- If the user doesn't specify explicit columns or information that is not in a column, choose all the columns, for example by using the \u201cSELECT *\u201d SQL statement, from ALL the tables used, including the ones from the sub-queries.\n- Mantain the EXACT COLUMN names as they are in the database, unless the user explicitly asks you to do so in the request, giving you the new name to use. This is crucial for the query to work correctly.\n- Mantain the exact class names as they are in the database, marked with single quotes, for example, 'SNIa'.\n\n# If you need to use 2 tables, try using a INNER JOIN statement, or a sub-query over 'probability' or 'object', if the query requires it. It is important to be really careful with the use of sub-queries or JOINs, as they can slow down the query.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Answer ONLY with the final SQL query, with the following format: \n  ```sql SQL_QUERY_HERE ```\nDON'T include anything else in your answer. If you want to add comments, use the SQL comment format inside the query.\n\n# Use the next decomposed planification to write the query:\nTo decompose the user request into steps for generating the appropriate PostgreSQL query, follow these detailed steps:\n\n1. **Identify the main tables involved:**\n   - The `object` table contains the primary information about each ZTF object, including its unique identifier.\n   - The `probability` table stores the classification probabilities, linked to objects via the `oid` column.\n   \n2. **Determine the filtering criteria based on probabilities:**\n   - For the stamp classifier:\n     - Filter `probability` entries where `classifier_name` is `'stamp_classifier_1.0.4'`.\n     - Filter for `class_name` equal to `'SN'` (assuming 'AGN' is a class within the stamp classifier, but based on the context, the class name should be `'AGN'`).\n     - Filter for `probability` > 0.6.\n     - Filter for `ranking` = 1 to ensure the top-ranked probability is used.\n   - For the light curve classifier:\n     - Filter `probability` entries where `classifier_name` is `'lc_classifier'`.\n     - Filter for `class_name` equal to `'LPV'`.\n     - Filter for `probability` > 0.7.\n     - Filter for `ranking` = 1.\n   \n3. **Join the `probability` table with the `object` table:**\n   - Use the `oid` column to connect classification probabilities with object identifiers.\n   \n4. **Select objects that satisfy both conditions simultaneously:**\n   - Use inner joins or subqueries to ensure the object has both the stamp classifier probability > 0.6 for AGN and the light curve classifier probability > 0.7 for LPV.\n   \n5. **Limit the number of results:**\n   - Use `LIMIT 50` to restrict the output to at most 50 objects.\n   \n6. **Select the required columns:**\n   - The ZTF object identifier, which is `oid` in the `object` table.\n   - The probability of being an AGN from the stamp classifier, labeled as `prob_stamp_agn`.\n   - The probability of being an LPV from the light curve classifier, labeled as `prob_lc_lpv`.\n   \n7. **Order the results if necessary:**\n   - If no specific order is requested, the default can be arbitrary or based on one of the probabilities.\n   \n8. **Ensure the query adheres to default conditions:**\n   - Use `ranking = 1` for both classifiers.\n   - Use the specified classifier names.\n   \n9. **Construct the final query structure:**\n   - Use subqueries or CTEs (WITH clauses) to separately filter the probabilities for each classifier, then join these with the `object` table to get the object identifiers and probabilities.\n   \n10. **Test the query to verify correctness:**\n    - Confirm that the filtering conditions are correctly applied.\n    - Confirm that the join logic correctly matches objects with their probabilities.\n    - Confirm that the result set does not exceed 50 objects.\n\nThis systematic decomposition ensures the query will accurately retrieve the desired objects with the specified probability thresholds, correctly labeled columns, and within the result limit.\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n"}, {"role": "user", "content": "\n # User Request: ''Get at most 50 ZTF objects that satisfy the following conditions: have a probability larger than 0.6 of being an AGN in the stamp classifier version 'stamp_classifier_1.0.4', and a probability larger than 0.7 of being an LPV in the light curve classifier. Return the following columns: ZTF identifier, probability in the stamp classifier (name this column 'prob_stamp_agn'), and probability in the light curve classifier (name this column 'prob_lc_lpv')''"}], "max_tokens": 4000, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "97-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\n# As a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nBe thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\n# The Database has the following tables that can be used to generate the SQL query:\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE INDEX ix_object_ndet ON object USING btree (ndet);\nCREATE INDEX ix_object_firstmjd ON object USING btree (firstmjd);\nCREATE INDEX ix_object_g_r_max ON object USING btree (g_r_max);\nCREATE INDEX ix_object_g_r_mean_corr ON object USING btree (g_r_mean_corr);\nCREATE INDEX ix_object_meanra ON object USING btree (meanra);\nCREATE INDEX ix_object_meandec ON object USING btree (meandec);\n\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\nCREATE INDEX ix_probabilities_oid ON probability USING hash (oid);\nCREATE INDEX ix_probabilities_probability ON probability USING btree (probability);\nCREATE INDEX ix_probabilities_ranking ON probability USING btree (ranking);\nCREATE INDEX ix_classification_rank1 ON probability USING btree (ranking) WHERE (ranking = 1);\n\n\n\n\n## DEFAULT CONDITIONS YOU NEED TO SET\n### IF THE 'probability' TABLE is used, use always the next conditions, unless the user explicitly specifies different probability conditions.\n- 'probability.ranking' = 1 ; this only return the most likely probabilities, if the user request all ranking probabilities, don't use it.\n- 'probability.classifier_name='lc_classifier' ; this will return the classifications by the light curve classifier\n### GENERAL\n### Important points to consider\n- If the user doesn't specify explicit columns or information that is not in a column, choose all the columns, for example by using the \u201cSELECT *\u201d SQL statement, from ALL the tables used, including the ones from the sub-queries.\n- Mantain the EXACT COLUMN names as they are in the database, unless the user explicitly asks you to do so in the request, giving you the new name to use. This is crucial for the query to work correctly.\n- Mantain the exact class names as they are in the database, marked with single quotes, for example, 'SNIa'.\n\n# If you need to use 2 tables, try using a INNER JOIN statement, or a sub-query over 'probability' or 'object', if the query requires it. It is important to be really careful with the use of sub-queries or JOINs, as they can slow down the query.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Answer ONLY with the final SQL query, with the following format: \n  ```sql SQL_QUERY_HERE ```\nDON'T include anything else in your answer. If you want to add comments, use the SQL comment format inside the query.\n\n# Use the next decomposed planification to write the query:\n1. **Identify the relevant tables and columns:**\n   - The `object` table contains the ZTF object identifiers (`oid`) and other basic properties.\n   - The `probability` table contains classification probabilities, with columns: `oid`, `class_name`, `classifier_name`, `classifier_version`, `probability`.\n   - The classification probabilities for the stamp classifier are stored with `classifier_name='stamp_classifier'` and `classifier_version='stamp_classifier_1.0.4'`.\n   - The probabilities for the light curve classifier are stored with `classifier_name='lc_classifier'`.\n\n2. **Filter the probability table for the specific classifications and classifiers:**\n   - Select entries where `class_name='AGN'`, `classifier_name='stamp_classifier'`, and `classifier_version='stamp_classifier_1.0.4'`.\n   - Select entries where `class_name='LPV'` and `classifier_name='lc_classifier'`.\n\n3. **Apply probability thresholds:**\n   - For AGN in the stamp classifier, keep only entries with `probability > 0.6`.\n   - For LPV in the light curve classifier, keep only entries with `probability > 0.7`.\n\n4. **Join the filtered probabilities with the `object` table:**\n   - Join on `oid` to associate each classification probability with the corresponding object.\n\n5. **Ensure the object satisfies both conditions simultaneously:**\n   - Use an inner join or a subquery approach to find objects that meet both criteria:\n     - Have an AGN probability > 0.6 in the specified stamp classifier version.\n     - Have an LPV probability > 0.7 in the light curve classifier.\n\n6. **Limit the results to at most 50 objects:**\n   - Use a `LIMIT 50` clause to restrict the number of returned objects.\n\n7. **Select the required columns:**\n   - The ZTF object identifier (`oid`).\n   - The AGN probability from the stamp classifier, labeled as `prob_stamp_agn`.\n   - The LPV probability from the light curve classifier, labeled as `prob_lc_lpv`.\n\n8. **Order the results if necessary:**\n   - Optionally, order by one of the probabilities or by `oid` for consistency, but this is not explicitly required.\n\n9. **Construct the final query structure:**\n   - Use subqueries or CTEs to separately filter the probabilities for AGN and LPV, then join them on `oid`.\n   - Select the `oid`, `probability` for AGN as `prob_stamp_agn`, and for LPV as `prob_lc_lpv`.\n   - Limit the total number of results to 50.\n\nThis systematic approach ensures that the query accurately retrieves up to 50 ZTF objects that meet both probability thresholds for AGN and LPV classifications, with the specified classifier versions, and returns the requested columns.\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n"}, {"role": "user", "content": "\n # User Request: ''Get at most 50 ZTF objects that satisfy the following conditions: have a probability larger than 0.6 of being an AGN in the stamp classifier version 'stamp_classifier_1.0.4', and a probability larger than 0.7 of being an LPV in the light curve classifier. Return the following columns: ZTF identifier, probability in the stamp classifier (name this column 'prob_stamp_agn'), and probability in the light curve classifier (name this column 'prob_lc_lpv')''"}], "max_tokens": 4000, "temperature": 0.0, "top_p": 1.0, "n": 1}}
