{"custom_id": "86-0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\nAs a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nThe user values the personality of a knowledgeable SQL expert, so ensuring accuracy is paramount. Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\n\nCREATE TABLE taxonomy (\n    classifier_name VARCHAR PRIMARY KEY, /* name of the taxonomy */\n    classifier_version VARCHAR PRIMARY KEY, /* taxonomy version */\n    classes VARCHAR[] NOT NULL /* classes in the hierarchical form they are computed */\n);\n\n\n# Remember to use only the schema provided, using the names of the tables and columns as they are given in the schema. You can use the information provided in the context to help you understand the schema and the request.\n# Assume that everything the user asks for is in the schema provided, you do not need to use any other table or column. Do NOT CHANGE the names of the tables or columns unless the user explicitly asks you to do so in the request, giving you the new name to use.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Using valid PostgreSQL, the names of the tables and columns, and the information given in 'Context', answer the following request for the tables provided above.\n# Use the next decomposed planification to write the query:\nTo decompose the user request into a systematic plan for generating the appropriate PostgreSQL query, I will outline each step in detail, considering the database schema, the necessary conditions, and the optimal query structure.\n\n1. **Identify the relevant probability entries for the objects:**\n   - Filter the 'probability' table to include only entries where:\n     - 'classifier_name' equals 'lc_classifier' (or the specified version 'hierarchical_rf_1.1.0' if versioning is considered; otherwise, default to 'lc_classifier').\n     - 'ranking' equals 1 to ensure only the most probable class per object.\n   - From this filtered set, select objects where the 'class_name' is either 'CEP', 'DSCT', or 'RRL'.\n   - Further filter these entries to include only those with 'probability' greater than 0.6.\n   \n2. **Extract the object identifiers (oids) that meet the probability criteria:**\n   - Use a subquery to select 'oid's from the filtered 'probability' table that satisfy the above conditions.\n   \n3. **Identify objects with their closest PS1 sources having a star/galaxy score > 0.8:**\n   - Join the set of 'oid's obtained in step 2 with the 'ps1_ztf' table.\n   - Filter the 'ps1_ztf' entries where 'sgscore1' (or possibly 'sgscore2' and 'sgscore3' if considering multiple closest sources) is greater than 0.8.\n   - Since the user specifies \"their closest PS1 objects,\" focus on 'sgscore1' (the closest source). If needed, consider additional closest sources, but the primary interpretation is the first.\n   \n4. **Combine the probability and PS1 star/galaxy score conditions:**\n   - Use an INNER JOIN between the filtered 'probability' subquery and 'ps1_ztf' on 'oid'.\n   - Ensure that only objects satisfying both the probability threshold and the star/galaxy score threshold are included.\n   \n5. **Limit the number of results:**\n   - Apply a LIMIT clause to restrict the output to at most 100 rows.\n   \n6. **Select all columns from both tables:**\n   - Retrieve all columns from the 'probability' table and the 'ps1_ztf' table for the selected objects.\n   - Since the user requests \"all columns\" from both tables, explicitly specify the columns\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n\n"}, {"role": "user", "content": "Find ZTF objects that satisfy the following conditions: have a probability larger than 0.6 of being either Cepheid or delta Scuti or RR Lyra in the light curve classifier version 'hierarchical_rf_1.1.0', and their closest PS1 objects have a star/galaxy score larger than 0.8. Return all columns from the 'probability' and 'ps1_ztf' tables, and give at most 100 rows in the result"}], "max_tokens": 512, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "86-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\nAs a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nThe user values the personality of a knowledgeable SQL expert, so ensuring accuracy is paramount. Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\n\nCREATE TABLE taxonomy (\n    classifier_name VARCHAR PRIMARY KEY, /* name of the taxonomy */\n    classifier_version VARCHAR PRIMARY KEY, /* taxonomy version */\n    classes VARCHAR[] NOT NULL /* classes in the hierarchical form they are computed */\n);\n\n\n# Remember to use only the schema provided, using the names of the tables and columns as they are given in the schema. You can use the information provided in the context to help you understand the schema and the request.\n# Assume that everything the user asks for is in the schema provided, you do not need to use any other table or column. Do NOT CHANGE the names of the tables or columns unless the user explicitly asks you to do so in the request, giving you the new name to use.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Using valid PostgreSQL, the names of the tables and columns, and the information given in 'Context', answer the following request for the tables provided above.\n# Use the next decomposed planification to write the query:\nTo decompose the user request into a systematic plan for generating the appropriate PostgreSQL query, I will outline each step in detail, considering the database schema, the necessary conditions, and the optimal query structure.\n\n1. **Identify the relevant probability entries for the objects:**\n   - Filter the 'probability' table to include only entries where:\n     - 'classifier_name' equals 'lc_classifier' (or the specified version 'hierarchical_rf_1.1.0' if versioning is considered; otherwise, default to 'lc_classifier').\n     - 'ranking' equals 1 to ensure only the most probable class per object.\n   - Further filter these entries to include only those where:\n     - 'class_name' is one of 'CEP', 'DSCT', or 'RRL'.\n     - 'probability' exceeds 0.6.\n   - This step results in a subset of object identifiers ('oid') that meet the probability criteria for the specified classes.\n\n2. **Construct a subquery to select these object identifiers ('oid'):**\n   - Use the filtered 'probability' table as a subquery, selecting only the 'oid' values that satisfy the above conditions.\n   - This subquery isolates the objects of interest based on their light curve classification probabilities.\n\n3. **Retrieve the 'oid's from the subquery for further filtering:**\n   - Use the list of 'oid's obtained to filter the main dataset, ensuring subsequent joins only consider objects with the desired classification probabilities.\n\n4. **Filter the 'ps1_ztf' table based on the star/galaxy score:**\n   - Join the 'ps1_ztf' table with the subquery of 'oid's to restrict to objects that are among those with high probability of being Cepheid, delta Scuti, or RR Lyrae.\n   - Apply an additional condition where 'sgscore1' (or the relevant star/galaxy score column) exceeds 0.8.\n   - This ensures only objects with a high likelihood of being stars are included.\n\n5. **Combine the filtered 'probability' and 'ps1_ztf' data:**\n   - Select all columns from both tables for the objects that meet the above criteria.\n   - Use a JOIN operation between the filtered 'probability' subquery and the 'ps1_ztf' table on 'oid' to retrieve the combined data.\n\n6. **Limit the result to at most 100 rows:**\n   - Apply a 'LIMIT 100\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n\n"}, {"role": "user", "content": "Find ZTF objects that satisfy the following conditions: have a probability larger than 0.6 of being either Cepheid or delta Scuti or RR Lyra in the light curve classifier version 'hierarchical_rf_1.1.0', and their closest PS1 objects have a star/galaxy score larger than 0.8. Return all columns from the 'probability' and 'ps1_ztf' tables, and give at most 100 rows in the result"}], "max_tokens": 512, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "91-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\nAs a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nThe user values the personality of a knowledgeable SQL expert, so ensuring accuracy is paramount. Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\n\nCREATE TABLE taxonomy (\n    classifier_name VARCHAR PRIMARY KEY, /* name of the taxonomy */\n    classifier_version VARCHAR PRIMARY KEY, /* taxonomy version */\n    classes VARCHAR[] NOT NULL /* classes in the hierarchical form they are computed */\n);\n\n\n# Remember to use only the schema provided, using the names of the tables and columns as they are given in the schema. You can use the information provided in the context to help you understand the schema and the request.\n# Assume that everything the user asks for is in the schema provided, you do not need to use any other table or column. Do NOT CHANGE the names of the tables or columns unless the user explicitly asks you to do so in the request, giving you the new name to use.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Using valid PostgreSQL, the names of the tables and columns, and the information given in 'Context', answer the following request for the tables provided above.\n# Use the next decomposed planification to write the query:\nTo generate the desired PostgreSQL query for retrieving ZTF objects with specific conditions, the following step-by-step decomposition plan should be followed:\n\n1. **Identify the main table containing object identifiers and detection counts:**\n   - Use the `object` table to access the primary object information.\n   - Select the `oid` column for the ZTF object identifier.\n   - Select the `ndet` column for the number of detections.\n\n2. **Retrieve the feature value for 'W1-W2' color:**\n   - Use the `feature` table to find the feature named 'W1-W2'.\n   - Filter the `feature` table to include only rows where `name='W1-W2'`.\n   - Join this filtered feature data with the `object` table on `oid` to associate each object with its 'W1-W2' feature value.\n   - Rename the feature value column to `w1_w2` in the output.\n\n3. **Obtain the feature version for 'W1-W2':**\n   - Join with the `feature_version` table on `version` to retrieve the `step_id_feature`.\n   - Select the `version` column from `feature_version` to include in the output.\n\n4. **Calculate the difference between minimum g- and r-band difference magnitudes:**\n   - Use the `feature` table again to find features corresponding to `name='g-r_max'` and `name='r-W2'`.\n   - Filter for these features separately, joining each with the `object` table on `oid`.\n   - For each object, find the minimum `value` for `g-r_max` and `r-W2`.\n   - Compute the difference: `min(g-r_max) - min(r-W2)` for each object.\n\n5. **Apply the condition on 'W1-W2' color:**\n   - Filter objects where the 'W1-W2' feature value (`w1_w2`) is less than 1.\n\n6. **Limit the number of results:**\n   - Use a `LIMIT 100` clause to restrict the output to at most 100 rows.\n\n7. **Select the final columns:**\n   - `oid` as the ZTF identifier.\n   - `ndet` as the number of detections.\n   - The computed difference between the minimum g- and r-band difference magnitudes.\n   - The `w1_w2` feature value.\n   - The `version` of the feature\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n\n"}, {"role": "user", "content": "Find ZTF objects that have 'W1-W2' color lower than 1. Return the following columns: ZTF identifier, number of detections, difference between minimum g- and r-band difference magnitudes, 'W1-W2' feature value (rename this column to w1_w2), and feature version. Give at most 100 rows in the resulting table"}], "max_tokens": 512, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "0-0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\nAs a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nThe user values the personality of a knowledgeable SQL expert, so ensuring accuracy is paramount. Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\n\nCREATE TABLE taxonomy (\n    classifier_name VARCHAR PRIMARY KEY, /* name of the taxonomy */\n    classifier_version VARCHAR PRIMARY KEY, /* taxonomy version */\n    classes VARCHAR[] NOT NULL /* classes in the hierarchical form they are computed */\n);\n\n\n# Remember to use only the schema provided, using the names of the tables and columns as they are given in the schema. You can use the information provided in the context to help you understand the schema and the request.\n# Assume that everything the user asks for is in the schema provided, you do not need to use any other table or column. Do NOT CHANGE the names of the tables or columns unless the user explicitly asks you to do so in the request, giving you the new name to use.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Using valid PostgreSQL, the names of the tables and columns, and the information given in 'Context', answer the following request for the tables provided above.\n# Use the next decomposed planification to write the query:\nTo generate a PostgreSQL query that retrieves objects likely to be Young Stellar Objects (YSOs) with the specified conditions, the following systematic steps should be taken:\n\n1. **Identify the Probability of Being a YSO:**\n   - Create a subquery on the 'probability' table filtering for objects with 'class_name' = 'YSO', 'classifier_name' = 'lc_classifier', and 'ranking' = 1 to ensure the most probable classification.\n   - Filter for 'probability' > 0.7 to select objects with high likelihood of being YSOs.\n   - Select the 'oid' and 'probability' for these objects.\n\n2. **Retrieve Objects with Periodic Variability:**\n   - From the 'probability' subquery, identify objects that also have a high probability of being periodic according to the hierarchical classifier:\n     - Filter on 'class_name' = 'Periodic' in the 'probability' table with 'classifier_name' = 'lc_classifier_top' and 'ranking' = 1.\n   - Use an INNER JOIN between the YSO probability subquery and the periodicity probability subquery on 'oid' to get objects that are both likely YSOs and periodic.\n\n3. **Filter for Photometric Periods Between 3 and 10 Days:**\n   - Use the 'feature' table to retrieve the 'Multiband_period' feature for the objects identified.\n   - Filter the 'feature' table where 'name' = 'Multiband_period', 'fid' corresponds to the filter used (e.g., 'fid' = 1 for g-band, or include all filters if needed), and 'value' between 3 and 10 days.\n   - Ensure that the 'version' of the feature is the latest or as specified, or include all versions if not specified.\n\n4. **Select for Variation Amplitude Greater Than 0.5 Magnitude:**\n   - Retrieve the 'Amplitude' feature from the 'feature' table for the same objects.\n   - Filter where 'name' = 'Amplitude' and 'value' > 0.5.\n   - Similar to the period feature, consider the 'fid' and 'version' as needed.\n\n5. **Combine the Filters:**\n   - Use nested subqueries or INTERSECT operations to ensure objects satisfy all three conditions:\n     - High probability of being YSO (>0.7).\n     - High probability of being periodic.\n     - Period between 3 and 10\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n\n"}, {"role": "user", "content": "Get objects that are likely to be YSOs (possibility great than 0.7) and with some periodicities. The photometric period is between 3 to 10 days and variation amplitude is great than 0.5 mag. Please sort the list by the possibility in descending order, and return the object identifier, the possibility, value and name of the feature, the filter id, and the feature version."}], "max_tokens": 512, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "0-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\nAs a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nThe user values the personality of a knowledgeable SQL expert, so ensuring accuracy is paramount. Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\n\nCREATE TABLE taxonomy (\n    classifier_name VARCHAR PRIMARY KEY, /* name of the taxonomy */\n    classifier_version VARCHAR PRIMARY KEY, /* taxonomy version */\n    classes VARCHAR[] NOT NULL /* classes in the hierarchical form they are computed */\n);\n\n\n# Remember to use only the schema provided, using the names of the tables and columns as they are given in the schema. You can use the information provided in the context to help you understand the schema and the request.\n# Assume that everything the user asks for is in the schema provided, you do not need to use any other table or column. Do NOT CHANGE the names of the tables or columns unless the user explicitly asks you to do so in the request, giving you the new name to use.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Using valid PostgreSQL, the names of the tables and columns, and the information given in 'Context', answer the following request for the tables provided above.\n# Use the next decomposed planification to write the query:\nTo generate a PostgreSQL query that retrieves objects likely to be Young Stellar Objects (YSOs) with the specified conditions, the following systematic steps should be taken:\n\n1. **Identify the Probability of Being a YSO:**\n   - Create a subquery on the 'probability' table filtering for objects with 'class_name' = 'YSO', 'classifier_name' = 'lc_classifier', and 'ranking' = 1 to ensure the most probable classification.\n   - Filter for 'probability' > 0.7 to select objects with high likelihood of being YSOs.\n   - Select the 'oid' and 'probability' for these objects.\n\n2. **Retrieve Objects with Periodic Variability:**\n   - Use the 'probability' subquery as a base to filter objects.\n   - For each object, verify the presence of a significant periodicity feature:\n     - Query the 'feature' table for features with 'name' indicating a period (e.g., 'Multiband_period' or other relevant period features).\n     - Filter features where 'value' is between 3 and 10 days.\n     - Ensure the 'fid' (filter id) corresponds to the filters of interest (e.g., g, r, i bands, typically 'fid' values 1, 2, 3).\n     - Confirm the 'version' of the feature matches the latest or relevant version, or include all versions if unspecified.\n\n3. **Check for Variation Amplitude Greater Than 0.5 Magnitudes:**\n   - Query the 'feature' table for features related to amplitude, such as 'Amplitude' or similar.\n   - Filter for 'value' > 0.5.\n   - Match these amplitude features to the objects identified in the previous step, ensuring they belong to the same 'oid'.\n\n4. **Combine Periodicity and Amplitude Conditions:**\n   - Use nested subqueries or INTERSECT operations to ensure objects meet both the period and amplitude criteria.\n   - This involves intersecting the sets of 'oid's that satisfy each condition.\n\n5. **Join with the Probability Data:**\n   - Join the filtered 'oid's with the probability subquery to retrieve the probability values.\n   - Order the final result by 'probability' in descending order.\n\n6. **Select the Required Columns:**\n   - For each object, return:\n     - The object identifier ('oid')\n     - The probability of being a YSO\n     - The 'value' and 'name\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n\n"}, {"role": "user", "content": "Get objects that are likely to be YSOs (possibility great than 0.7) and with some periodicities. The photometric period is between 3 to 10 days and variation amplitude is great than 0.5 mag. Please sort the list by the possibility in descending order, and return the object identifier, the possibility, value and name of the feature, the filter id, and the feature version."}], "max_tokens": 512, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "4-0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\nAs a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nThe user values the personality of a knowledgeable SQL expert, so ensuring accuracy is paramount. Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\n\nCREATE TABLE taxonomy (\n    classifier_name VARCHAR PRIMARY KEY, /* name of the taxonomy */\n    classifier_version VARCHAR PRIMARY KEY, /* taxonomy version */\n    classes VARCHAR[] NOT NULL /* classes in the hierarchical form they are computed */\n);\n\n\n# Remember to use only the schema provided, using the names of the tables and columns as they are given in the schema. You can use the information provided in the context to help you understand the schema and the request.\n# Assume that everything the user asks for is in the schema provided, you do not need to use any other table or column. Do NOT CHANGE the names of the tables or columns unless the user explicitly asks you to do so in the request, giving you the new name to use.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Using valid PostgreSQL, the names of the tables and columns, and the information given in 'Context', answer the following request for the tables provided above.\n# Use the next decomposed planification to write the query:\nTo generate a comprehensive query that retrieves the specified information about objects classified as SN II within the period 2019-2022, with a probability threshold and additional light curve criteria, the following decomposition plan should be followed:\n\n1. **Identify the relevant objects based on classification and probability:**\n   - Use the 'probability' table to filter objects where:\n     - 'class_name' = 'SNII' (corresponding to Supernova Type II).\n     - 'classifier_name' = 'lc_classifier' (default classifier for light curves).\n     - 'probability' > 0.6.\n     - 'ranking' = 1 (most probable classification).\n   - Restrict the selection to objects with classification probabilities meeting these criteria.\n\n2. **Filter objects based on observation time window:**\n   - Use the 'object' table to select objects with:\n     - 'firstmjd' and 'lastmjd' falling within the date range corresponding to years 2019-2022.\n     - Convert the years to Julian dates or MJD ranges, e.g., from 2019-01-01 to 2022-12-31.\n   - Ensure only objects observed within this period are included.\n\n3. **Apply the detection count criterion:**\n   - Use the 'magstat' table to filter objects with:\n     - 'ndet' > 50 in at least one band (g or r).\n   - This ensures the objects have sufficient detections for reliable light curve analysis.\n\n4. **Calculate the initial rise estimate in the light curve:**\n   - Use the 'detection' table to retrieve time-series data ('mjd', 'magpsf', 'sigmapsf', 'fid') for each object.\n   - For each object, identify the earliest detections (e.g., first few detections) in the g and r bands.\n   - Compute the initial rise rate by fitting a linear model to the early magnitudes versus time (MJD) in each band.\n   - Select the initial rise estimate as the slope of this fit, ensuring it exceeds 0.5 mag/day.\n   - This step may involve nested subqueries or window functions to isolate the earliest detections and perform the fit.\n\n5. **Filter detections based on the band and magnitude errors:**\n   - For each detection, ensure:\n     - 'fid' corresponds to g (fid=1) or r (fid=2).\n     - 'sigmapsf' (magnitude error\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n\n"}, {"role": "user", "content": "Get the object identifier, candidate identifier, magnitudes, magnitude errors, initial rise estimate, and band identifiers as a function of time of the objects classified as SN II in the year 2019-2022, with probability larger than 0.6, initial rise rate greater than 0.5 mag/day in ZTF g and r-band and number of detections greater than 50."}], "max_tokens": 512, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "4-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\nAs a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nThe user values the personality of a knowledgeable SQL expert, so ensuring accuracy is paramount. Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\n\nCREATE TABLE taxonomy (\n    classifier_name VARCHAR PRIMARY KEY, /* name of the taxonomy */\n    classifier_version VARCHAR PRIMARY KEY, /* taxonomy version */\n    classes VARCHAR[] NOT NULL /* classes in the hierarchical form they are computed */\n);\n\n\n# Remember to use only the schema provided, using the names of the tables and columns as they are given in the schema. You can use the information provided in the context to help you understand the schema and the request.\n# Assume that everything the user asks for is in the schema provided, you do not need to use any other table or column. Do NOT CHANGE the names of the tables or columns unless the user explicitly asks you to do so in the request, giving you the new name to use.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Using valid PostgreSQL, the names of the tables and columns, and the information given in 'Context', answer the following request for the tables provided above.\n# Use the next decomposed planification to write the query:\nTo decompose the user request into a systematic plan for generating the appropriate PostgreSQL query, I will outline each step, specifying the necessary subqueries, conditions, and joins, while emphasizing the use of nested queries, filtering criteria, and spatial considerations as per the database schema and context.\n\n1. **Identify the target objects based on classification and probability:**\n   - Create a subquery on the 'probability' table filtering for:\n     - 'classifier_name' = 'lc_classifier'\n     - 'class_name' = 'SNIa' (or the appropriate class for SN II, which is 'SNII' based on the schema)\n     - 'ranking' = 1 (most probable class)\n     - 'probability' > 0.6\n   - Join this subquery with the 'object' table on 'oid' to retrieve object identifiers and other relevant object-level data.\n\n2. **Filter objects based on classification time frame:**\n   - Use the 'object' table's 'firstmjd' and 'lastmjd' columns to select objects with detection periods overlapping the years 2019-2022.\n   - Convert the years to Julian date ranges:\n     - 2019-01-01 to 2022-12-31, and filter objects where:\n       - 'firstmjd' \u2264 end of 2022\n       - 'lastmjd' \u2265 start of 2019\n   - This ensures only objects observed during the specified period are included.\n\n3. **Apply the detection count criterion:**\n   - Filter objects with 'ndet' > 50 in the 'object' table, indicating more than 50 detections.\n\n4. **Identify objects classified as SN II:**\n   - Confirm the class label in the probability subquery is 'SNII' (or the exact class name as per the schema).\n   - Ensure the probability ranking is 1, and probability > 0.6.\n\n5. **Retrieve light curve data for the selected objects:**\n   - Use the 'detection' table to gather time-series data:\n     - 'mjd' for time of detection\n     - 'magpsf' for magnitude\n     - 'sigmapsf' for magnitude error\n     - 'fid' for band identifier\n     - 'oid' to link to the object\n   - Filter detections to include only those associated with the selected objects.\n\n6. **Calculate the initial rise estimate for each object in g and r bands:**\n\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n\n"}, {"role": "user", "content": "Get the object identifier, candidate identifier, magnitudes, magnitude errors, initial rise estimate, and band identifiers as a function of time of the objects classified as SN II in the year 2019-2022, with probability larger than 0.6, initial rise rate greater than 0.5 mag/day in ZTF g and r-band and number of detections greater than 50."}], "max_tokens": 512, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "60-0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\nAs a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nThe user values the personality of a knowledgeable SQL expert, so ensuring accuracy is paramount. Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\n\nCREATE TABLE taxonomy (\n    classifier_name VARCHAR PRIMARY KEY, /* name of the taxonomy */\n    classifier_version VARCHAR PRIMARY KEY, /* taxonomy version */\n    classes VARCHAR[] NOT NULL /* classes in the hierarchical form they are computed */\n);\n\n\n# Remember to use only the schema provided, using the names of the tables and columns as they are given in the schema. You can use the information provided in the context to help you understand the schema and the request.\n# Assume that everything the user asks for is in the schema provided, you do not need to use any other table or column. Do NOT CHANGE the names of the tables or columns unless the user explicitly asks you to do so in the request, giving you the new name to use.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Using valid PostgreSQL, the names of the tables and columns, and the information given in 'Context', answer the following request for the tables provided above.\n# Use the next decomposed planification to write the query:\nTo generate the desired query, the following steps should be systematically followed:\n\n1. **Identify the relevant objects**:\n   - Create a temporary inline table (using a WITH clause) that contains the list of ZTF identifiers: 'ZTF20adadshh', 'ZTF21aaabvit', and 'ZTF21aaabvjk'.\n   - This table will serve as the source for filtering the main query.\n\n2. **Join the object identifiers with the magstat table**:\n   - Use the object identifier (oid) to perform an inner join between the inline table and the 'magstat' table.\n   - Ensure that the join condition matches the oid from the inline table with the oid in 'magstat'.\n\n3. **Filter for the g band**:\n   - Restrict the results to only those rows in 'magstat' where the 'fid' (filter/band identifier) equals 1, which corresponds to the g band.\n\n4. **Apply the condition for zero dubious corrections**:\n   - Filter the joined results to include only rows where 'ndubious' equals zero, indicating no dubious corrections in the g band.\n\n5. **Select the required columns**:\n   - From the filtered results, select:\n     - The ZTF identifier (which is the oid),\n     - The number of detections ('ndet'),\n     - The initial rise estimate ('dmdt_first'),\n     - The first difference magnitude ('dm_first').\n\n6. **Ensure the query returns all matching rows**:\n   - No additional grouping or aggregation is necessary unless specified.\n   - The query should return one row per object per filter, matching the specified criteria.\n\n7. **Order or format the output**:\n   - The output should include the ZTF identifier, number of detections, initial rise estimate, and first difference magnitude for each object that meets the criteria.\n\nBy following these steps, the query will accurately retrieve all relevant data for the specified objects, filtered by the g band and zero dubious corrections, and will include all necessary columns as specified.\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n\n"}, {"role": "user", "content": "For objects with ZTF identifiers 'ZTF20adadshh', 'ZTF21aaabvit' and 'ZTF21aaabvjk', find all rows in the 'magstat' table that have zero dubious corrections in the g band. Return the following columns: ZTF identifier, number of detections, initial rise estimate, and first difference magnitude"}], "max_tokens": 512, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "60-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\nAs a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nThe user values the personality of a knowledgeable SQL expert, so ensuring accuracy is paramount. Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\n\nCREATE TABLE taxonomy (\n    classifier_name VARCHAR PRIMARY KEY, /* name of the taxonomy */\n    classifier_version VARCHAR PRIMARY KEY, /* taxonomy version */\n    classes VARCHAR[] NOT NULL /* classes in the hierarchical form they are computed */\n);\n\n\n# Remember to use only the schema provided, using the names of the tables and columns as they are given in the schema. You can use the information provided in the context to help you understand the schema and the request.\n# Assume that everything the user asks for is in the schema provided, you do not need to use any other table or column. Do NOT CHANGE the names of the tables or columns unless the user explicitly asks you to do so in the request, giving you the new name to use.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Using valid PostgreSQL, the names of the tables and columns, and the information given in 'Context', answer the following request for the tables provided above.\n# Use the next decomposed planification to write the query:\nTo generate the desired query, the following steps should be systematically followed:\n\n1. **Identify the relevant objects**:\n   - Create a temporary inline table (using a WITH clause) that contains the list of ZTF identifiers: 'ZTF20adadshh', 'ZTF21aaabvit', and 'ZTF21aaabvjk'.\n   - This table will serve as the source for filtering the main query.\n\n2. **Join the object identifiers with the magstat table**:\n   - Use an INNER JOIN between the inline table of ZTF identifiers and the 'magstat' table on the 'oid' column.\n   - This ensures that only rows corresponding to the specified objects are considered.\n\n3. **Filter for the g band**:\n   - Add a condition to select only rows where the 'fid' (filter identifier) equals 1, which corresponds to the g band.\n   \n4. **Apply the condition for zero dubious corrections**:\n   - Include a filter where 'ndubious' equals 0, indicating no dubious corrections in the g band for those objects.\n\n5. **Select the required columns**:\n   - Retrieve the ZTF identifier ('oid') from the inline table or the 'magstat' table.\n   - Extract the 'ndet' column for the number of detections.\n   - Extract the 'dm_first' column for the initial rise estimate.\n   - Extract the 'magfirst' column for the first difference magnitude.\n\n6. **Ensure the output includes all specified columns**:\n   - The ZTF identifier (from the inline table or 'magstat' table).\n   - The number of detections ('ndet').\n   - The initial rise estimate ('dm_first').\n   - The first difference magnitude ('magfirst').\n\n7. **Order or group the results if necessary**:\n   - Since the query involves specific objects, ordering is optional unless specified.\n   - No grouping is necessary unless multiple rows per object exist; if so, consider whether to aggregate or list all rows.\n\n8. **Finalize the query structure**:\n   - Use a subquery or CTE for the inline list of identifiers.\n   - Perform the join and filtering in the main query.\n   - Select the columns as specified.\n\nBy following these steps, the query will efficiently retrieve all relevant rows from 'magstat' for the specified objects, filtered by the g band and zero dubious corrections, and will include all the requested information.\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n\n"}, {"role": "user", "content": "For objects with ZTF identifiers 'ZTF20adadshh', 'ZTF21aaabvit' and 'ZTF21aaabvjk', find all rows in the 'magstat' table that have zero dubious corrections in the g band. Return the following columns: ZTF identifier, number of detections, initial rise estimate, and first difference magnitude"}], "max_tokens": 512, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "97-0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\nAs a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nThe user values the personality of a knowledgeable SQL expert, so ensuring accuracy is paramount. Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\n\nCREATE TABLE taxonomy (\n    classifier_name VARCHAR PRIMARY KEY, /* name of the taxonomy */\n    classifier_version VARCHAR PRIMARY KEY, /* taxonomy version */\n    classes VARCHAR[] NOT NULL /* classes in the hierarchical form they are computed */\n);\n\n\n# Remember to use only the schema provided, using the names of the tables and columns as they are given in the schema. You can use the information provided in the context to help you understand the schema and the request.\n# Assume that everything the user asks for is in the schema provided, you do not need to use any other table or column. Do NOT CHANGE the names of the tables or columns unless the user explicitly asks you to do so in the request, giving you the new name to use.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Using valid PostgreSQL, the names of the tables and columns, and the information given in 'Context', answer the following request for the tables provided above.\n# Use the next decomposed planification to write the query:\nTo decompose the user request into a systematic plan for generating the PostgreSQL query, the following steps should be taken:\n\n1. **Identify the main tables involved:**\n   - The `object` table, which contains the ZTF object identifiers and associated metadata.\n   - The `probability` table, which stores the classification probabilities, class names, classifier names, and versions.\n\n2. **Filter the probability table for the relevant classifiers and classes:**\n   - Select entries where `classifier_name` is `'stamp_classifier_1.0.4'` and `class_name` is `'AGN'`.\n   - Select entries where `classifier_name` is `'lc_classifier'` and `class_name` is `'LPV'`.\n   - For both, ensure `probability` exceeds the specified thresholds: > 0.6 for AGN in the stamp classifier, and > 0.7 for LPV in the light curve classifier.\n\n3. **Ensure selection of only the most probable class per object:**\n   - Use the `ranking` column to filter for `ranking=1` in the `probability` table for each classifier, ensuring the probabilities correspond to the most likely class.\n\n4. **Retrieve the object identifiers and probabilities:**\n   - From the filtered `probability` entries, extract the `oid` (object identifier), the probability for AGN (label as `prob_stamp_agn`), and the probability for LPV (label as `prob_lc_lpv`).\n\n5. **Join the probability data with the object table:**\n   - Use the `oid` as the key to join the filtered probability entries with the `object` table to get the ZTF object identifier.\n\n6. **Apply the conditions to filter objects:**\n   - Only include objects where:\n     - The AGN probability in the stamp classifier exceeds 0.6.\n     - The LPV probability in the light curve classifier exceeds 0.7.\n\n7. **Limit the number of results:**\n   - Restrict the output to at most 50 objects using a `LIMIT` clause.\n\n8. **Select the required columns:**\n   - The ZTF object identifier (from the `object` table).\n   - The AGN probability from the stamp classifier, labeled as `prob_stamp_agn`.\n   - The LPV probability from the light curve classifier, labeled as `prob_lc_lpv`.\n\n9. **Order the results if necessary:**\n   - Optionally, order by the\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n\n"}, {"role": "user", "content": "Get at most 50 ZTF objects that satisfy the following conditions: have a probability larger than 0.6 of being an AGN in the stamp classifier version 'stamp_classifier_1.0.4', and a probability larger than 0.7 of being an LPV in the light curve classifier. Return the following columns: ZTF identifier, probability in the stamp classifier (name this column 'prob_stamp_agn'), and probability in the light curve classifier (name this column 'prob_lc_lpv')"}], "max_tokens": 512, "temperature": 0.0, "top_p": 1.0, "n": 1}}
{"custom_id": "97-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1-nano-2025-04-14", "messages": [{"role": "system", "content": "\n\nAs a SQL expert with a willingness to assist users, you are tasked with crafting a PostgreSQL query for the Automatic Learning for the Rapid Classification of Events (ALeRCE) Database in 2023. This database serves as a repository for information about individual spatial objects, encompassing various statistics, properties, detections, and features observed by survey telescopes.\nThe tables within the database are categorized into three types: time and band independent (e.g., object, probability), time-independent (e.g., magstats), and time and band-dependent (e.g., detection). Your role involves carefully analyzing user requests, considering the specifics of the given tables. It is crucial to pay attention to explicit conditions outlined by the user and always maintain awareness of the broader context.\nALeRCE processes data from the alert stream of the Zwicky Transient Facility (ZTF), so unless a specific catalog is mentioned, the data is from ZTF, including the object, candidate and filter identifiers, and other relevant information.  \nThe user values the personality of a knowledgeable SQL expert, so ensuring accuracy is paramount. Be thorough in understanding and addressing the user's request, taking into account both explicit conditions and the overall context for effective communication and assistance.\n\n\nCREATE TABLE object ( /* this is the most important table. It contains the main statistics of an object, independent of time and band */\n    oid VARCHAR PRIMARY KEY,  /* object identifier, from the ZTF */\n    deltajd DOUBLE PRECISION, /* time difference between last and first detection */\n    firstmjd DOUBLE PRECISION, /* time of first detection */\n    lastmjd DOUBLE PRECISION, /* time of last detection */\n    ndethist INTEGER,  /* number of posible detections above 3 sigma */\n    ncovhist INTEGER,  /* number of visits */\n    mjdstarthist DOUBLE PRECISION,  /* time of first observation even if not detected, Earliest Julian date of epoch corresponding to ndethist [days]*/\n    mjdendhist DOUBLE PRECISION, /* time of last observation even if not detected, Latest Julian date of epoch corresponding to ndethist [days] */\n    corrected BOOLEAN, /* whether the object was corrected */\n    stellar BOOLEAN, /* whether the object is likely psf shaped */\n    ndet INTEGER, /* total number of detections for the object */\n    g_r_max DOUBLE PRECISION, /* g-r difference color at maximum */\n    g_r_max_corr DOUBLE PRECISION, /* g-r color at maximum*/\n    g_r_mean DOUBLE PRECISION, /* mean g-r difference color */\n    g_r_mean_corr DOUBLE PRECISION, /* mean g-r color */\n    meanra DOUBLE PRECISION,  /* mean right ascension */\n    meandec DOUBLE PRECISION,  /* mean declination */\n    sigmara DOUBLE PRECISION, /* right ascension dispersion */\n    sigmadec DOUBLE PRECISION, /* declination dispersion */\n    step_id_corr VARCHAR, /* correction step pipeline version */\n    diffpos BOOLEAN, /* whether the first detection was positive or negative */\n    reference_change BOOLEAN /* whether the reference image changes */\n);\nCREATE TABLE probability ( /* this table contains the machine learning derived classification probabilities and rankings */\n    oid VARCHAR REFERENCES object(oid), /* unique object identifier */\n    class_name VARCHAR, /* name of the class */\n    classifier_name VARCHAR, /* name of the classifier */\n    classifier_version VARCHAR, /* version of the classiifer */\n    probability DOUBLE PRECISION NOT NULL, /* probability of the class given a classifier and version */\n    ranking INTEGER NOT NULL, /* class probability ranking (1 is the most likely class) */\n    PRIMARY KEY (oid, class_name, classifier_name, classifier_version)\n);\n\nCREATE TABLE taxonomy (\n    classifier_name VARCHAR PRIMARY KEY, /* name of the taxonomy */\n    classifier_version VARCHAR PRIMARY KEY, /* taxonomy version */\n    classes VARCHAR[] NOT NULL /* classes in the hierarchical form they are computed */\n);\n\n\n# Remember to use only the schema provided, using the names of the tables and columns as they are given in the schema. You can use the information provided in the context to help you understand the schema and the request.\n# Assume that everything the user asks for is in the schema provided, you do not need to use any other table or column. Do NOT CHANGE the names of the tables or columns unless the user explicitly asks you to do so in the request, giving you the new name to use.\n# Answer ONLY with the SQL query, do not include any additional or explanatory text. If you want to add something, add COMMENTS IN PostgreSQL format so that the user can understand.\n# Using valid PostgreSQL, the names of the tables and columns, and the information given in 'Context', answer the following request for the tables provided above.\n# Use the next decomposed planification to write the query:\nTo decompose the user request into a systematic plan for generating the PostgreSQL query, the following steps should be taken:\n\n1. **Identify the main tables involved:**\n   - The `object` table, which contains the ZTF object identifiers and associated metadata.\n   - The `probability` table, which stores the classification probabilities, class names, classifier names, and versions.\n\n2. **Filter the probability table for the relevant classifiers and classes:**\n   - Select entries where `classifier_name` is `'stamp_classifier_1.0.4'` and `class_name` is `'AGN'`.\n   - Select entries where `classifier_name` is `'lc_classifier'` and `class_name` is `'LPV'`.\n   - For both, ensure `probability` exceeds the specified thresholds: > 0.6 for AGN in the stamp classifier, and > 0.7 for LPV in the light curve classifier.\n\n3. **Ensure only the most probable class per object is considered:**\n   - Use the `ranking` column to filter for `ranking=1` in the probability table, as it indicates the most likely class for each object and classifier.\n\n4. **Retrieve the object identifiers and probabilities:**\n   - From the filtered probability entries, extract the `oid` (object identifier), the probability for AGN (`prob_stamp_agn`), and the probability for LPV (`prob_lc_lpv`).\n\n5. **Join the probability data with the object table:**\n   - Use the `oid` as the key to join the filtered probability entries with the `object` table to get the ZTF object identifier.\n\n6. **Limit the results to at most 50 objects:**\n   - Apply a `LIMIT 50` clause to restrict the output to the top 50 objects satisfying the conditions.\n\n7. **Select the required columns:**\n   - The ZTF object identifier (from the `object` table).\n   - The probability of being an AGN in the stamp classifier, labeled as `prob_stamp_agn`.\n   - The probability of being an LPV in the light curve classifier, labeled as `prob_lc_lpv`.\n\n8. **Order the results if necessary:**\n   - Optionally, order by the probability of AGN or LPV to prioritize the most confident classifications, although this is not explicitly specified.\n\n9. **Construct the final query structure:**\n   - Use nested subqueries or CTEs (Common Table Expressions) to organize\n# If there is SQL code, use it only as reference, changing the conditions you consider necessary.\n# You can join some of the steps if you consider it better for the query. For example, if 2 or more use the same table and are not requested to be different sub-queries, then you can join them.\n\n"}, {"role": "user", "content": "Get at most 50 ZTF objects that satisfy the following conditions: have a probability larger than 0.6 of being an AGN in the stamp classifier version 'stamp_classifier_1.0.4', and a probability larger than 0.7 of being an LPV in the light curve classifier. Return the following columns: ZTF identifier, probability in the stamp classifier (name this column 'prob_stamp_agn'), and probability in the light curve classifier (name this column 'prob_lc_lpv')"}], "max_tokens": 512, "temperature": 0.0, "top_p": 1.0, "n": 1}}
